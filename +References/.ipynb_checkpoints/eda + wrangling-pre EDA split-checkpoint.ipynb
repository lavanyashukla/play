{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "everything from data sourcing right up to, but not including, model building.\n",
    "\n",
    "\"identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data\" in the context of \"mapping data from one 'raw' form into another...\" all the way up to \"training a statistical model\" which I like to think of data preparation as encompassing, or \"everything from data sourcing right up to, but not including, model building.\"\n",
    "\n",
    "#### Cleansing:\n",
    "is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.\n",
    "#### Wrangling:\n",
    "...the process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools. This may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data munging as a process typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data using algorithms (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid runtime error messages\n",
    "pd.set_option('display.float_format', lambda x:'%f'%x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* What process generates this data?\n",
    "* Is it generated from industrial equipment, a website, internal software?\n",
    "* When was it created?\n",
    "* How often is it updated?\n",
    "* What database(if any) is it stored in?\n",
    "* Who are the admins of the database?\n",
    "* Can you view the schema?\n",
    "* What is the process that the raw data has gone through before it reached your hands? Has it already been pre-processed before it reaches you?\n",
    "* Is there a data dictionary describing every column?\n",
    "* What systems use the data?\n",
    "* Have their been previous data scientists working with this dataset?\n",
    "* How has data changed over time? Which columns have been added/subtracted? \n",
    "* Is data for some columns not being collected?\n",
    "#### Subject Matter Research\n",
    "* Read articles, watch videos, talk to local subject matter experts\n",
    "* Read articles/papers by academics who have already studied the field using statistical analysis\n",
    "* Could be beneficial to do some analysis first as to not bias your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a folder in your file system to hold all your files for the analysis\n",
    "- Create a documents/spreadsheets file to store the names, titles, contact information and notes of all the people connected to your data\n",
    "- Find and introduce yourself to all the people connected to your data\n",
    "- Connections to others is key to making your projects work. The more you are visible to others the more information will freely pass your way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Source: Files -- Reading various filetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Zip File\n",
    "import zipfile\n",
    "# Extract all contents from zip file\n",
    "with zipfile.ZipFile('armenian-online-job-postings.zip', 'r') as myzip:\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CSV\n",
    "df = pd.read_csv('patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reading TSV files\n",
    "df = pd.read_csv('bestofrt.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pickle file\n",
    "df = pd.read_pickle('adataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Character Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error.\n",
    "\n",
    "*UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte*\n",
    "\n",
    "Use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "PS: Just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "import chardet\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n",
    "\n",
    "Set the parameter for rawdata.read(10000) to something higher/lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Source: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "### Using os.listdir()\n",
    "# List of dictionaries to build file by file and later convert to a DataFrame\n",
    "df_list = []\n",
    "folder = 'rt_html'\n",
    "\n",
    "for movie_html in os.listdir(folder):\n",
    "    with open(os.path.join(folder, movie_html)) as file:\n",
    "        # Your code here\n",
    "        # Note: a correct implementation may take ~15 seconds to run\n",
    "\t\t\t\t# Parse and Clean data from webpage\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "        title = soup.find('title').contents[0][:-len(\" - Rotten Tomatoes\")]\n",
    "        audience_score = soup.find('div', class_='audience-score').find('span', class_='superPageFontColor').contents[0][:-1]\n",
    "        num_audience_ratings = soup.find('div', class_='audience-info').find_all('div')[1].contents[2].strip().replace(',','')\n",
    "        \n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'title': title,\n",
    "                        'audience_score': int(audience_score),\n",
    "                        'number_of_audience_ratings': int(num_audience_ratings)})\n",
    "df = pd.DataFrame(df_list, columns = ['title', 'audience_score', 'number_of_audience_ratings'])\n",
    "\n",
    "### Using glob.glob()\n",
    "# List of dictionaries to build file by file and later convert to a DataFrame\n",
    "df_list = []\n",
    "for ebert_review in glob.glob('ebert_reviews/*.txt'):\n",
    "    with open(ebert_review, encoding='utf-8') as file:\n",
    "        title = file.readline()[:-1]\n",
    "        review_url = file.readline()[:-1]\n",
    "        review_text = file.read()[:-1]\n",
    "        \n",
    "        break:\n",
    "\n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'title': title,\n",
    "                        'review_url': review_url,\n",
    "                        'review_text': review_text})\n",
    "df = pd.DataFrame(df_list, columns = ['title', 'review_url', 'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Source: Downloading Files From The Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# response.contents is in bytes format, not text, therefore we need to open the file in 'wb' (write binary) mode.\n",
    "\n",
    "import requests\n",
    "import os\n",
    "# Make directory if it doesn't already exist\n",
    "folder_name = 'ebert_reviews'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "ebert_review_urls = ['https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9900_1-the-wizard-of-oz-1939-film/1-the-wizard-of-oz-1939-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9901_2-citizen-kane/2-citizen-kane.txt']\n",
    "\n",
    "# Implement the code in the video above in a for loop for all Ebert reviews\n",
    "for url in ebert_review_urls:\n",
    "    response = requests.get(url)\n",
    "    # print (response.content)\n",
    "    with open(os.path.join(folder_name,url.split('/')[-1]), mode=\"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Source: Mashup of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If collecting data from multiple sources, ensure there's a column with unique values that you can use to merge between 2 sources. With multiple sources, these bridging unique values can be different columns for different sets of files being merged.\n",
    "\n",
    "Loops through the Wikipedia page titles in *title_list* and:\n",
    "\n",
    "- ***Stores the ranking of that movie in the Top 100 list based on its position in title_list. Ranking is needed so we can join this DataFrame with the master DataFrame later*.** We can't join on title because the titles of the Rotten Tomatoes pages and the Wikipedia pages differ.\n",
    "- Uses **`[try` and `except` blocks](http://www.pythonforbeginners.com/error-handling/python-try-and-except)** to attempt to query MediaWiki for a movie poster image URL and to attempt to download that image. If the attempt fails and an error is encountered, the offending movie is documented in *image_errors*.\n",
    "- Appends a dictionary with *ranking*, *title*, and *poster_url* as the keys and the extracted values for each as the values to *df_list*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wptools\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "title_list = [\n",
    " 'The_Wizard_of_Oz_(1939_film)',\n",
    " 'Citizen_Kane',\n",
    " 'The_Third_Man',\n",
    " 'Get_Out_(film)',\n",
    " 'Mad_Max:_Fury_Road',\n",
    " 'The_Cabinet_of_Dr._Caligari',\n",
    " 'All_About_Eve',\n",
    " 'Inside_Out_(2015_film)',\n",
    " 'The_Godfather',\n",
    " 'Metropolis_(1927_film)',\n",
    " 'E.T._the_Extra-Terrestrial',\n",
    " 'Modern_Times_(film)']\n",
    "\n",
    "folder_name = 'bestofrt_posters'\n",
    "# Make directory if it doesn't already exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# List of dictionaries to build and convert to a DataFrame later\n",
    "df_list = []\n",
    "image_errors = {}\n",
    "for title in title_list:\n",
    "    try:\n",
    "        # This cell is slow so print ranking to gauge time remaining\n",
    "        ranking = title_list.index(title) + 1\n",
    "        print(ranking)\n",
    "        page = wptools.page(title, silent=True)\n",
    "        # Your code here (three lines)\n",
    "        images = page.get().data['image']\n",
    "        # First image is usually the poster\n",
    "        first_image_url = images[0]['url']\n",
    "        r = requests.get(first_image_url)\n",
    "        # Download movie poster image\n",
    "        i = Image.open(BytesIO(r.content))\n",
    "        image_file_format = first_image_url.split('.')[-1]\n",
    "        i.save(folder_name + \"/\" + str(ranking) + \"_\" + title + '.' + image_file_format)\n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'ranking': int(ranking),\n",
    "                        'title': title,\n",
    "                        'poster_url': first_image_url})\n",
    "    \n",
    "    # Not best practice to catch all exceptions but fine for this short script\n",
    "    except Exception as e:\n",
    "        print(str(ranking) + \"_\" + title + \": \" + str(e))\n",
    "        image_errors[str(ranking) + \"_\" + title] = images\n",
    "\n",
    "for key in image_errors.keys():\n",
    "    print(key)\n",
    "\n",
    "# Inspect unidentifiable images and download them individually\n",
    "for rank_title, images in image_errors.items():\n",
    "    if rank_title == '22_A_Hard_Day%27s_Night_(film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/4/47/A_Hard_Days_night_movieposter.jpg'\n",
    "    if rank_title == '53_12_Angry_Men_(1957_film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/9/91/12_angry_men.jpg'\n",
    "    if rank_title == '72_Rosemary%27s_Baby_(film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/e/ef/Rosemarys_baby_poster.jpg'\n",
    "    if rank_title == '93_Harry_Potter_and_the_Deathly_Hallows_–_Part_2':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/d/df/Harry_Potter_and_the_Deathly_Hallows_%E2%80%93_Part_2.jpg'\n",
    "    title = rank_title[3:]\n",
    "    df_list.append({'ranking': int(title_list.index(title) + 1),\n",
    "                    'title': title,\n",
    "                    'poster_url': url})\n",
    "    r = requests.get(url)\n",
    "    # Download movie poster image\n",
    "    i = Image.open(BytesIO(r.content))\n",
    "    image_file_format = url.split('.')[-1]\n",
    "    i.save(folder_name + \"/\" + rank_title + '.' + image_file_format)\n",
    "\n",
    "# Create DataFrame from list of dictionaries\n",
    "df = pd.DataFrame(df_list, columns = ['ranking', 'title', 'poster_url'])\n",
    "df = df.sort_values('ranking').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Writing to CSV\n",
    "Great for simple datasets.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Lack of standards\n",
    "- Data redundancy\n",
    "- Sharing data can be cumbersome\n",
    "- Not great for large datasets (see *\"When does small become large?\"* in the Cornell link in *More Information*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Write to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"adataset_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Save to Database\n",
    "Pros:\n",
    "\n",
    "Fast. Scales with data, can handle large amounts of data. Can be queried. Can access data where it lives, no need to make copies. Easy to audit and replicate the analysis. Can run queries on multiple tables. Can answer deeper questions than regular dashboards. Check for data integrity (for e.g. type of column type). Can be accessed by multiple people concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_sql('master', engine, index=False)\n",
    "df_gather = pd.read_sql('SELECT * FROM master', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the context of data wrangling, it's recommended that databases and SQL only come into play for gathering data or storing data. That is:\n",
    "\n",
    "- **Connecting to a database and importing data** into a pandas DataFrame (or the analogous data structure in your preferred programming language), then assessing and cleaning that data, or\n",
    "- **Connecting to a database and storing data** you just gathered (which could potentially be from a database), assessed, and cleaned\n",
    "\n",
    "These tasks are especially necessary when you have large amounts of data, which is where SQL and other databases excel over flat files.\n",
    "\n",
    "The two scenarios above can be further broken down into three main tasks:\n",
    "\n",
    "- Connecting to a database in Python\n",
    "- Storing data ***from*** a pandas DataFrame ***in*** a database to which you're connected, and\n",
    "- Importing data ***from*** a database to which you're connected ***to*** a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **independent variable (X)** is the variable that is changed or controlled in a scientific experiment to test the effects on the dependent variable.\n",
    "A **dependent variable (Y)** is the variable being tested and measured in a scientific experiment.\n",
    "\n",
    "D is the dependent variable\n",
    "R is the responding variable\n",
    "Y is the axis on which the dependent or responding variable is graphed (the vertical axis)\n",
    "\n",
    "M is the manipulated variable or the one that is changed in an experiment\n",
    "I is the independent variable\n",
    "X is the axis on which the independent or manipulated variable is graphed (the horizontal axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Choose a data set that you would like to work with.\n",
    "\n",
    "**STEP 2.** Identify a specific topic of interest\n",
    "\n",
    "**STEP 3.** Prepare a codebook of your own (i.e., print individual pages or copy screen and paste into a new document) from the larger codebook that includes the questions/items/variables that measure your selected topics.)\n",
    "\n",
    "**STEP 4.** Identify a second topic that you would like to explore in terms of its association with your original topic.\n",
    "\n",
    "**STEP 5.** Add questions/items/variables documenting this second topic to your personal codebook.\n",
    "\n",
    "**STEP 6.** Identify a basic research question that interests you. - \"Is there an association between A and B\n",
    "in subset of the population C\"?\n",
    "\n",
    "**STEP 7.** Once you have a basic research question, you can think about other variables that might explain your association.\n",
    "\n",
    "**STEP 8.** Perform a literature review to see what research has been previously done on this topic. Use sites such as Google Scholar ([http://scholar.google.com](http://scholar.google.com/)) to search for published academic work in the area(s) of interest. Try to find multiple sources, and take note of basic bibliographic information.\n",
    "\n",
    "**STEP 9.** Based on your literature review, develop a hypothesis about what you believe the association might be between these topics. Be sure to integrate the specific variables you selected into the hypothesis.\n",
    "\n",
    "**STEP 10.** Ddscribe what you intend to do with your variables (data management/feature creation). For example, do I want to\n",
    "look at how often adolescents binge drink, or do I want to create a binary variables (yes/no) for whether adolescents binge drank in the past month? Do I want depression to be a quantitative variable that aggregates multiple questions that ask about depression symptoms, or do I want to use a cutoff to create a categorical variable that groups adolescents into low, medium, or high levels of depression? These decisions should be described when you describe the variables you will be using.\n",
    "\n",
    "All of this is a starting point for your analysis and will change as you perform the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Writing About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"How-to-Write-About-Your-Data1.png\">\n",
    "\n",
    "<img src=\"How-to-Write-About-Your-Data2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example:\n",
    "\n",
    "Sample\n",
    "\n",
    "The sample is from the first wave of the National Epidemiologic Survey on Alcohol and\n",
    "Related Conditions (NESARC), the largest nationwide longitudinal survey of alcohol and\n",
    "drug use and associated psychiatric and medical comorbidities. Participants (N=43,093)\n",
    "represented the civilian, non-institutionalized adult population of the United States, and\n",
    "included persons living in households, military personnel living off base, and persons\n",
    "residing in the following group quarters: boarding or rooming houses, non-transient hotels\n",
    "and motels, shelters, facilities for housing workers, college quarters, and group homes. The\n",
    "NESARC included over sampling of Blacks, Hispanics and young adults aged 18 to 24 years.\n",
    "The data analytic sample for this study included participants 18-25 years old who reported\n",
    "smoking at least 1 cigarette per day in the past 30 days (N=1,320).\n",
    "\n",
    "Procedure\n",
    "\n",
    "Data were collected by trained U.S. Census Bureau Field Representatives during 2001–\n",
    "2002 through computer-assisted personal interviews (CAPI). One adult was selected for\n",
    "interview in each household, and interviews were conducted in respondents’ homes\n",
    "following informed consent procedures.\n",
    "\n",
    "Measures\n",
    "\n",
    "Lifetime major depression (i.e. those experienced in the past 12 months and prior to the\n",
    "past 12 months) was assessed using the NIAAA, Alcohol Use Disorder and Associated\n",
    "Disabilities Interview Schedule – DSM-IV (AUDADIS-IV) (Grant et al., 2003; Grant, Harford,\n",
    "Dawson, & Chou, 1995). The tobacco module of the AUDADIS-IV contains detailed\n",
    "questions on the frequency, quantity, and patterning of tobacco use as well as symptom\n",
    "criteria for DSM-IV nicotine dependence. Current smoking was evaluated through both\n",
    "smoking frequency (“About how often did you usually smoke in the past year?”) coded\n",
    "dichotomously to represent presence or absence of daily smoking, and quantity (“On the\n",
    "days that you smoked in the last year, about how many cigarettes did you usually smoke?”),\n",
    "a quantitative variable that ranged from 1 cigarette per day to 98 cigarettes per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring or EDA** will help you augment your data to maximize the potential of your analysis, viz and model. We use visualizations to summarize our data's main characteristics. From there, we remove outliers, create more descriptive features from existing ones (feature engineering.)\n",
    "\n",
    "e.g.\n",
    "- Using summary statistics like `count` on the state column or `mean` on the weight column to see if patients from certain states or of certain weights are more likely to have diabetes, which we can use to exclude certain patients from the analysis and make it less biased.\n",
    "\n",
    "* Create a data dictionary with the column name, data type, range of values and notes on each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA usually involves a combination of the following methods:\n",
    "\n",
    "* **Univariate** visualization of and summary statistics for each field in the raw dataset, specifically for variables of interest\n",
    "* **Bivariate** visualization and summary statistics for assessing the relationship between each variable in the dataset and the target variable of interest (e.g. time until churn, spend)\n",
    "* **Multivariate** visualizations to understand interactions between different fields in the data\n",
    "* Dimensionality reduction to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data\n",
    "* Clustering of similar observations in the dataset into differentiated groupings, which by collapsing the data into a few small data points, patterns of behavior can be more easily identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many observations do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Example observations\n",
    "Then, you'll want to display example observations from the dataset. This will give you a \"feel\" for the values of each feature.\n",
    "\n",
    "Thank about:\n",
    "Do the columns make sense?\n",
    "Do the values in those columns make sense?\n",
    "Are the values on the right scale?\n",
    "Is missing data going to be a big problem based on a quick eyeball test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing your data sets up cleaning your data, which sets up analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at data in it's entirety\n",
    "Goal: (in pandas or sublime text or excel)\n",
    "(getting acquainted with the dataset, getting to know what it's all about and acquiring a mental picture of it, looking at all features or dialing in on specific features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": true
   },
   "source": [
    "#### Is the Data Tidy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tidiness errors to look out for - does the table contain more than 1 observational unit? are there more tables than needed because one observational unit is expressed in multiple tables?\n",
    "\n",
    "* Most data from relational databases will be tidy\n",
    "* Data from spreadsheets or scraped from the web/pdfs might not be\n",
    "* Find data type of each column - continuous, categorical (ordinal or nominal), or date. Fix if needed\n",
    "* Rearrange column order in a sensible manner - categorical first, continuous last. Group common variables together. Melt, Pivot\n",
    "* Spot data quality and data structure issues and any inconsistencies - are there quality issues we can see? are column headers mistakenly values, instead of variable names?\n",
    "\n",
    "\n",
    "1. **Detect:** spot data quality and data structure issues and any inconsistencies - are there quality issues we can see? does it meet the 3 reqs for tidy data? are column headers mistakenly *values*, instead of *variable names*?\n",
    "2. **Document:** makes notes of them\n",
    "  - Assessments should actually not include verbs and instead should only be observations of issues with the data, i.e., \"Nondescriptive column headers\", not \"F# Tidiness - does the table contain more than 1 observational unit? are there more tables than needed because one observational unit is expressed in multiple tables?ix nondescriptive column headers\". Verbs come into \n",
    "  \n",
    "  play when defining cleaning operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Assess Programmatically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using code to view specific portions and summaries of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many observations do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Example observations\n",
    "Then, you'll want to display example observations from the dataset. This will give you a \"feel\" for the values of each feature.\n",
    "\n",
    "Thank about:\n",
    "Do the columns make sense?\n",
    "Do the values in those columns make sense?\n",
    "Are the values on the right scale?\n",
    "Is missing data going to be a big problem based on a quick eyeball test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the data types of my features? Are they numeric? Categorical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all numerical columns are numerical. e.g. sometimes they maybe represented as:\n",
    "term                     36 months\n",
    "emp_length               10+ years\n",
    "revol_util                   83.7%\n",
    "\n",
    "Convert these to numerical values.\n",
    "\n",
    "Same for dates:\n",
    "pull_d        Sep-2016\n",
    "\n",
    "Convert these to datetime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) See if any columns are missing values\n",
    "\n",
    "# 2) Make sure every column's datatype is appropriate - obect, int, float, categorical, datetime etc. It's important to get them right to use pandas calculation/summary functions for that datatype.\n",
    "## If a column has a finite set of values it should be argued to be of categorical type, unless it has a lot of finite values. Categorical data with tons of categories isn't that useful.\n",
    "## Say you had one to a few observations from each country, it would probably be best to treat country like a string and group observations on a larger unit, like world_region (Africa, Asia, Central America, etc.). If you had a lot of observations from a few countries, like test scores from students sampled in a handful of countries, making country categorical would be more appropriate.\n",
    "\n",
    "# 3) make sure column names are descriptive and error free (may want to remove spaces and special chars from col names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names, check for spaces, capitalization inconsistencies\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I have a target variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Other Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Convert all column names in dataframe to lower/uppercase etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.columns = map(str.upper, data.columns)\n",
    "data.columns = map(str.lower, data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Rename columns, assign names to 'Unnamed' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {df.columns[0]:'ID'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Remove features that'll not be available when making predictions on a new datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "e.g. In predicting default rates, the depdendent variable/label is for e.g. 'Defaultin2yrs'. There are also variables like 'NumberOfTime30_59DaysPastDueNotW', 'NumberOfTime60_89DaysPastDueNotW', 'NumberOfTimes90DaysLate'. These variables give info on how much customers were delayed in payment and frequency. In Financial Industry, these types of variables are the inputs for creating the dependent variable. Hence, these variables cannot be used as independent variables.\n",
    "\n",
    "Moreover, the use of this model is to score a new customer and obviously these variables will not be available for a new customer. Hence, remove these variables straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 4, 8, 10 are indexes of columns to remove\n",
    "df.drop(df.columns[[4, 8, 10]], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Univariate Analysis\n",
    "* Look at one variable at a time.\n",
    "* Summarize and examize\n",
    "\n",
    "Distribution of a variable:\n",
    "* What values the variable takes\n",
    "* How often the variable takes those values\n",
    "\n",
    "### Categorical variables\n",
    "* There is less available options with categorical variables\n",
    "* Count the frequency of each variable\n",
    "* Low frequency strings might be outliers\n",
    "* You might want to relabel low frequency strings 'other'\n",
    "* Find the number of unique labels for each column\n",
    "* In pandas, change the data type to categorical (better when there aren't too many unique values)\n",
    "* Bar plots of counts\n",
    "* String columns allow for feature engineering by splitting the string, counting certain letters, finding the length of, etc... Feature engineering can be done later when modeling\n",
    "* See if a numerical/datetime variable has a string type incorrectly because there is an 'NA'/'missing' value or '%/$' other sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an int value is being treated as a string value by pandas,\n",
    "# use this to convert the variable to numeric before calling the value_counts function\n",
    "# Do this for each column you run value_counts on\n",
    "df[col_name] = pandas.to_numeric(df[col_name], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times a value occurs\n",
    "# Method 1\n",
    "df.col_name.value_counts(dropna=False)\n",
    "# Method 2\n",
    "df.groupby(col_name).size()\n",
    "\n",
    "# The percentage of how much the value occurs\n",
    "# Method 1\n",
    "df.col_name.value_counts(dropna=False, normalize=True)\n",
    "# Method 2\n",
    "df.groupby(col_name).size() * 100 / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "df[col_name].value_counts(normalize=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Way Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"wrangling-notebook-1.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Way Frequency Table\n",
    "freq_table = pd.crosstab(index=df[\"Symbol\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of each value\n",
    "\n",
    "# Method 1\n",
    "freq_table = pd.crosstab(index=df[\"payer_name\"],  # Make a crosstab\n",
    "                              columns=\"count\", normalize=True)\n",
    "# value_counts() excludes NA values, so numbers might not add up to 1 if there are missing values. This is okay because we care about the % of each value to take the missing values in account.\n",
    "\n",
    "# Method 2\n",
    "prop = freq_table / freq_table.sum()\n",
    "prop = prop.sort_values('count', ascending=False)\n",
    "\n",
    "# Show top 10 values (by proportion) in column\n",
    "prop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to combine both the above cells and get freq table and proportions in one table\n",
    "# Claims By Payer Name\n",
    "freq_table = pd.crosstab(index=df[\"payer_name\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "freq_table_per = pd.crosstab(index=df[\"payer_name\"],  # Make a crosstab\n",
    "                              columns=\"percentage\", normalize=True)\n",
    "freq_table['percentage'] = freq_table_per['percentage']\n",
    "freq_table.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Two Way Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"wrangling-notebook-3.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-4.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Higher Dimensional Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"wrangling-notebook-6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Higher DImensional Frequency Table\n",
    "surv_sex_class = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                             columns=[titanic_train[\"Pclass\"],\n",
    "                                      titanic_train[\"Sex\"]],\n",
    "                             margins=True)\n",
    "surv_sex_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Proportions of each value\n",
    "prop = surv_sex_class/surv_sex_class.ix[\"All\"]    # Divide by column totals\n",
    "\n",
    "prop = prop.sort_values('count', ascending=False)\n",
    "\n",
    "# Show top 10 values (by proportion) in column\n",
    "prop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wrangling Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### If a variable has too many response categories, collapse response categories. E.g. dates -> week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### If you are researching different kinds of brand use (chanel, forever21, ted baker etc). One question in your dataset was asked about each substance.\n",
    "If you wanted to see which participants used at least one of the brands that you are studying, aggregate the brand variables into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Give your variables values that make more sense\n",
    "Useful to convert categorical variables to numerical ones. E.g.\n",
    "\n",
    "Categorical time period to number of days\n",
    "+ 1 - once a month\n",
    "+ 2 - 3-4 times a week\n",
    "+ 3 - twice a week\n",
    "+ 4 - once a week\n",
    "+ 5 - 2-3 times a month\n",
    "+ 6 - once a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# e.g.  Variables might be coded form 1 - 6, where 1 is highest frequency and 6 is lowest.\n",
    "# In this case we could reverse the assigned codes so higher frequencies/values have larger codes.\n",
    "#recoding values for S3AQ3B1 into a new variable, USFREQ\n",
    "recode1 = {1: 6, 2: 5, 3: 4, 4: 3, 5: 2, 6: 1}\n",
    "sub2['USFREQ']= sub2['S3AQ3B1'].map(recode1)\n",
    "\n",
    "# e.g. convert categorical variables to numerical ones\n",
    "# 1 - once a month\n",
    "# 2 - 3-4 times a week\n",
    "# 3 - twice a week\n",
    "# 4 - once a week\n",
    "# 5 - 2-3 times a month\n",
    "# 6 - once a month\n",
    "#recoding values for S3AQ3B1 into a new variable, USFREQMO\n",
    "recode2 = {1: 30, 2: 22, 3: 14, 4: 5, 5: 2.5, 6: 1}\n",
    "sub2['USFREQMO']= sub2['S3AQ3B1'].map(recode2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create secondary variables\n",
    "Created by doing an arithmetic or logical operation on 2 existing primary variables\n",
    "e.g. X per Time, Number per month = Number per Day * Days per month\n",
    "\n",
    "To combine more than 2 variables:\n",
    "e.g. Get multiracial from num_asian + num_caucasian + num_black etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub2['NUMCIGMO_EST']=sub2['USFREQMO'] * sub2['S3AQ3C1']\n",
    "sub2['NUMCIGMO_EST']=sub2['USFREQMO'] / sub2['S3AQ3C1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Group/Bin values with individual variables\n",
    "(quantitative and categorical both)\n",
    "e.g. age from numerical to categorical, time from days to weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# e.g. quartile split (use qcut function & ask for 4 groups - gives you quartile split)\n",
    "# print 'AGE - 4 categories - quartiles'\n",
    "sub2['AGEGROUP4']=pandas.qcut(sub2.AGE, 4, labels=[\"1=0%tile\",\"2=25%tile\",\"3=50%tile\",\"4=75%tile\"])\n",
    "c4 = sub2['AGEGROUP4'].value_counts(sort=False, dropna=True)\n",
    "\n",
    "# Custom split\n",
    "# categorize quantitative variable based on customized splits using cut function\n",
    "# splits into 3 groups (18-20, 21-22, 23-25) - remember that Python starts counting from 0, not 1\n",
    "sub2['AGEGROUP3'] = pandas.cut(sub2.AGE, [17, 20, 22, 25])\n",
    "c5 = sub2['AGEGROUP3'].value_counts(sort=False, dropna=True)\n",
    "print(c5)\n",
    "\n",
    "# Convert newly created categorical variable to type categorical\n",
    "# df.state = df.state.astype('category')\n",
    "\n",
    "# crosstabs evaluating which ages were put into which AGEGROUP3\n",
    "# Crosstabs function - Compute a simple cross-tabulation of two (or more) factors.\n",
    "## By default computes a frequency table of the factors unless an array of values and an aggregation function are passed\n",
    "print (pandas.crosstab(sub2['AGEGROUP3'], sub2['AGE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variables\n",
    "* There are a lot more options for continuous variables\n",
    "* Use the five number summary - with **`.describe`**\n",
    "* Boxplots are great ways to find outliers\n",
    "* Use histograms and kernel density estimators to visualize the distribution.\n",
    "* Know the shape of the distribution\n",
    "* Think about making categorical variables out of continuous variables by cutting them into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the min, max values make sense? Do the mean, std and other values seem what you would expect or does anything stand out about them?\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a dist plot (also a box and whiskers plot)\n",
    "sns.distplot(df[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Outliers section for how to find and deal with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.col_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various methods of indexing and selecting data (.loc and bracket notation with/without boolean indexing, also .iloc)\n",
    "df.loc[df['city'] == \"New York\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate weird values, e.g.\n",
    "weight_lbs = patients[patients.surname == 'Zaitseva'].weight * 2.20462\n",
    "height_in = patients[patients.surname == 'Zaitseva'].height\n",
    "bmi_check = 703 * weight_lbs / (height_in * height_in)\n",
    "bmi_check\n",
    "patients[patients.surname == 'Zaitseva'].bmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Use bootstrapping to get more 'samples'\n",
    "* Bootstrapping is done by resampling your data with replacement and gives you a 'new' random dataset\n",
    "* This helps you get multiple looks at the data\n",
    "* You can get estimates for the mean and variance of continuous columns this way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total missing values do we have?\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['col_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values exist. Missing values may exist as dashes, slases, zeroes, NA, or none. Account for all of these. Also 0 and null are different because they lead to diff values of std, variance etc.)\n",
    "sum(df.col_name.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recover missing data from context\n",
    "E.g. People who answer no to 'do you drink', will bave missing values for 'how many glasses do you drink'.\n",
    "We know from context that this value should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['how_many'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this value missing becuase it wasn't recorded or becuase it dosen't exist?\n",
    "\n",
    "This is the point at which we get into the part of data science that I like to call \"data intution\", by which I mean \"really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis\". It can be a frustrating part of data science, especially if you're newer to the field and don't have a lot of experience. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:\n",
    "\n",
    "Is this value missing becuase it wasn't recorded or becuase it dosen't exist?\n",
    "\n",
    "If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\" and we'll learn how to do it next! :)\n",
    "\n",
    "This is a great place to read over the dataset documentation if you haven't already! If you're working with a dataset that you've gotten from another person, you can also try reaching out to them to get more information.\n",
    "\n",
    "If you're doing very careful data analysis, this is the point at which you'd look at each column individually to figure out the best strategy for filling those missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace any missing values in variables with NaN (Can be expressed as 0, NA, not answered, don't know, refuse to answer, blank etc)\n",
    "You might have response categories that provide additional information but don't help you answer your research question. For e.g. Unknowns, NAs.\n",
    "Example '9' for non-answers.\n",
    "You can replace these values with NaN.\n",
    "\n",
    "Examine each variable and make sure each of the values represent a valid category. Convert any 0s, NAs, not answered, blank values, don't know, refuse to answer etc. to NaNs and ensure the rest of the data is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name']=df['col_name'].replace(outlier, numpy.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "* Use your natural human ability to look at boxplots to find thresholds for what an outlier might be\n",
    "* Generate a new column of data that is 0/1 for outlier or not. This will quickly help you find them later.\n",
    "\n",
    "\n",
    "Outliers can cause problems with certain types of models. For example, linear regression models are less robust to outliers than decision tree models.\n",
    "\n",
    "In general, if you have a legitimate reason to remove an outlier, it will help your model’s performance.\n",
    "\n",
    "However, **outliers are innocent until proven guilty**. You should never remove an outlier just because it’s a \"big number.\" That big number could be very informative for your model.\n",
    "\n",
    "You must have a good reason for removing an outlier, such as suspicious measurements that are unlikely to be real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IQR Method:**\n",
    "A commonly used rule says that a data point is an outlier if it is more than 1.5 . IQR above the third quartile or below the first quartile.\n",
    "Said differently, low outliers are < Q1 - (1.5 . IQR) and high outliers are > Q3 + (1.5 . IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at df.describe()\n",
    "# if min/max was too much in describe(), see how many values are > or < than a reasonable_value\n",
    "# reasonable_value can be where the long tail starts\n",
    "len(df[(df[col_name]>reasonable_value)])\n",
    "\n",
    "\n",
    "# We can cap the longtail (if the number of outliers isn't huge) to a reasonable_value, and in addition use imputation to handle missing values\n",
    "\n",
    "\n",
    "## If there's a lot of out of range values, it is not appropriate to consider all these as outliers and cap to reasonable_value. A better approach is to make these missing and impute the values\n",
    "# Set out of range values to missing\n",
    "df[col_name] = df[col_name].map(lambda x: np.NaN if x > reasonable_value else x)\n",
    "\n",
    "# Test\n",
    "df[col_name].describe()\n",
    "sns.distplot(df[col_name])\n",
    "\n",
    "# Impute\n",
    "# For imputation, use ffill method which will retain the distribution and mean of the variable.\n",
    "# We use ffill since the proportion of missing is large, imputation using mean is not appropriate as it will change the distribution too much.\n",
    "df[col_name].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Test\n",
    "df[col_name].describe()\n",
    "sns.distplot(df[col_name])\n",
    "\n",
    "\n",
    "## If there are not a lot of out of range values, it is ok to cap the values\n",
    "df.loc[df[col_name]>reasonable_value, col_name]=reasonable_value\n",
    "# or\n",
    "df.loc[df[col_name]<reasonable_value, col_name]=reasonable_value\n",
    "\n",
    "# Test\n",
    "df[col_name].describe()\n",
    "sns.distplot(df[col_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Z-score Method **\n",
    "\n",
    "The intuition behind the Z-score method of outlier detection is that, once we’ve centred and rescaled the data, anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier.\n",
    "\n",
    "The Z-score method relies on the mean and standard deviation of a group of data to measure central tendency and dispersion. This is troublesome, because the mean and standard deviation are highly affected by outliers – they are not robust. In fact, the skewing that outliers bring is one of the biggest reasons for finding and removing outliers from a dataset!\n",
    "\n",
    "So we use the modified Z-score method is that it uses the median and MAD rather than the mean and standard deviation. The median and MAD are robust measures of central tendency and dispersion, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.sort_values()\n",
    "# See if the min or max values are too far away from the avg value (this could be inaccurate or this could be because of a difference in units: one value in KGs while the others are in LBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all numerical columns\n",
    "df_num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_num\n",
    "\n",
    "# Calculate modified z_score\n",
    "for col in df_num:\n",
    "    df_outliers = df[(np.abs(df[col]-df[col].median()) > (3.5*df[col].mad()))]\n",
    "    \n",
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_modified_z_score(ys):\n",
    "    threshold = 3.5\n",
    "\n",
    "    median_y = np.median(ys)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in ys])\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y\n",
    "                         for y in ys]\n",
    "    return np.where(np.abs(modified_z_scores) > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another robust measure of dispersion.\n",
    "\n",
    "PS: One caveat: both of these methods will encounter problems with a strongly skewed dataset. If the data is distributed in a strongly asymmetrical fashion, it will need to be re-expressed before applying any of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others Ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if the min or max values are too far away from the avg value (this could be inaccurate or this could be because of a difference in units: one value in KGs while the others are in LBs)\n",
    "df.col_name.sort_values()\n",
    "\n",
    "# Fetch outliers\n",
    "very_large_value = 100000000\n",
    "df[df.col_name > very_large_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[np.abs(df.Data-df.Data.mean()) <= (3*df.Data.std())]\n",
    "# keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\n",
    "\n",
    "df[~(np.abs(df.Data-df.Data.mean()) > (3*df.Data.std()))]\n",
    "# or if you prefer the other way around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper/Lower bounded values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes values are cut off at an arbitrary high or low value and therefore, the first and last value have a lot of datapoints. Check for this. This is very important to remember when assessing the generalizability of models trained on this later. Might want to mention this as part of dataset insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Duplicated data\n",
    "* Lots of data gets accidentally duplicated. Check for duplicates or near duplicates of rows and columns\n",
    "* If any columns are calculated entirely by that of another column or columns (like with depth from the diamonds data), ensure the calculation holds. \n",
    "\n",
    "Duplicate observations most frequently arise during data collection, such as when you:\n",
    "* Combine datasets from multiple places\n",
    "* Scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if there's any duplicate values(cause of nicknames/misspellings/multiple ways of expressing a value), or default 'John Doe' type values\n",
    "df[df.col_name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated columns\n",
    "# See if there are a lot of duplicated columns across tables which means that the tables could be combined. Ideally only the id column(s) should be common across tables\n",
    "all_columns = pd.Series(list(first) + list(second) + list(third))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional:) Remove Columns with only One Value\n",
    "These columns won't be useful for the model since they don't add any information to each loan application. In addition, removing these columns will reduce the number of columns we'll need to explore further in the next stage.\n",
    "\n",
    "The pandas Series method nunique() returns the number of unique values, excluding any null values. We can use apply this method across the dataset to remove these columns in one easy step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,df.apply(pd.Series.nunique) != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional:) Remove Columns with unique values that don't occur more than N times\n",
    "there may be some columns with more than one unique values but one of the values has insignificant frequency in the dataset. Let's find out and drop such column(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a good value for n here\n",
    "n = 4\n",
    "for col in df.columns:\n",
    "    if (len(df[col].unique()) < n):\n",
    "        print(df[col].value_counts())\n",
    "        print()\n",
    "df = df.drop('col_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Irrelevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Irrelevant observations are those that don’t actually fit the specific problem that you’re trying to solve.\n",
    "\n",
    "For example, if you were building a model for Single-Family homes only, you wouldn't want observations for Apartments in there.\n",
    "This is also a great time to review your charts from Exploratory Analysis. You can look at the distribution charts for categorical features to see if there are any classes that shouldn’t be there.\n",
    "Checking for irrelevant observations before engineering features can save you many headaches down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making new binary columns to label some finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Just like it was described above to make a 0/1 column for outliers, you can do the same for any other finding\n",
    "* You can drop the duplicated rows or you can make a binary column labeling them. \n",
    "* Same for rows that do not have a correct calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Typos, Inconsistent Capitalization in Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/typos-example-before.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Check for mislabeled classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "i.e. separate classes that should really be the same.\n",
    "\n",
    "e.g. If ’N/A’ and ’Not Applicable’ appear as two separate classes, you should combine them.\n",
    "e.g. ’IT’ and ’information_technology’ should be a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Standardization - Formatting Issues in other variable types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Standardize all data types - zipcodes, emails, phone numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Transformation\n",
    "OHE\n",
    "Log distribution transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Numerical Variables\n",
    "Often, a quick and dirty grid of histograms is enough to understand the distributions\n",
    "\n",
    "Here are a few things to look out for:\n",
    "\n",
    "- Distributions that are unexpected\n",
    "- Potential outliers that don't make sense\n",
    "- Features that should be binary (i.e. \"wannabe indicator variables\")\n",
    "- Boundaries that don't make sense\n",
    "- Potential measurement errors\n",
    "\n",
    "PS: When there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly. In such cases, it's good to look at the plot on a **log scale**. The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/histogram-grid-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Categorical Variables\n",
    "\n",
    "Caterogical variables can be visualized using bar plots.\n",
    "\n",
    "In particular, you'll want to look out for sparse classes, which are classes that have a very small number of observations.\n",
    "\n",
    "A \"class\" is simply a unique value for a categorical feature.CAterogical variables can be visualizing\n",
    "\n",
    "Classes with short bars are sparse classes. They tend to be problematic when building models.\n",
    "\n",
    "In the best case, they don't influence the model much.\n",
    "In the worse case, they can cause the model to be overfit.\n",
    "Therefore, make a note to combine or reassign some of these classes later.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/grouping-sparse-classes-before.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Segmentations - Categorical + Numerical Variables Together\n",
    "\n",
    "Segmentations are powerful ways to observe the relationship between categorical features and numeric features.\n",
    "\n",
    "Box plots allow you to do so.\n",
    "\n",
    "Here are a few insights you could draw from the following chart.\n",
    "\n",
    "The median transaction price (middle vertical bar in the box) for Single-Family homes was much higher than that for Apartments / Condos / Townhomes.\n",
    "The min and max transaction prices are comparable between the two classes.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/boxplot-segmentation-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": false
   },
   "source": [
    "### Univariate vs Bivariate and Graphical vs Non-Graphical\n",
    "\n",
    "| Univariate             | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical | Bar char of frequencies (count / percent) | Frequency/Contingency table (count / percent) |\n",
    "| Num/Continuous  | Histogram/rugplot/KDE, box/violin/swarm, qqplot, fat tails  | central tendency -mean/median/mode, spread - variance, std, skew, kurt, IQR  |\n",
    "\n",
    "| Bivariate/multivariate            | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical vs Categorical | heat map, mosaic plot | Two-way Contingency table (count/percent) |\n",
    "| Continuous vs Continuous  | all pairwise scatterplots, kde, heatmaps |  all pairwise correlation/regression   |\n",
    "| Categorical vs Continuous  | [bar, violin, swarm, point, strip seaborn plots](http://seaborn.pydata.org/tutorial/categorical.html)  | Summary statistics for each level |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Types of distributions\n",
    "\n",
    "Described their their:\n",
    "+ Shape\n",
    "+ Center\n",
    "+ Spread\n",
    "\n",
    "**Shape:**\n",
    "<img src=\"wrangling-notebook-7.png\">\n",
    "          \n",
    "All of these are symmetric, but differ in their peakness or modality (unimodal, bimodal, uniform).\n",
    "\n",
    "Modes - values that occur most often.\n",
    "The mode doesn't always occur at the center.\n",
    "The center is the midpoint, the value that divides the distribution so that half of the values take smaller values and the other half take larger values.\n",
    "Spread - approx range covered by the distribution. Measured by range.\n",
    "\n",
    "Distributions can be both left/right kewed and have more than one mode.\n",
    "<img src=\"wrangling-notebook-8.png\">\n",
    "\n",
    "The histogram can give us the shape, but the center and spread are harder to determine from the histrogram.\n",
    "\n",
    "**Center:**\n",
    "* Mean: Average = sum of values/n\n",
    "* Median: Write the values in ascending order. Median is the middle value or the avg of the middle 2 values.\n",
    "* Mode: Value with the highest frequency\n",
    "\n",
    "<img src=\"wrangling-notebook-9.png\">\n",
    "These distributions have the same center. So spread alone isn't enough to describe a distribution. We also need to talk about the spreads which are different for each distribution above.\n",
    "\n",
    "**Spread:**\n",
    "* **Standard Deviation** - Typical/avg difference between the observations and the mean. Sqrt(variance). We take the sqrt inorder to bring the units back to units (we squared them while calculating variance)\n",
    "* **Variance** - |(x - x^)^2|/(n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bivariate and Multivariate EDA\n",
    "\n",
    "Boxplots are great when you have a numeric column that you want to compare across different categories.\n",
    "When you want to visualize two numeric columns, scatter plots are ideal.\n",
    "\n",
    "### Categorical vs Categorical\n",
    "* Create two way contingency table of frequency counts\n",
    "* Create a heat map\n",
    "* Find expected counts and possibly do a chi-squared test\n",
    "\n",
    "### Categorical vs Continuous\n",
    "* Use the seaborn categorical plots\n",
    "\n",
    "### Continuous vs Continuous\n",
    "* Plot all combinations of scatterplots\n",
    "* Use a hierarchical clustering plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In bivariate analysis, there's a dependent and an independent variable.\n",
    "\n",
    "** X is a predictor of Y. **\n",
    "\n",
    "<img src=\"wrangling-notebook-10.png\">\n",
    "\n",
    "Determine which variable plays which role in our analysis.\n",
    "\n",
    "Use this classification system to determine which plots and statistical tools to use:\n",
    "\n",
    "<img src=\"wrangling-notebook-12.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-11.png\">\n",
    "\n",
    "<img src=\"** Visualizing_Flowchart.png\">\n",
    "\n",
    "** Types of relationships shown in scatter plots **\n",
    "<img src=\"wrangling-notebook-13.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-14.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Correlations\n",
    "\n",
    "In general, you should look out for:\n",
    "\n",
    "Which features are strongly correlated with the target variable?\n",
    "Are there interesting or unexpected strong correlations between other features?\n",
    "\n",
    "Note:\n",
    "Correlation is a value between -1 and 1 that represents how closely two features move in unison. You don't need to remember the math to calculate them. Just know the following intuition:\n",
    "\n",
    "Positive correlation means that as one feature increases, the other increases. E.g. a child’s age and her height.\n",
    "Negative correlation means that as one feature increases, the other decreases. E.g. hours spent studying and number of parties attended.\n",
    "Correlations near -1 or 1 indicate a strong relationship.\n",
    "Those closer to 0 indicate a weak relationship.\n",
    "0 indicates no relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assessments From EDA To Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Quality\n",
    "##### `patients` table\n",
    "- Zip code is a float not a string\n",
    "- Zip code has four digits sometimes\n",
    "- Tim Neudorf height is 27 in instead of 72 in\n",
    "- Full state names sometimes, abbreviations other times\n",
    "- Dsvid Gustafsson\n",
    "- Missing demographic information (address - contact columns) ***(can't clean yet)***\n",
    "- Erroneous datatypes (assigned sex, state, zip_code, and birthdate columns)\n",
    "- Multiple phone number formats\n",
    "- Default John Doe data\n",
    "- Multiple records for Jakobsen, Gersten, Taylor\n",
    "- kgs instead of lbs for Zaitseva weight\n",
    "\n",
    "##### `treatments` table\n",
    "- Missing HbA1c changes\n",
    "- The letter 'u' in starting and ending doses for Auralin and Novodra\n",
    "- Lowercase given names and surnames\n",
    "- Missing records (280 instead of 350)\n",
    "- Erroneous datatypes (auralin and novodra columns)\n",
    "- Inaccurate HbA1c changes (leading 4s mistaken as 9s)\n",
    "- Nulls represented as dashes (-) in auralin and novodra columns\n",
    "\n",
    "##### `adverse_reactions` table\n",
    "- Lowercase given names and surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Tidiness\n",
    "- Contact column in `patients` table should be split into phone number and email\n",
    "- Three variables in two columns in `treatments` table (treatment, start dose and end dose)\n",
    "- Adverse reaction should be part of the `treatments` table\n",
    "- Given name and surname columns in `patients` table duplicated in `treatments` and `adverse_reactions` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "<img src=\"wrangling-notebook-15.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P values\n",
    "\n",
    "<img src=\"Mind-Map-p-values.png\">\n",
    "\n",
    "<img src=\"Mind-Map-p-values-scale.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ANOVA Test\n",
    "Categorical → Quantitative\n",
    "\n",
    "- Goal: A (C) is correlated to the level of B (Q) i.e. Examine the differences in the means of B across the categories of A.\n",
    "- Null Hypothesis: The means across the categories of A are equal.\n",
    "- Alternate Hypothesis: The means across categories of A are ≠.\n",
    "- There are many ways for the population means to not be equal.\n",
    "\n",
    "<img src=\"wrangling-notebook-16.png\">\n",
    "\n",
    "<img src=\"Mind-Map-ANOVA-scale.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi \n",
    "\n",
    "data = pandas.read_csv('nesarc.csv', low_memory=False)\n",
    "\n",
    "#setting variables you will be working with to numeric\n",
    "data['S3AQ3B1'] = pandas.to_numeric(data['S3AQ3B1'], errors='coerce')\n",
    "data['S3AQ3C1'] = pandas.to_numeric(data['S3AQ3C1'], errors='coerce')\n",
    "data['CHECK321'] = pandas.to_numeric(data['CHECK321'], errors='coerce')\n",
    "\n",
    "#subset data to young adults age 18 to 25 who have smoked in the past 12 months\n",
    "sub1=data[(data['AGE']>=18) & (data['AGE']<=25) & (data['CHECK321']==1)]\n",
    "\n",
    "#SETTING MISSING DATA\n",
    "sub1['S3AQ3B1']=sub1['S3AQ3B1'].replace(9, numpy.nan)\n",
    "sub1['S3AQ3C1']=sub1['S3AQ3C1'].replace(99, numpy.nan)\n",
    "\n",
    "# ADDITIONAL DATA WRANGLING\n",
    "#recoding number of days smoked in the past month\n",
    "recode1 = {1: 30, 2: 22, 3: 14, 4: 5, 5: 2.5, 6: 1}\n",
    "sub1['USFREQMO']= sub1['S3AQ3B1'].map(recode1)\n",
    "\n",
    "#converting new variable USFREQMMO to numeric\n",
    "data['USFREQMO'] = pandas.to_numeric(data['USFREQMO'], errors='coerce')\n",
    "\n",
    "# Creating a secondary variable multiplying the days smoked/month and the number of cig/per day\n",
    "sub1['NUMCIGMO_EST']=sub1['USFREQMO'] * sub1['S3AQ3C1']\n",
    "\n",
    "data['NUMCIGMO_EST'] = pandas.to_numeric(data['NUMCIGMO_EST'], errors='coerce')\n",
    "\n",
    "ct1 = sub1.groupby('NUMCIGMO_EST').size()\n",
    "print (ct1)\n",
    "\n",
    "\n",
    "# ANOVA Testing\n",
    "# ols - ordinary least squares\n",
    "# using ols function for calculating the F-statistic and associated p value\n",
    "# Calculate differences in the mean num of response var B (NUMCIGMO_EST), for cases with and without explanatory var A (MAJORDEPLIFE)\n",
    "# C() indicates categorical variable\n",
    "\n",
    "# ---- For categorical variables with 2 Groups\n",
    "sub1 = df\n",
    "model1 = smf.ols(formula='NUMCIGMO_EST ~ C(MAJORDEPLIFE)', data=sub1)\n",
    "results1 = model1.fit()\n",
    "# Generates F-statistic and p-value which is called Prob (F-statistic) in the output\n",
    "# We look at the P-value here to determine if correlation exists or not (< 0.5 correlation exists, otherwise no)\n",
    "print (results1.summary())\n",
    "\n",
    "# Create a subset of the dataset and calculate the means of B, dropping na to get only non-null values, because the OLS analysis also includes only non-null values\n",
    "# We get the means and std to say that these means are statistically equal or not (depending on the p-value)\n",
    "# If p-value < 0.5, by looking at the means tables, we can see which category (presence of A or its absence) resulted in more B.\n",
    "\n",
    "sub2 = sub1[['NUMCIGMO_EST', 'MAJORDEPLIFE']].dropna()\n",
    "\n",
    "print ('means for numcigmo_est by major depression status')\n",
    "m1= sub2.groupby('MAJORDEPLIFE').mean()\n",
    "print (m1)\n",
    "\n",
    "print ('standard deviations for numcigmo_est by major depression status')\n",
    "sd1 = sub2.groupby('MAJORDEPLIFE').std()\n",
    "print (sd1)\n",
    "\n",
    "# ---- For categorical variables with more than 2 Groups\n",
    "sub3 = sub1[['NUMCIGMO_EST', 'ETHRACE2A']].dropna()\n",
    "\n",
    "model2 = smf.ols(formula='NUMCIGMO_EST ~ C(ETHRACE2A)', data=sub3).fit()\n",
    "print (model2.summary())\n",
    "\n",
    "print ('means for numcigmo_est by major depression status')\n",
    "m2= sub3.groupby('ETHRACE2A').mean()\n",
    "print (m2)\n",
    "\n",
    "print ('standard deviations for numcigmo_est by major depression status')\n",
    "sd2 = sub3.groupby('ETHRACE2A').std()\n",
    "print (sd2)\n",
    "\n",
    "# Post-Hoc Tests\n",
    "# Help you figure out which categories are similar in dependence\n",
    "# F-test and p-value don't provide insight into why the null hypothesis can be rejected because there are multiple levels to the categorical explanatory variable\n",
    "# They do not tell us in what way the population means are not statistically equal.\n",
    "# There are many ways for population means not to be all equal.\n",
    "# Maybe they are not all equal to each other. Maybe only 2 of the population means are not equal to each other, but the rest are. \n",
    "\n",
    "# ANOVA doesn't tell us which groups are different. To determine that, we need or perform a post-hoc test (conducts post-hoc paired comparisons)\n",
    "# Performs paired post-hoc ANOVA testing in a way to prevent Type One Error (rejecting the null hypothesis when the null hypothesis is true)\n",
    "# We can't manually pair different categories and do ANOVA testing on them because there's a ahcnace of making a 5% error on each analysis that we conduct (cause our p value ceiling is 0.05). Our overall chance of committing type 1 error can be far > 5%. For e,g, after 15 tests, it is upto 54%. This error rate for group pair comparison is called the family wise error rate.\n",
    "# Post-hoc tests are designed to evaluate the difference between pairs of means while protecting against inflation of Type 1 errors.\n",
    "# There's a lot of post-hoc tests for ANOVA to choose from, they differ in their conservativeness of preventing type 1 error inflation.\n",
    "# We use Tukey's Honestly Significant Difference Test\n",
    "# multi.MultiComparison(quantitative response var, categorical explanatory var)\n",
    "# the last column (reject) shows pairs in which we can reject the null hypothesis. True = there is difference in B for these different categories of A.\n",
    "# Type 1 Error: When we think there is correlation, when there's actually no correlation.\n",
    "mc1 = multi.MultiComparison(sub3['NUMCIGMO_EST'], sub3['ETHRACE2A'])\n",
    "res1 = mc1.tukeyhsd()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example Explanation of ANOVA results from code above:\n",
    "\n",
    "**Model Interpretation for ANOVA:**\n",
    "\n",
    "When examining the association between current number of B (quantitative response) and A (categorical explanatory), an Analysis of Variance (ANOVA) revealed that among daily, B of type T (my sample), those with A reported significantly more B (Mean=14.6, s.d. ±9.15) compared to those without A (Mean=11.4, s.d. ±7.43), F(1, 1313)=44.68, p<0001.\n",
    "\n",
    "Note that the degrees of freedom that I report in parentheses) following ‘F’ can be found in the OLS table as the DF model and DF residuals. In this example 44.68 is the actual F value from the OLS table and we commonly report a very small p value as simply <.0001.\n",
    "\n",
    "**Model Interpretation for post hoc ANOVA results:**\n",
    "\n",
    "ANOVA revealed that among daily, B of type T (my sample), A (collapsed into 5 ordered categories, which is the categorical explanatory variable) and B (quantitative response variable) were significantly associated, F (4, 1308)=11.79, p=0001. Post hoc comparisons of mean number of A by pairs of B per day categories revealed that those individuals with B more than 10 per day (i.e. 11 to 15, 16 to 20 and >20) reported significantly more A compared to those with B of 10 or fewer per day (i.e. 1 to 5 and 6 to 10). All other comparisons were statistically similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Chi-Squared Test\n",
    "\n",
    "- Categorical explanatory → Categorical response vars\n",
    "- Can be used for 2 or more categories.\n",
    "- Step 1: Which variable plays the role of the explanatory variable\n",
    "- Step 2: Calculate conditional percentages separately (% of the response variable for each value of the explanatory variable).\n",
    "- Step 3: Is it statistically significant?\n",
    "  - Null Hypothesis: No relationship between the 2 categorical variables, independent e.g. genders and drinking (difference is due to sample variability).\n",
    "  - Alternate Hypothesis: Correlation between the 2 categorical variables, not indepedent e.g. genders and drinking (difference is due to gender).\n",
    "- The idea behind the Chi-squared test of independence is to measure how far the data is from what is claimed in the null hypothesis.\n",
    "- We'll have 2 sets of counts:\n",
    "  - **The Observed Counts**: The data as is\n",
    "  - **The Expected Counts**: To represent data if there was no correlation between A and B, and the null hypothesis were true.\n",
    "    - Calculate the counts we'd expect to see if both the explanatory variable and response variable were independent, and the null hypothesis is true.\n",
    "    - P(A and B) = P(A) * P(B) (given A & B are independent - which we are assuming here)\n",
    "    - This is the probability of one instance of A and B.\n",
    "    - The probability of A and B happening N times is = N*P(A and B)\n",
    "    - Measure how far the observed counts are from the expected counts.\n",
    "  - We'll base our decision on the size of the discrepancy between what we observe and what we would expect to observe if the null hypothesis was true.\n",
    "- The Chi-Squared test tells us how far the observed data is from the expected data, if the null hypothesis was true.\n",
    "  - For a 2 by 2, the cutoff limit is 3.84. Chi-square score > 3.84 is considered large.\n",
    "  - For cases other than 2 by 2s, the cutoff limit is determined by the null distribution in that case.\n",
    "  - Therefore, we don't use the chi-square statistic, and only rely on the p-value.\n",
    "  - The p-value of a chi-squared test is the probability of observing a chi-square score at least as large as the one observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pandas.read_csv('nesarc.csv', low_memory=False)\n",
    "\n",
    "# new code setting variables you will be working with to numeric\n",
    "data['TAB12MDX'] = pandas.to_numeric(data['TAB12MDX'], errors='coerce')\n",
    "data['CHECK321'] = pandas.to_numeric(data['CHECK321'], errors='coerce')\n",
    "data['S3AQ3B1'] = pandas.to_numeric(data['S3AQ3B1'], errors='coerce')\n",
    "data['S3AQ3C1'] = pandas.to_numeric(data['S3AQ3C1'], errors='coerce')\n",
    "data['AGE'] = pandas.to_numeric(data['AGE'], errors='coerce')\n",
    "\n",
    "#subset data to young adults age 18 to 25 who have smoked in the past 12 months\n",
    "sub1=data[(data['AGE']>=18) & (data['AGE']<=25) & (data['CHECK321']==1)]\n",
    "\n",
    "#make a copy of my new subsetted data\n",
    "sub2 = sub1.copy()\n",
    "\n",
    "# recode missing values to python missing (NaN)\n",
    "sub2['S3AQ3B1']=sub2['S3AQ3B1'].replace(9, numpy.nan)\n",
    "sub2['S3AQ3C1']=sub2['S3AQ3C1'].replace(99, numpy.nan)\n",
    "\n",
    "#recoding values for S3AQ3B1 into a new variable, USFREQMO\n",
    "recode1 = {1: 30, 2: 22, 3: 14, 4: 6, 5: 2.5, 6: 1}\n",
    "sub2['USFREQMO']= sub2['S3AQ3B1'].map(recode1)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct1=pandas.crosstab(sub2['TAB12MDX'], sub2['USFREQMO'])\n",
    "print (ct1)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct1.sum(axis=0)\n",
    "colpct=ct1/colsum\n",
    "print(colpct)\n",
    "\n",
    "# chi-square\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs1= scipy.stats.chi2_contingency(ct1)\n",
    "print (cs1)\n",
    "\n",
    "\n",
    "# We calculate column percentages because:\n",
    "# + Row %: all rows add up to 100%\n",
    "# + Cell %: all cells in the table together add up to 100%\n",
    "# + Column %: all column add up to 100%\n",
    "'''\n",
    "We calculate column percentages to determine the Chi-Squared test results because\n",
    "the explanatory variable categories are represented as different columns,\n",
    "and response variable values are represented as rows,\n",
    "it'll be the explanatory variable categories(i.e. columns) that we want to interpret.\n",
    "i.e. we are interested in how the rate of response variable differs according to which\n",
    "explanatory group the observations belong to.\n",
    "We're not interested in the column percentages in observations without dependence.\n",
    "\n",
    "'''\n",
    "# If our explanatory variable had only 2 categories, we could interpret the 2 corresponding column percentages\n",
    "# and be able to say which group had a significantly higher rate of dependence.\n",
    "# In case of more than one category, we know that not all are equal,\n",
    "# but don't know which are different and which are not.\n",
    "\n",
    "# Plot the frequency of different categories of explanatory variable vs % of response variable as bar charts.\n",
    "# See next cell for plot\n",
    "# set variable types \n",
    "sub2[\"USFREQMO\"] = sub2[\"USFREQMO\"].astype('category')\n",
    "# new code for setting variables to numeric:\n",
    "sub2['TAB12MDX'] = pandas.to_numeric(sub2['TAB12MDX'], errors='coerce')\n",
    "\n",
    "# graph percent with nicotine dependence within each smoking frequency group \n",
    "seaborn.factorplot(x=\"USFREQMO\", y=\"TAB12MDX\", data=sub2, kind=\"bar\", ci=None)\n",
    "plt.xlabel('Days smoked per month')\n",
    "plt.ylabel('Proportion Nicotine Dependent')\n",
    "\n",
    "# Same post-hoc principles as in ANOVA test. Same principles of avoiding Type 1 errors.\n",
    "# Using Bonferroni Adjustment: p-value/c (number of comparisons we plan to make)\n",
    "# We need to then run a Chi-Square test for each of the paired comparisons.\n",
    "# If your explanatory variable has more than two levels or groups, you'll also need to conduct\n",
    "# a post hoc test. We use the Bonferroni Adjustment to protect against Type I error,\n",
    "# and then run the Chi Square Test of Independence for each paired comparison.\n",
    "\n",
    "# Create new variables that allow you to choose only 2 categories at a time. We use recodes with the map func to do this\n",
    "# We keep only 2 values (1 and 2.5) and remove the rest.\n",
    "# We create a contingency table, get col %s and a new chi-square table with only these 2 variables\n",
    "# If the p-value is greater than the Bonferroni Adjusted p-value, we know there is no correlation.\n",
    "recode2 = {1: 1, 2.5: 2.5}\n",
    "sub2['COMP1v2']= sub2['USFREQMO'].map(recode2)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct2=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v2'])\n",
    "print (ct2)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct2.sum(axis=0)\n",
    "colpct=ct2/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs2= scipy.stats.chi2_contingency(ct2)\n",
    "print (cs2)\n",
    "\n",
    "# ---- We finish by doing the same thing above for each of the other pairs\n",
    "recode3 = {1: 1, 6: 6}\n",
    "sub2['COMP1v6']= sub2['USFREQMO'].map(recode3)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct3=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v6'])\n",
    "print (ct3)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct3.sum(axis=0)\n",
    "colpct=ct3/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs3= scipy.stats.chi2_contingency(ct3)\n",
    "print (cs3)\n",
    "\n",
    "recode4 = {1: 1, 14: 14}\n",
    "sub2['COMP1v14']= sub2['USFREQMO'].map(recode4)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct4=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v14'])\n",
    "print (ct4)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct4.sum(axis=0)\n",
    "colpct=ct4/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs4= scipy.stats.chi2_contingency(ct4)\n",
    "print (cs4)\n",
    "\n",
    "recode5 = {1: 1, 22: 22}\n",
    "sub2['COMP1v22']= sub2['USFREQMO'].map(recode5)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct5=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v22'])\n",
    "print (ct5)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct5.sum(axis=0)\n",
    "colpct=ct5/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs5= scipy.stats.chi2_contingency(ct5)\n",
    "print (cs5)\n",
    "\n",
    "recode6 = {1: 1, 30: 30}\n",
    "sub2['COMP1v30']= sub2['USFREQMO'].map(recode6)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct6=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v30'])\n",
    "print (ct6)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct6.sum(axis=0)\n",
    "colpct=ct6/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs6= scipy.stats.chi2_contingency(ct6)\n",
    "print (cs6)\n",
    "\n",
    "recode7 = {2.5: 2.5, 6: 6}\n",
    "sub2['COMP2v6']= sub2['USFREQMO'].map(recode7)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct7=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP2v6'])\n",
    "print (ct7)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct7.sum(axis=0)\n",
    "colpct=ct7/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs7=scipy.stats.chi2_contingency(ct7)\n",
    "print (cs7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example Interpretation of Chi-Square Tests results from code above:\n",
    "\n",
    "When examining the association between lifetime major depression (categorical response) and past year nicotine dependence (categorical explanatory), a chi-square test of independence revealed that among daily, young adults smokers (my sample), those with past year nicotine dependence were more likely to have experienced major depression in their lifetime(36.2%) compared to those without past year nicotine dependence (12.7%), X2 =88.60, 1 df, p=0001.\n",
    "\n",
    "The df or degree of freedom we record is the number of levels of the explanatory variable -1. Here the df is 1 nicotine dependence which has 2 levels (df 2-1=1).\n",
    "\n",
    "Model Interpretation for post hoc Chi-Square Test results:\n",
    "\n",
    "A Chi Square test of independence revealed that among daily, young adult smokers (my sample), number of cigarettes smoked per day (collapsed into 5 ordered categories) and past year\n",
    "\n",
    "nicotine dependence (binary categorical variable) were significantly associated, X2 =45.16, 4 df, p=.0001.\n",
    "\n",
    "Post hoc comparisons of rates of nicotine dependence by pairs of cigarettes per day categories revealed that higher rates of nicotine dependence were seen among those smoking more cigarettes, up to 11 to 15 cigarettes per day. In comparison, prevalence of nicotine dependence was statistically similar among those groups smoking 10 to 15, 16 to 20, and > 20 cigarettes per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Chart generated from above code.\n",
    "<img src=\"wrangling-notebook-18.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Explanation of Chi-Square Test**\n",
    "One way to plot the similarities: Use letters to denote categories where depdendent variable rates are similar.\n",
    "<img src=\"wrangling-notebook-19.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Order of Cleaning Tasks**\n",
    "  - **Missing Data:** You'll address the missing data first (we want all our data in there before we start looking at quality/tidiness issues in the data. If we add data later, we'll have to do the quality/tidinessa assessments again) - concatenate, join, impute.\n",
    "  - **Tidiness:** You'll tackle the tidiness issues next ()\n",
    "  - **Quality:** And finally, you'll clean up the quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cope the dataset\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do you deal with missing values – ignore or treat them? The answer would depend on the percentage of those missing values in the dataset, the variables affected by missing values, whether those missing values are a part of dependent or the independent variables, etc. Missing Value treatment becomes important since the data insights or the performance of your predictive model could be impacted if the missing values are not appropriately handled.\n",
    "\n",
    "There are all sorts of strategies for dealing with missing data, and none of them are applicable universally. Some people will say \"never use instances which include empty values.\" Others will argue \"never use an attribute's mean value to replace missing values.\" Conversely, you may hear more complex methods endorsed wholesale, such as \"only first clustering a dataset into the number of known classes and then using intra-cluster regression to calculate missing values is valid.\"\n",
    "\n",
    "Listen to none of this. \"Never\" and \"only\" and other inflexible assertions hold no value in the nuanced world of data finessing; different types of data and processes suggest different best practices for dealing with missing values. However, since this type of knowledge is both experience and domain based, we will focus on the more basic strategies which can be employed.\n",
    "\n",
    "<img src=\"https://d35fo82fjcw0y8.cloudfront.net/2016/03/03210602/missing_value_table.jpg\">\n",
    "\n",
    "Some common methods for dealing with missing values include:\n",
    "\n",
    "* dropping instances\n",
    "* dropping attributes\n",
    "* imputing the attribute mean for all missing values\n",
    "* imputing the attribute median for all missing values\n",
    "* imputing the attribute mode for all missing values\n",
    "* using regression to impute attribute missing values\n",
    "\n",
    "clearly the type of modeling methods being employed will have an effect on your decision -- for example, decision trees are not amenable to missing values. Additionally, you could technically entertain any statistical method you could think of for determining missing values from the dataset, but the listed approaches are tried, tested, and commonly used approaches.\n",
    "\n",
    "--\n",
    "\n",
    "The choice of method to deal with missing values can vary widely from one dataset to the next (largely determined by whether rows with missing values are intrinsically like or unlike those without missing values)\n",
    "\n",
    "You cannot simply ignore missing values in your dataset. You must handle them in some way for the very practical reason that most algorithms do not accept missing values.\n",
    "\n",
    "Dropping missing values is sub-optimal because when you drop observations, you drop information.\n",
    "\n",
    "The fact that the value was missing may be informative in itself.\n",
    "Plus, in the real world, you often need to make predictions on new data even if some of the features are missing!\n",
    "Imputing missing values is sub-optimal because the value was originally missing but you filled it in, which always leads to a loss in information, no matter how sophisticated your imputation method is.\n",
    "\n",
    "Again, \"missingness\" is almost always informative in itself, and you should tell your algorithm if a value was missing.\n",
    "Even if you build a model to impute your values, you’re not adding any real information. You’re just reinforcing the patterns already provided by other features.\n",
    "\n",
    "In short, you should always tell your algorithm that a value was missing because missingness is informative.\n",
    "\n",
    "#### Missing categorical data\n",
    "The best way to handle missing data for categorical features is to simply label them as ’Missing’!\n",
    "\n",
    "You’re essentially adding a new class for the feature.\n",
    "This tells the algorithm that the value was missing.\n",
    "This also gets around the technical requirement for no missing values.\n",
    "\n",
    "#### Missing numeric data\n",
    "For missing numeric data, you should flag and fill the values.\n",
    "\n",
    "Flag the observation with an indicator variable of missingness.\n",
    "Then, fill the original missing value with 0 just to meet the technical requirement of no missing values.\n",
    "By using this technique of flagging and filling, you are essentially allowing the algorithm to estimate the optimal constant for missingness, instead of just filling it in with the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Method 1: `Drop missing Values`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Drop missing values\n",
    "If you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: This is generally not recommended for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)\n",
    "\n",
    "Unless the nature of missing data is ‘Missing completely at random’, the best avoidable method in many cases is deletion.\n",
    "\n",
    "<img src=\"https://d35fo82fjcw0y8.cloudfront.net/2016/03/03210603/listwise-deletion.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Removes every row in dataset that has at least one missing value, not always good\n",
    "nfl_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove all columns with at least one missing value\n",
    "columns_with_na_dropped = df.dropna(axis=1)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# n many cases, you'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames.\n",
    "cols_with_missing = [col for col in original_data.columns \n",
    "                                 if original_data[col].isnull().any()]\n",
    "redued_original_data = original_data.drop(cols_with_missing, axis=1)\n",
    "reduced_test_data = test_data.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Method 2:  `Imputation by Mean`\n",
    "\n",
    "Popular Averaging Techniques\n",
    "\n",
    "Mean, median and mode are the most popular averaging techniques, which are used to infer missing values. Approaches ranging from global average for the variable to averages based on groups are usually considered.\n",
    "\n",
    "For example: if you are inferring missing value for Revenue, you might assign the average defined by mean, median or mode to such missing value. You could also consider taking into account some other variables such as Gender of the User and/or the Device OS to calculate such an average to be assigned to the missing values.\n",
    "\n",
    "Though you can get a quick estimate of the missing values, you are artificially reducing the variation in the dataset as the missing observations could have the same value. This may impact the statistical analysis of the dataset since depending on the percentage of missing observations imputed, metrics such as mean, median, correlation, etc may get affected.\n",
    "\n",
    "<img src=\"https://d35fo82fjcw0y8.cloudfront.net/2016/03/03210602/imputation-by-averaging.jpg\">\n",
    "\n",
    "The above table shows the difference in imputed missing values of Revenue arrived by taking its global mean and mean based on which OS platform it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "my_imputer = Imputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)\n",
    "# The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#####  `Extending Imputation`\n",
    "Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[c].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "new_data = my_imputer.fit_transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#####  `Fill In Missing Values Automatically `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace all NA's with 0\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace all NA's the value that comes directly after it in the same column, \n",
    "# then replace all the reamining na's with 0\n",
    "df.fillna(method = 'bfill', axis=0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Method 3:  `Imputation by Predictive Techniques`\n",
    "\n",
    "Imputation of missing values from predictive techniques assumes that the nature of such missing observations are not observed completely at random and the variables chosen to impute such missing observations have some relationship with it, else it could yield imprecise estimates.\n",
    "\n",
    "In the examples discussed earlier, a predictive model could be used to impute the missing values for Device, OS, Revenues. There are various statistical methods like regression techniques, machine learning methods like SVM and/or data mining methods to impute such missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Imputation by Regression\n",
    "# Assume ‘y’ depends on ‘x’\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Other Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Drop the columns where all elements are missing values:\n",
    "df.dropna(axis=1, how='all')\n",
    "\n",
    "# Drop the columns where any of the elements are missing values\n",
    "df.dropna(axis=1, how='any')\n",
    "\n",
    "# Keep only the rows which contain 2 missing values maximum\n",
    "df.dropna(thresh=2)\n",
    "\n",
    "# Drop the columns where any of the elements are missing values\n",
    "df.dropna(axis=1, how='any')\n",
    "\n",
    "# Fill all missing values with the mean of the particular column\n",
    "df.fillna(df.mean())\n",
    "\n",
    "# Fill any missing value in column 'A' with the column median\n",
    "df['A'].fillna(df['A'].median())\n",
    "\n",
    "# Fill any missing value in column 'Depeche' with the column mode\n",
    "df['Depeche'].fillna(df['Depeche'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Fetch from another source`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Import the cut treatments into a DataFrame and concatenate it with the original treatments DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cut = pd.read_csv('df_cut.csv')\n",
    "df_clean = pd.concat([df_clean, df_cut],\n",
    "                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert anew_df.shape[0] == df.shape[0] + df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Calculate from df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Recalculate the `hba1c_change` column: `hba1c_start` minus `hba1c_end`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.col_name = (df.col_name_start - df.col_name_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.col_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Outliers\n",
    "\n",
    "There are times when including outliers in modeling is appropriate, and there are times when they are not (regardless of what anyone tries to tell you). This is situation-dependent, and no one can make sweeping assertions as to whether your situation belongs in column A or column B.\n",
    "\n",
    "Outliers can be the result of poor data collection, or they can be genuinely good, anomalous data. These are 2 different scenarios, and must be approached differently, and so no \"one size fits all\" advice is applicable here, similar to that of dealing with missing values.\n",
    "\n",
    "One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Imbalanced Classes\n",
    "\n",
    "So, what if your otherwise robust dataset -- lacking both missing values and outliers -- is made up of 2 classes: one which includes 95 percent of the instances, and the other which includes a mere 5 percent? Or worse -- 99.8 vs 0.2 percent?\n",
    "\n",
    "If so, your dataset is imbalanced, at least as far as the classes are concerned. This can be problematic, in ways which I'm sure do not need to be pointed out. But no need to to toss the data to the side yet; there are, of course, strategies for dealing with this.\n",
    "\n",
    "Note that, while this may not genuinely be a data preparation task, such a dataset characteristic will make itself known early in the data preparation stage (the importance of EDA), and the validity of such data can certainly be assessed preliminarily during this preparation stage.\n",
    "\n",
    "A good explanation of why we can run into imbalanced data, and why we can do so in some domains much more frequently than in others (from 7 Techniques to Handle Imbalanced Data, linked above):\n",
    "\n",
    "Data used in these areas often have less than 1% of rare, but “interesting” events (e.g. fraudsters using credit cards, user clicking advertisement or corrupted server scanning its network). However, most machine learning algorithms do not work very well with imbalanced datasets. The following seven techniques can help you, to train a classifier to detect the abnormal class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data Transformations\n",
    "\n",
    "Data transformation is the application of a deterministic mathematical function to each point in a data set — that is, each data point zi is replaced with the transformed value yi = f(zi), where f is a function. Transforms are usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that is to be applied, or to improve the interpretability or appearance of graphs.\n",
    "\n",
    "Transforming data is one of the most important aspects of data preparation, and one which requires more finesse than most others. When missing values manifest themselves in data, they are generally easy to find, and can be (at least, superficially) dealt with by one of the common methods outlined above -- or by more complex measures gained from insight over time in a domain. However, when and if data transformations are required -- to say nothing of the type of transformation required -- is often not as easily identifiable.\n",
    "\n",
    "A plethora of transformations exist; instead of trying to generalize when and why transformations are useful, let's look at a few specific transformations in order to get a better handle on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### OHE\n",
    "One-hot encoding \"transforms categorical features to a format that works better with classification and regression algorithms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Log Transformation\n",
    "The log distribution transformation can be useful if \"you assume a model form that is non-linear but can be transformed to a linear model\" (taken from below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Overview\n",
    "\n",
    "While melting takes a set of columns and turns it into a single column, pivoting will create a new column for each unique value in a specified column.\n",
    "\n",
    "<img src=\"wrangling-notebook-melt-pivot-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Melt` many columns in table contain one variable\n",
    "Used to fix columns containing values, instead of variables.\n",
    "\n",
    "Explanation friendly -> analysis friendly shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.melt(treatments_clean, id_vars=['given_name', 'surname', 'hba1c_start', 'hba1c_end', 'hba1c_change'],\n",
    "                           value_vars = ['col1', 'col2'], var_name='treatment', value_name='dose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Pivot` one column in table contain many variables\n",
    "Used to fix multiple variables being stored in the same column\n",
    "\n",
    "Analysis friendly -> Explanation friendly shape\n",
    "\n",
    "Pivoting doesn't work if there are duplicate values. Use pivot table method to specify how to reconcile duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pivot and reset index\n",
    "# columns (the name of the column you want to pivot), and\n",
    "# values (the values to be used when the column is pivoted)\n",
    "\n",
    "# reset_index - pivot_table returns a pandas DataFrame with a hierarchical index (also known as a MultiIndex) -  In essence, they allow you to group columns or rows by another variable - in this case, by 'Keep' as well as 'Fixed'. To fix it to intended form, use reset_index()\n",
    "df = df.pivot_table(index=['Keep', 'Fixed'], columns='measurement', values='reading').reset_index()\n",
    "\n",
    "\n",
    "# By using .pivot_table() and the aggfunc parameter, you can not only reshape your data, but also remove duplicates. Finally, you can then flatten the columns of the pivoted DataFrame using .reset_index().\n",
    "# Removing Duplicate Values\n",
    "# Pivot airquality_dup: airquality_pivot\n",
    "df = df_dup.pivot_table(index=['Keep', 'Fixed'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Melt the *auralin* and *novodra* columns to a *treatment* and a *dose* column (dose will still contain both start and end dose at this point). Then split the dose column on ' - ' to obtain *start_dose* and *end_dose* columns. Drop the intermediate *dose* column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Split Columns\n",
    "df = df[df.dose != \"-\"]\n",
    "df['dose_start'], df['dose_end'] = df['dose'].str.split(' - ', 1).str\n",
    "df = df.drop('dose', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Extract` one column in table contains many variables: phone number and email\n",
    "\n",
    "E.g. here the column name has multiple values, so there is an extra first step to melt the columns.\n",
    "<img src=\"wrangling-notebook-multiple-values-in-col-example.png\">\n",
    "\n",
    "This is the resulting dataset:\n",
    "<img src=\"wrangling-notebook-multiple-values-in-col-fixed-example.png\">\n",
    "\n",
    "Now we can apply the extraction functions on the values in the column like below.\n",
    "\n",
    "We can use extract, split, or string slicing like def.col_name[k:l], or string functions - whatever works best in this situation.\n",
    "\n",
    "E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt['str_split'].str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt['str_split'].str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Extract the *phone number* and *email* variables from the *contact* column using regular expressions and pandas' `str.extract` method. Drop the *contact* column when done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reg expression to get phone nmbers:\n",
    "r_phone = r'([\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9])'\n",
    "# Reg expression to get emails:\n",
    "r_email = r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n",
    "\n",
    "df['phone_number'] = df.col_name.str.extract('((?:\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})', expand=True)\n",
    "df['email'] = df.col_name.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.][a-zA-Z]+)', expand=True)\n",
    "# Note: axis=1 denotes that we are referring to a column, not a row\n",
    "df = df.drop('col_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Confirm contact column is gone\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.col_name.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Confirm that no emails start with an integer (regex didn't match for this)\n",
    "df.col_name.sort_values().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Merge & Concat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Concat**\n",
    "\n",
    "Think of it as sticking together data that was once a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Row uber1, uber2, and uber3\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "# Column concatenate uber1, uber2, and uber3\n",
    "col_concat = pd.concat([uber1, uber2, uber3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Concatenating Many Files using GLOB\n",
    "# * = any arbitrary num of characters\n",
    "# ? = matches one alphanumeric character\n",
    "import glob\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Merge**\n",
    "\n",
    "Merging data allows you to combine disparate datasets into a single dataset to do more complex analysis.\n",
    "\n",
    "Used when you can't concatenate data because the ordering of the rows isn't the same.\n",
    "\n",
    "Think of it as combining disparate datasets based on a commong set of columns.\n",
    "\n",
    "Types of merges:\n",
    "* one-to-one\n",
    "* many-to-one / one-to-many\n",
    "* many-to-many\n",
    "\n",
    "<img src=\"wrangling-notebook-merge-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**One-to-One Merge**\n",
    "\n",
    "E.g.\n",
    "<img src=\"wrangling-notebook-merge-one-to-one-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Many-to-One Merge**\n",
    "\n",
    "E.g.\n",
    "<img src=\"wrangling-notebook-merge--one-to-many-example.png\">\n",
    "\n",
    "**Many-to-Many Merge**\n",
    "\n",
    "E.g.\n",
    "<img src=\"wrangling-notebook-merge--many-to-many-example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Same code to do all merges\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Merge the *adverse_reaction* column to the `treatments` table, joining on *given name* and *surname*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, df2, on=['given_name', 'surname'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### columns in one table duplicated in other tables and\n",
    "#### `Lowercase column names`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Adverse reactions table is no longer needed so ignore that part. Isolate the patient ID and names in the `patients` table, then convert these names to lower case to join with `treatments`. Then drop the given name and surname columns in the treatments table (so these being lowercase isn't an issue anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id_names = df[['patient_id', 'given_name', 'surname']]\n",
    "id_names.given_name = id_names.given_name.str.lower()\n",
    "id_names.surname = id_names.surname.str.lower()\n",
    "df = pd.merge(df, id_names, on=['given_name', 'surname'])\n",
    "df = df.drop(['given_name', 'surname'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm the merge was executed correctly\n",
    "treatments_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Patient ID should be the only duplicate column\n",
    "all_columns = pd.Series(list(patients_clean) + list(treatments_clean))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "values = array(data)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "scaling, you're changing the range of your data while in normalization you're changing the shape of the distribution of your data.\n",
    "\n",
    "By scaling your variables, you can help compare different variables on equal footing. You are transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points, like support vector machines, or SVM or k-nearest neighbors, or KNN. With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n",
    "\n",
    "Scaling improves output of regression model and even clustering improves.\n",
    "\n",
    "Scaling just changes the range of your data.\n",
    "\n",
    "Discussion on the difference between Scaling and Normalization:\n",
    "\n",
    "Scaling is a column operation and normalization is a row operation. The standard scaling process is transform all features independently such that they each have a mean of zero and a standard deviation of one. The standard normalization process is to transform all features for a given case to a unit vector.\n",
    "\n",
    "Scaling is particularly important for linear models; it ensures that all inputs are treated equally in the regularization process, and allows one to choose a meaningful range for random starting weights.\n",
    "\n",
    "Normalization can be important when training neural networks, to prevent exploding or vanishing gradients. It can also help to improve convergence time.\n",
    "\n",
    "normalization in the context of math/physics is to create a normal vector, or a vector with unit length. This unit length refers to Euclidean distance, so you need to consider all dimensions of your training data (all columns).\n",
    "\n",
    "You could use normalization to scale a single column, but it's not very efficient. You'd get the same results from a MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "// Normal distribution: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox (Box-Cox Transformation: https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation)\n",
    "normalized_data = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Character Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text\n",
    "\n",
    "There are many different encodings, and if you tried to read in text with a different encoding that the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n",
    "\n",
    "æ–‡å—åŒ–ã??\n",
    "\n",
    "You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n",
    "\n",
    "����������\n",
    "\n",
    "Character encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n",
    "\n",
    "UTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble.\n",
    "\n",
    "You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a cd player. If you try to play a cassette in a CD player, it just won't work.\n",
    "\n",
    "Note: We can run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ascii using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!\n",
    "\n",
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# character encoding module\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Inconsistent Data Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = df['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.\n",
    "\n",
    "# convert to lower case\n",
    "df['City'] = df['City'].str.lower()\n",
    "# remove trailing white spaces\n",
    "df['City'] = df['City'].str.strip()\n",
    "# Strip u\n",
    "df.dose_start = df.dose_start.str.strip('u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use fuzzy matching to correct inconsistent data entry\n",
    "# Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n",
    "# Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings.\n",
    "\n",
    "# get the top 10 closest matches to \"d.i khan\"\n",
    "matches = fuzzywuzzy.process.extract(\"a. b. value\", df, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "# take a look at them\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use the function above to replace close matches\n",
    "# Choose a good value for min_ratio, by looking at the ratio of similar values in the matches table above\n",
    "replace_matches_in_column(df=df, column='City', string_to_match=\"a. b. value\", min_ratio = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = df['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Erroneous Datatypes\n",
    "(assigned sex, state, zip_code, and birthdate columns) and Erroneous datatypes (auralin and novodra columns) and\n",
    "\n",
    "#### The letter 'u' in starting and ending doses for Auralin and Novodra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Convert assigned sex and state to categorical data types. Zip code data type was already addressed above. Convert birthdate to datetime data type. Strip the letter 'u' in start dose and end dose and convert those columns to data type integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# See the datatypes of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To category (makes DF smaller in memory if # of categories is small)\n",
    "df.assigned_sex = df.assigned_sex.astype('category')\n",
    "df.state = df.state.astype('category')\n",
    "\n",
    "# To string\n",
    "df.astr = df.astr.astype(str)\n",
    "\n",
    "# To datetime\n",
    "df.birthdate = pd.to_datetime(df.birthdate)\n",
    "\n",
    "# Strip u and to integer\n",
    "df.dose_start = df.dose_start.str.astype(int)\n",
    "df.dose_end = df.dose_end.str.astype(int)\n",
    "\n",
    "# String to numeric with errors = 'coerce' to set non-int values like '-' or 'None' to NaN\n",
    "# You can use the pd.to_numeric() function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. You can either use some exploratory data analysis techniques and find the bad value, or you can choose to ignore or coerce the value into a missing value, NaN.\n",
    "tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Duplicate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Remove the Jake Jakobsen, Pat Gersten, and Sandy Taylor rows from the `patients` table. These are the nicknames, which happen to also not be in the `treatments` table (removing the wrong name would create a consistency issue between the `patients` and `treatments` table). These are all the second occurrence of the duplicate. These are also the only occurences of non-null duplicate addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tilde means not: http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\n",
    "df = df[~((df.address.duplicated()) & df.address.notnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df.surname == 'Jakobsen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.surname == 'Gersten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df.surname == 'Taylor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Manually fix one off errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Replace height for rows in the `patients` table that have a height of 27 in (there is only one) with 72 in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.height = df.height.replace(27, 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be empty\n",
    "df[df.height == 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Confirm the replacement worked\n",
    "df[df.surname == 'Neudorf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Replace given name for rows in the `patients` table that have a given name of 'Dsvid' with 'David'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.given_name = df.given_name.replace('Dsvid', 'David')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.surname == 'Gustafsson']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Default John Doe data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Remove the non-recoverable John Doe records from the `patients` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.surname != 'Doe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be no Doe records\n",
    "df.surname.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be no 123 Main Street records\n",
    "df.address.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Extracting Number From String\n",
    "\n",
    "e.g. 'the recipe calls for 6 strawberries and 2 bananas'\n",
    "\n",
    "When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the re.findall() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Return a list of all the numeric values\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Monetary Values\n",
    "\n",
    "* 17\n",
    "\n",
    "* $17\n",
    "\n",
    "* $17.89\n",
    "\n",
    "* $17.892\n",
    "\n",
    "* $17892.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "result = pattern.match('$17.89')\n",
    "bool(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parsing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print the first few rows of the date column. Check the data type of our date column\n",
    "print(landslides['date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Determine format of the dates above, and pass it in format parameter (http://strftime.org/)\n",
    "# create a new column, date_parsed, with the parsed dates\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")\n",
    "\n",
    "# If we get a multiple datatypes error, have pandas infer what the format should be\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)\n",
    "\n",
    "# We don't always use infer = true for 2 reasons: The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get just the day of the month from the date_parsed column\n",
    "day_of_month_landslides = landslides['date_parsed'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the day of the month to check the date parsing\n",
    "# One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense.\n",
    "# To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.)\n",
    "\n",
    "# remove na's\n",
    "day_of_month_landslides = day_of_month_landslides.dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Zip code\n",
    "is a float not a string and Zip code has four digits sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Convert the zip code column's data type from a float to a string using `astype`, remove the '.0' using string slicing, and pad four digit zip codes with a leading 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.zip_code = df.zip_code.astype(str).str[:-2].str.pad(5, fillchar='0')\n",
    "# Reconvert NaNs entries that were converted to '0000n' by code above\n",
    "df.zip_code = df.zip_code.replace('0000n', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.zip_code.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Phone Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Strip all \" \", \"-\", \"(\", \")\", and \"+\" and store each number without any formatting. Pad the phone number with a 1 if the length of the number is 10 digits (we want country code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.phone_number = df.phone_number.str.replace(r'\\D+', '').str.pad(11, fillchar='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reg expression to get phone nmbers:\n",
    "r_phone = r'([\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9])'\n",
    "\n",
    "df['phone_number'] = df.col_name.str.extract('((?:\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.phone_number.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reg expression to get emails:\n",
    "r_email = r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n",
    "\n",
    "df['email'] = df.col_name.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.][a-zA-Z]+)', expand=True)\n",
    "# Note: axis=1 denotes that we are referring to a column, not a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### State Names\n",
    "Full sometimes, abbreviations other times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Apply a function that converts full state name to state abbreviation for California, New York, Illinois, Florida, and Nebraska."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Mapping from full state name to abbreviation\n",
    "state_abbrev = {'California': 'CA',\n",
    "                'New York': 'NY',\n",
    "                'Illinois': 'IL',\n",
    "                'Florida': 'FL',\n",
    "                'Nebraska': 'NE'}\n",
    "\n",
    "# Function to apply\n",
    "def abbreviate_state(patient):\n",
    "    if patient['state'] in state_abbrev.keys():\n",
    "        abbrev = state_abbrev[patient['state']]\n",
    "        return abbrev\n",
    "    else:\n",
    "        return patient['state']\n",
    "    \n",
    "df['state'] = df.apply(abbreviate_state, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Weights\n",
    "kgs instead of lbs and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define\n",
    "Use [advanced indexing](https://stackoverflow.com/a/44913631) to isolate the row where the surname is Zaitseva and convert the entry in its weight field from kg to lbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weight_kg = df.weight.min()\n",
    "mask = df.surname == 'Zaitseva'\n",
    "column_name = 'weight'\n",
    "df.loc[mask, column_name] = weight_kg * 2.20462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 48.8 shouldn't be the lowest anymore\n",
    "df.weight.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Complex Cleaning\n",
    "\n",
    "For when the cleaning Step requires multiple steps:\n",
    "e.g.\n",
    "- extract number from string\n",
    "- perform transformation on extracted number\n",
    "\n",
    "We can write a function to apply these transformations to every row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example 1: Cleaning out 2 monetary coumns and calculating their difference in a 3rd column\n",
    "def diff_money(row, pattern):\n",
    "    icost = row['Initial Cost']\n",
    "    tef = row['Total Est. Fee']\n",
    "\n",
    "    if bool(pattern.match(icost)) and bool(pattern.match(tef)):\n",
    "    icost = icost.replace(\"$\",\n",
    "    \"\")\n",
    "    tef = tef.replace(\"$\",\n",
    "    \"\")\n",
    "\n",
    "    icost = float(icost)\n",
    "    tef = float(tef)\n",
    "\n",
    "    return icost - tef\n",
    "    else:\n",
    "    return(NaN)\n",
    "\n",
    "df_subset['diff'] = df_subset.apply(diff_money, axis=1, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example 2: Encode values of a column (Male/Female) as 0/1\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)\n",
    "\n",
    "# Print the first five rows of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example 3: Replace $ sign in monetary value string\n",
    "\n",
    "# Using 2 ways to do this:\n",
    "\n",
    "# lambda function using replace\n",
    "tips['total_dollar_replace'] = tips['total_dollar'].apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips['total_dollar'].apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Combine Sparse Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Add Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Remove Unused Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded a function score_dataset(X_train, X_test, y_train, y_test) to compare the quality of diffrent approaches to missing values. This function reports the out-of-sample MAE score from a RandomForest.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write new data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = pd.read_csv(\"AvaMD-Claims-Master.csv\")\n",
    "new = pd.read_csv(\"AvaMD-Claims-v2.csv\")\n",
    "old.shape[0]\n",
    "new.shape[0]\n",
    "new.to_csv(\"AvaMD-Claims-Master.csv\", index=False, mode='a')\n",
    "merged = pd.read_csv(\"AvaMD-Claims-Master.csv\")\n",
    "merged.shape[0]\n",
    "assert (merged.shape[0] == old.shape[0] + new.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dataland Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Dataland Functions\n",
    "\n",
    "# Join 2 dataframes\n",
    "dataland_join(df_a, df_b):\n",
    "    return pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "# Send a file, get a dataframe\n",
    "dataland_read(file_to_read):\n",
    "    # TODO: Determine file type and pick appropriate function to read file\n",
    "    pd_from_file = pd.read_csv(file_to_read)\n",
    "    return pd_from_file;\n",
    "\n",
    "# Drops a column from a dataframe\n",
    "dataland_drop_cols(df_a, drop_cols):\n",
    "    return df_a.drop(drop_cols, axis=1)\n",
    "\n",
    "# Copy a column from a datafram\n",
    "dataland_copy(df_a):\n",
    "    return df_a.copy()\n",
    "\n",
    "# Split a column\n",
    "\n",
    "# Strip\n",
    "\n",
    "# Merge columns\n",
    "dataland_copy(df_a, df_b, on_cols, how):\n",
    "    return pd.merge(df_a, df_b, on=on_cols, how=how)\n",
    "\n",
    "# Melt columns\n",
    "dataland_melt(df_a, id_vars, value_vars, var_name, value_name):\n",
    "    return pd.melt(df_a, id_vars=id_vars, value_vars=value_vars, var_name=var_name, value_name=value_name)\n",
    "\n",
    "# Pivot\n",
    "\n",
    "# Replace\n",
    "\n",
    "# Duplicated\n",
    "\n",
    "# IsNull\n",
    "\n",
    "\n",
    "## Standardize Columns\n",
    "\n",
    "# Standardize phone number format\n",
    "dataland_standardize_phone(df_a, phone_col):\n",
    "    r_phone = r'([\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9])'\n",
    "    return df_a[phone_col].str.extract('((?:\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})', expand=True)\n",
    "\n",
    "# Standardize email format\n",
    "dataland_standardize_email(df_a, email_col):\n",
    "    return df_a[email_col].str.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.][a-zA-Z]+)', expand=True)\n",
    "\n",
    "# Make string lowercase\n",
    "dataland_standardize_lowercase(df_a, col):\n",
    "    return df_a['col'].str.lower()\n",
    "\n",
    "# Convert Type\n",
    "\n",
    "# DateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide uncleaned data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key point: let users download both cleaned and uncleaned data, because the test set for their models will need to come from the uncleaned data (to match real world data they’ll see), while the training data will come from the cleaned data.\n",
    "\n",
    "Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data is a first step, and pipelines can help implement this separation. Leaking predictors are a more frequent issue, and leaking predictors are harder to track down. A combination of caution, common sense and data exploration can help identify leaking predictors so you remove them from your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinds of Leaky Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky Validation Strategy\n",
    "This type of leak occurs when you aren't careful distinguishing training data from validation data. **For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split.**\n",
    "\n",
    "Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n",
    "\n",
    "#### Preventing Leaky Validation Strategies\n",
    "If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn Pipelines. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky Predictors\n",
    "This occurs when your predictors include data that will not be available at the time you make predictions.\n",
    "\n",
    "For example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:\n",
    "\n",
    "got_pneumonia\tage\tweight\tmale\ttook_antibiotic_medicine\t...\n",
    "False\t65\t100\tFalse\tFalse\t...\n",
    "False\t72\t130\tTrue\tFalse\t...\n",
    "True\t58\t100\tFalse\tTrue\t...\n",
    "\n",
    "\n",
    "People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage.\n",
    "\n",
    "The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.\n",
    "\n",
    "To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n",
    "\n",
    "<img src=\"https://i.imgur.com/CN4INKb.png\">\n",
    "\n",
    "#### Preventing Leaky Predictors\n",
    "There is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.\n",
    "\n",
    "However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n",
    "\n",
    "To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n",
    "If you build a model and find it extremely accurate, you likely have a leakage problem.\n",
    "\n",
    "E.g. We should use cross-validation to ensure accurate measures of model quality.\n",
    "\n",
    "If CV score is very high (e.g. 98%), we know from experience that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = data.card\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\n",
    "modeling_pipeline = make_pipeline(RandomForestClassifier())\n",
    "cv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())\n",
    "# Cross-val accuracy: 0.979528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, basic data comparisons can be very helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditures_cardholders = data.expenditure[data.card]\n",
    "expenditures_noncardholders = data.expenditure[~data.card]\n",
    "\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))\n",
    "\n",
    "# Fraction of those who received a card with no expenditures: 0.02\n",
    "# Fraction of those who received a card with no expenditures: 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone with card == False had no expenditures, while only 2% of those with card == True had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means *expenditures on the card they applied for.**.\n",
    "\n",
    "Since share is partially determined by expenditure, it should be excluded too. The variables active, majorcards are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\n",
    "\n",
    "We would run a model without leakage as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "cv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())\n",
    "# Cross-val accuracy: 0.806677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get in the Mindset of a Detective\n",
    "- As an analyst, it is your job to extract information from data\n",
    "- Go beyond the keyboard to investigate as if you are detective\n",
    "- Have courage to ask the important questions\n",
    "- You can also think of yourself as making a documentary about the data. - - You will ultimately tell some story about it. Make it accurate and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think About\n",
    "Training Data -> Cleaned Training Data -- Train Models on cleaned data\n",
    "Testing/Real World Data -- Can you predict directly on it or do you need to transform it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Python Code Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axis in pandas dataframes\n",
    "<img src=\"wrangling-notebook-17.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippets for Pandas\n",
    "import pandas as pd\n",
    "‘’’\n",
    "Reading Files, Selecting Columns, and Summarizing\n",
    "‘’’\n",
    "\n",
    "\n",
    "# reading in a file from local computer or directly from a URL\n",
    "\n",
    "# various file formats that can be read in out wrote out\n",
    "‘’’\n",
    "Format Type     Data Description      Reader           Writer\n",
    "text                  CSV            read_csv          to_csv\n",
    "text                 JSON            read_json         to_json\n",
    "text                 HTML            read_html         to_html\n",
    "text             Local clipboard  read_clipboard     to_clipboard\n",
    "binary             MS Excel          read_excel        to_excel\n",
    "binary            HDF5 Format        read_hdf           to_hdf\n",
    "binary           Feather Format     read_feather      to_feather\n",
    "binary              Msgpack         read_msgpack      to_msgpack\n",
    "binary               Stata           read_stata        to_stata\n",
    "binary                SAS             read_sas \n",
    "binary        Python Pickle Format   read_pickle       to_pickle\n",
    "SQL                   SQL             read_sql          to_sql\n",
    "SQL             Google Big Query      read_gbq          to_gbq\n",
    "‘’’\n",
    "\n",
    "#to read about different types of files, and further functionality of reading in files, visit: http://pandas.pydata.org/pandas-docs/version/0.20/io.html\n",
    "df = pd.read_csv(‘local_path/file.csv’)\n",
    "df = pd.read_csv(‘https://file_path/file.csv')\n",
    "\n",
    "# when reading in tables, can specify separators, and note a column to be used as index separators can include tabs (“\\t”), commas(“,”), pipes (“|”), etc.\n",
    "df = pd.read_table(‘https://file_path/file', sep=’|’, index_col=’column_x’)\n",
    "\n",
    "# examine the df data\n",
    "df           \n",
    "# print the first 30 and last 30 rows\n",
    "type(df)     \n",
    "# DataFrame\n",
    "df.head()    \n",
    "# print the first 5 rows\n",
    "df.head(10)  \n",
    "# print the first 10 rows\n",
    "df.tail()    \n",
    "# print the last 5 rows\n",
    "df.index     \n",
    "# “the index” (aka “the labels”)\n",
    "df.columns   \n",
    "# column names (which is “an index”)\n",
    "df.dtypes    \n",
    "# data types of each column\n",
    "df.shape     \n",
    "# number of rows and columns\n",
    "df.values    \n",
    "# underlying numpy array — df are stored as numpy arrays for effeciencies.\n",
    "\n",
    "# select a column\n",
    "df[‘column_y’]         \n",
    "# select one column\n",
    "type(df[‘column_y’])   \n",
    "# determine datatype of column (e.g., Series)\n",
    "df.column_y            \n",
    "# select one column using the DataFrame attribute — not effective if column names have spaces\n",
    "\n",
    "# summarize (describe) the DataFrame\n",
    "df.describe()          \n",
    "# describe all numeric columns\n",
    "df.describe(include=[‘object’]) \n",
    "# describe all object columns\n",
    "df.describe(include=’all’)      \n",
    "# describe all columns\n",
    "\n",
    "# summarize a Series\n",
    "df.column_y.describe()   \n",
    "# describe a single column\n",
    "df.column_z.mean()       \n",
    "# only calculate the mean\n",
    "df[“column_z”].mean()    \n",
    "# alternate method for calculating mean\n",
    " \n",
    "\n",
    "# count the number of occurrences of each value\n",
    "df.column_y.value_counts()   \n",
    "# most useful for categorical variables, but can also be used with numeric variables\n",
    "\n",
    "#filter df by one column, and print out values of another column\n",
    "\n",
    "#when using numeric values, no quotations\n",
    "df[df.column_y == “string_value”].column_z\n",
    "df[df.column_y == 20 ].column_z    \n",
    " \n",
    "\n",
    "# display only the number of rows of the ‘df’ DataFrame\n",
    "df.shape[0]\n",
    "\n",
    "# display the 3 most frequent occurances of column in ‘df’\n",
    "df.column_y.value_counts()[0:3]\n",
    "‘’’\n",
    "Filtering and Sorting\n",
    "‘’’\n",
    "\n",
    "# boolean filtering: only show df with column_z < 20\n",
    "filter_bool = df.column_z < 20    \n",
    "# create a Series of booleans…\n",
    "df[filter_bool]                \n",
    "# …and use that Series to filter rows\n",
    "df[filter_bool].describe()     \n",
    "# describes a data frame filtered by filter_bool\n",
    "df[df.column_z < 20]           \n",
    "# or, combine into a single step\n",
    "df[df.column_z < 20].column_x  \n",
    "# select one column from the filtered results\n",
    "df[df[“column_z”] < 20].column_x     \n",
    "# alternate method \n",
    "df[df.column_z < 20].column_x.value_counts()   \n",
    "# value_counts of resulting Series, can also use .mean(), etc. instead of .value_counts()\n",
    "\n",
    "# boolean filtering with multiple conditions; indexes are in square brackets, conditions are in parens\n",
    "df[(df.column_z < 20) & (df.column_y==’string’)] \n",
    "# ampersand for AND condition \n",
    "df[(df.column_z < 20) | (df.column_z > 60)] \n",
    "# pipe for OR condition\n",
    "\n",
    "# sorting\n",
    "df.column_z.order()          \n",
    "# sort a column\n",
    "df.sort_values(‘column_z’)   \n",
    "# sort a DataFrame by a single column\n",
    "df.sort_values(‘column_z’, ascending=False)     \n",
    "# use descending order instead\n",
    "\n",
    "# Sort dataframe by multiple columns\n",
    "df = df.sort([‘col1’,’col2',’col3'],ascending=[1,1,0]) \n",
    " \n",
    "\n",
    "# can also filter ‘df’ using pandas.Series.isin \n",
    "df[df.column_x.isin([“string_1”, “string_2”])]\n",
    "‘’’\n",
    "Renaming, Adding, and Removing Columns\n",
    "‘’’\n",
    "\n",
    "# rename one or more columns\n",
    "df.rename(columns={‘original_column_1’:’column_x’, ‘original_column_2’:’column_y’}, inplace=True) \n",
    "#saves changes \n",
    " \n",
    "\n",
    "# replace all column names (in place)\n",
    "new_cols = [‘column_x’, ‘column_y’, ‘column_z’]\n",
    "df.columns = new_cols\n",
    "\n",
    "# replace all column names when reading the file\n",
    "df = pd.read_csv(‘df.csv’, header=0, names=new_cols)\n",
    "\n",
    "# add a new column as a function of existing columns\n",
    "df[‘new_column_1’] = df.column_x + df.column_y\n",
    "df[‘new_column_2’] = df.column_x * 1000   \n",
    "#can create new columns without for loops\n",
    "\n",
    "# removing columns\n",
    "df.drop(‘column_x’, axis=1)   \n",
    "# axis=0 for rows, 1 for columns — does not drop in place\n",
    "df.drop([‘column_x’, ‘column_y’], axis=1, inplace=True) \n",
    "# drop multiple columns\n",
    "\n",
    "# Lower-case all DataFrame column names\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# Even more fancy DataFrame column re-naming\n",
    "\n",
    "# lower-case all DataFrame column names (for example)\n",
    "df.rename(columns=lambda x: x.split(‘.’)[-1], inplace=True)\n",
    " \n",
    " \n",
    "‘’’\n",
    "Handling Missing Values\n",
    "‘’’\n",
    "\n",
    "# missing values are usually excluded by default\n",
    "df.column_x.value_counts()             \n",
    "# excludes missing values\n",
    "df.column_x.value_counts(dropna=False) \n",
    "# includes missing values\n",
    "\n",
    "# find missing values in a Series\n",
    "df.column_x.isnull()  \n",
    "# True if missing\n",
    "df.column_x.notnull() \n",
    "# True if not missing\n",
    "\n",
    "# use a boolean Series to filter DataFrame rows\n",
    "df[df.column_x.isnull()]  \n",
    "# only show rows where column_x is missing\n",
    "df[df.column_x.notnull()] \n",
    "# only show rows where column_x is not missing\n",
    "\n",
    "# understanding axes\n",
    "df.sum()       \n",
    "# sums “down” the 0 axis (rows)\n",
    "df.sum(axis=0) \n",
    "# equivalent (since axis=0 is the default)\n",
    "df.sum(axis=1) \n",
    "# sums “across” the 1 axis (columns)\n",
    "\n",
    "# adding booleans\n",
    "pd.Series([True, False, True])       \n",
    "# create a boolean Series\n",
    "pd.Series([True, False, True]).sum() \n",
    "# converts False to 0 and True to 1\n",
    "\n",
    "# find missing values in a DataFrame\n",
    "df.isnull() \n",
    "# DataFrame of booleans\n",
    "df.isnull().sum() \n",
    "# count the missing values in each column\n",
    "\n",
    "# drop missing values\n",
    "df.dropna(inplace=True)   \n",
    "# drop a row if ANY values are missing, defaults to rows, but can be applied to columns with axis=1\n",
    "df.dropna(how=’all’, inplace=True)  \n",
    "# drop a row only if ALL values are missing\n",
    "\n",
    "# fill in missing values\n",
    "df.column_x.fillna(value=’NA’, inplace=True) \n",
    "\n",
    "# fill in missing values with ‘NA’\n",
    "\n",
    "# value does not have to equal a string — can be set as some calculated value like df.column_x.mode(), or just a number like 0\n",
    " \n",
    " \n",
    "\n",
    "# turn off the missing value filter\n",
    "df = pd.read_csv(‘df.csv’, header=0, names=new_cols, na_filter=False)\n",
    "‘’’\n",
    "Split-Apply-Combine\n",
    "Diagram: http://i.imgur.com/yjNkiwL.png\n",
    "‘’’\n",
    "\n",
    "# for each value in column_x, calculate the mean column_y \n",
    "df.groupby(‘column_x’).column_y.mean()\n",
    "\n",
    "# for each value in column_x, count the number of occurrences\n",
    "df.column_x.value_counts()\n",
    "\n",
    "# for each value in column_x, describe column_y\n",
    "df.groupby(‘column_x’).column_y.describe()\n",
    "\n",
    "# similar, but outputs a DataFrame and can be customized\n",
    "df.groupby(‘column_x’).column_y.agg([‘count’, ‘mean’, ‘min’, ‘max’])\n",
    "df.groupby(‘column_x’).column_y.agg([‘count’, ‘mean’, ‘min’, ‘max’]).sort_values(‘mean’)\n",
    "\n",
    "# if you don’t specify a column to which the aggregation function should be applied, it will be applied to all numeric columns\n",
    "df.groupby(‘column_x’).mean()\n",
    "df.groupby(‘column_x’).describe()\n",
    "\n",
    "# can also groupby a list of columns, i.e., for each combination of column_x and column_y, calculate the mean column_z\n",
    "df.groupby([“column_x”,”column_y”]).column_z.mean()\n",
    "\n",
    "#to take groupby results out of hierarchical index format (e.g., present as table), use .unstack() method\n",
    "df.groupby(“column_x”).column_y.value_counts().unstack()\n",
    "\n",
    "#conversely, if you want to transform a table into a hierarchical index, use the .stack() method\n",
    "df.stack()\n",
    "‘’’\n",
    "Selecting Multiple Columns and Filtering Rows\n",
    "‘’’\n",
    "\n",
    "# select multiple columns\n",
    "my_cols = [‘column_x’, ‘column_y’]  \n",
    "# create a list of column names…\n",
    "df[my_cols]                   \n",
    "# …and use that list to select columns\n",
    "df[[‘column_x’, ‘column_y’]]  \n",
    "# or, combine into a single step — double brackets due to indexing a list.\n",
    "\n",
    "# use loc to select columns by name\n",
    "df.loc[:, ‘column_x’]    \n",
    "# colon means “all rows”, then select one column\n",
    "df.loc[:, [‘column_x’, ‘column_y’]]  \n",
    "# select two columns\n",
    "df.loc[:, ‘column_x’:’column_y’]     \n",
    "# select a range of columns (i.e., selects all columns including first through last specified)\n",
    "\n",
    "# loc can also filter rows by “name” (the index)\n",
    "df.loc[0, :]       \n",
    "# row 0, all columns\n",
    "df.loc[0:2, :]     \n",
    "# rows 0/1/2, all columns\n",
    "df.loc[0:2, ‘column_x’:’column_y’] \n",
    "# rows 0/1/2, range of columns\n",
    "\n",
    "# use iloc to filter rows and select columns by integer position\n",
    "df.iloc[:, [0, 3]]     \n",
    "# all rows, columns in position 0/3\n",
    "df.iloc[:, 0:4]        \n",
    "# all rows, columns in position 0/1/2/3\n",
    "df.iloc[0:3, :]        \n",
    "# rows in position 0/1/2, all columns\n",
    "\n",
    "#filtering out and dropping rows based on condition (e.g., where column_x values are null)\n",
    "drop_rows = df[df[“column_x”].isnull()]\n",
    "new_df = df[~df.isin(drop_rows)].dropna(how=’all’)\n",
    " \n",
    " \n",
    " \n",
    "‘’’\n",
    "Merging and Concatenating Dataframes\n",
    "‘’’ \n",
    "\n",
    "#concatenating two dfs together (just smooshes them together, does not pair them in any meaningful way) - axis=1 concats df2 to right side of df1; axis=0 concats df2 to bottom of df1\n",
    "new_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "#merging dfs based on paired columns; columns do not need to have same name, but should match values; left_on column comes from df1, right_on column comes from df2\n",
    "new_df = pd.merge(df1, df2, left_on=’column_x’, right_on=’column_y’)\n",
    "\n",
    "#can also merge slices of dfs together, though slices need to include columns used for merging\n",
    "new_df = pd.merge(df1[[‘column_x1’, ‘column_x2’]], df2, left_on=’column_x2', right_on=’column_y’)\n",
    "\n",
    "#merging two dataframes based on shared index values (left is df1, right is df2)\n",
    "new_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    " \n",
    " \n",
    "‘’’\n",
    "Other Frequently Used Features\n",
    "‘’’\n",
    "\n",
    "# map existing values to a different set of values\n",
    "df[‘column_x’] = df.column_y.map({‘F’:0, ‘M’:1})\n",
    "\n",
    "# encode strings as integer values (automatically starts at 0)\n",
    "df[‘column_x_num’] = df.column_x.factorize()[0]\n",
    "\n",
    "# determine unique values in a column\n",
    "df.column_x.nunique()   \n",
    "# count the number of unique values\n",
    "df.column_x.unique()    \n",
    "# return the unique values\n",
    "\n",
    "# replace all instances of a value in a column (must match entire value)\n",
    "df.column_y.replace(‘old_string’, ‘new_string’, inplace=True)\n",
    "\n",
    "#alter values in one column based on values in another column (changes occur in place)\n",
    "\n",
    "#can use either .loc or .ix methods\n",
    "df.loc[df[“column_x”] == 5, “column_y”] = 1\n",
    " \n",
    "df.ix[df.column_x == “string_value”, “column_y”] = “new_string_value”\n",
    "\n",
    "#transpose data frame (i.e. rows become columns, columns become rows)\n",
    "df.T\n",
    "\n",
    "# string methods are accessed via ‘str’\n",
    "df.column_y.str.upper() \n",
    "# converts to uppercase\n",
    "df.column_y.str.contains(‘value’, na=’False’) \n",
    "# checks for a substring, returns boolean series\n",
    "\n",
    "# convert a string to the datetime_column format\n",
    "df[‘time_column’] = pd.to_datetime_column(df.time_column)\n",
    "df.time_column.dt.hour   \n",
    "# datetime_column format exposes convenient attributes\n",
    "(df.time_column.max() — df.time_column.min()).days   \n",
    "# also allows you to do datetime_column “math”\n",
    "df[df.time_column > pd.datetime_column(2014, 1, 1)]   \n",
    "# boolean filtering with datetime_column format\n",
    "\n",
    "# setting and then removing an index, resetting index can help remove hierarchical indexes while preserving the table in its basic structure\n",
    "df.set_index(‘time_column’, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# sort a column by its index\n",
    "df.column_y.value_counts().sort_index()\n",
    "\n",
    "# change the data type of a column\n",
    "df[‘column_x’] = df.column_x.astype(‘float’)\n",
    "\n",
    "# change the data type of a column when reading in a file\n",
    "pd.read_csv(‘df.csv’, dtype={‘column_x’:float})\n",
    "\n",
    "# create dummy variables for ‘column_x’ and exclude first dummy column\n",
    "column_x_dummies = pd.get_dummies(df.column_x).iloc[:, 1:]\n",
    "\n",
    "# concatenate two DataFrames (axis=0 for rows, axis=1 for columns)\n",
    "df = pd.concat([df, column_x_dummies], axis=1)\n",
    "‘’’\n",
    "Less Frequently Used Features\n",
    "‘’’\n",
    "\n",
    "# create a DataFrame from a dictionary\n",
    "pd.DataFrame({‘column_x’:[‘value_x1’, ‘value_x2’, ‘value_x3’], ‘column_y’:[‘value_y1’, ‘value_y2’, ‘value_y3’]})\n",
    "\n",
    "# create a DataFrame from a list of lists\n",
    "pd.DataFrame([[‘value_x1’, ‘value_y1’], [‘value_x2’, ‘value_y2’], [‘value_x3’, ‘value_y3’]], columns=[‘column_x’, ‘column_y’])\n",
    "\n",
    "# detecting duplicate rows\n",
    "df.duplicated()       \n",
    "# True if a row is identical to a previous row\n",
    "df.duplicated().sum() \n",
    "# count of duplicates\n",
    "df[df.duplicated()]   \n",
    "# only show duplicates\n",
    "df.drop_duplicates()  \n",
    "# drop duplicate rows\n",
    "df.column_z.duplicated()   \n",
    "# check a single column for duplicates\n",
    "df.duplicated([‘column_x’, ‘column_y’, ‘column_z’]).sum()  \n",
    "# specify columns for finding duplicates\n",
    "\n",
    "# Clean up missing values in multiple DataFrame columns\n",
    "df = df.fillna({\n",
    " ‘col1’: ‘missing’,\n",
    " ‘col2’: ‘99.999’,\n",
    " ‘col3’: ‘999’,\n",
    " ‘col4’: ‘missing’,\n",
    " ‘col5’: ‘missing’,\n",
    " ‘col6’: ‘99’\n",
    "})\n",
    "\n",
    "# Concatenate two DataFrame columns into a new, single column - (useful when dealing with composite keys, for example)\n",
    "df[‘newcol’] = df[‘col1’].map(str) + df[‘col2’].map(str)\n",
    "\n",
    "# Doing calculations with DataFrame columns that have missing values\n",
    "\n",
    "# In example below, swap in 0 for df[‘col1’] cells that contain null\n",
    "df[‘new_col’] = np.where(pd.isnull(df[‘col1’]),0,df[‘col1’]) + df[‘col2’]\n",
    " \n",
    "\n",
    "# display a cross-tabulation of two Series\n",
    "pd.crosstab(df.column_x, df.column_y)\n",
    "\n",
    "# alternative syntax for boolean filtering (noted as “experimental” in the documentation)\n",
    "df.query(‘column_z < 20’) \n",
    "# df[df.column_z < 20]\n",
    "df.query(“column_z < 20 and column_y==’string’”)  \n",
    "# df[(df.column_z < 20) & (df.column_y==’string’)]\n",
    "df.query(‘column_z < 20 or column_z > 60’)        \n",
    "# df[(df.column_z < 20) | (df.column_z > 60)]\n",
    "\n",
    "# Loop through rows in a DataFrame\n",
    "for index, row in df.iterrows():\n",
    " print index, row[‘column_x’]\n",
    "\n",
    "# Much faster way to loop through DataFrame rows if you can work with tuples\n",
    "for row in df.itertuples():\n",
    " print(row)\n",
    "\n",
    "# Get rid of non-numeric values throughout a DataFrame:\n",
    "for col in df.columns.values:\n",
    " df[col] = df[col].replace(‘[⁰-9]+.-’, ‘’, regex=True)\n",
    "\n",
    "# Change all NaNs to None (useful before loading to a db)\n",
    "df = df.where((pd.notnull(df)), None)\n",
    "\n",
    "# Split delimited values in a DataFrame column into two new columns\n",
    "df[‘new_col1’], df[‘new_col2’] = zip(*df[‘original_col’].apply(lambda x: x.split(‘: ‘, 1)))\n",
    "\n",
    "# Collapse hierarchical column indexes\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# display the memory usage of a DataFrame\n",
    "df.info()         \n",
    "# total usage\n",
    "df.memory_usage() \n",
    "# usage by column\n",
    "\n",
    "# change a Series to the ‘category’ data type (reduces memory usage and increases performance)\n",
    "df[‘column_y’] = df.column_y.astype(‘category’)\n",
    "\n",
    "# temporarily define a new column as a function of existing columns\n",
    "df.assign(new_column = df.column_x + df.spirit + df.column_y)\n",
    "\n",
    "# limit which rows are read when reading in a file\n",
    "pd.read_csv(‘df.csv’, nrows=10)        \n",
    "# only read first 10 rows\n",
    "pd.read_csv(‘df.csv’, skiprows=[1, 2]) \n",
    "# skip the first two rows of data\n",
    "\n",
    "# randomly sample a DataFrame\n",
    "train = df.sample(frac=0.75, random_column_y=1) \n",
    "# will contain 75% of the rows\n",
    "test = df[~df.index.isin(train.index)] \n",
    "# will contain the other 25%\n",
    "\n",
    "# change the maximum number of rows and columns printed (‘None’ means unlimited)\n",
    "pd.set_option(‘max_rows’, None) \n",
    "# default is 60 rows\n",
    "pd.set_option(‘max_columns’, None) \n",
    "# default is 20 columns\n",
    "print df\n",
    "\n",
    "# reset options to defaults\n",
    "pd.reset_option(‘max_rows’)\n",
    "pd.reset_option(‘max_columns’)\n",
    "\n",
    "# change the options temporarily (settings are restored when you exit the ‘with’ block)\n",
    "with pd.option_context(‘max_rows’, None, ‘max_columns’, None):\n",
    " print df\n",
    "\n",
    "# convert to numpy array\n",
    "df_num = df_num.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "558px",
    "left": "160px",
    "top": "277.5px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
