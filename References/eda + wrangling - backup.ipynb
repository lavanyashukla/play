{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "everything from data sourcing right up to, but not including, model building.\n",
    "\n",
    "\"identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data\" in the context of \"mapping data from one 'raw' form into another...\" all the way up to \"training a statistical model\" which I like to think of data preparation as encompassing, or \"everything from data sourcing right up to, but not including, model building.\"\n",
    "\n",
    "#### Cleansing:\n",
    "is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.\n",
    "#### Wrangling:\n",
    "...the process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools. This may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data munging as a process typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data using algorithms (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Choose a data set that you would like to work with.\n",
    "\n",
    "**STEP 2.** Identify a specific topic of interest\n",
    "\n",
    "**STEP 3.** Prepare a codebook of your own (i.e., print individual pages or copy screen and paste into a new document) from the larger codebook that includes the questions/items/variables that measure your selected topics.)\n",
    "\n",
    "**STEP 4.** Identify a second topic that you would like to explore in terms of its association with your original topic.\n",
    "\n",
    "**STEP 5.** Add questions/items/variables documenting this second topic to your personal codebook.\n",
    "\n",
    "**STEP 6.** Perform a literature review to see what research has been previously done on this topic. Use sites such as Google Scholar ([http://scholar.google.com](http://scholar.google.com/)) to search for published academic work in the area(s) of interest. Try to find multiple sources, and take note of basic bibliographic information.\n",
    "\n",
    "**STEP 7.** Based on your literature review, develop a hypothesis about what you believe the association might be between these topics. Be sure to integrate the specific variables you selected into the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a folder in your file system to hold all your files for the analysis\n",
    "- Create a documents/spreadsheets to store the names, titles, contact information and notes of all the people connected to your data\n",
    "- Find and introduce yourself to all the people connected to your data\n",
    "- Connections to others is key to making your projects work. The more you are visible to others the more information will freely pass your way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip File\n",
    "import zipfile\n",
    "# Extract all contents from zip file\n",
    "with zipfile.ZipFile('armenian-online-job-postings.zip', 'r') as myzip:\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df = pd.read_csv('patients.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error.\n",
    "\n",
    "*UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte*\n",
    "\n",
    "Use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "PS: Just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "import chardet\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n",
    "\n",
    "Set the parameter for rawdata.read(10000) to something higher/lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What process generates this data?\n",
    "* Is it generated from industrial equipment, a website, internal software?\n",
    "* When was it created?\n",
    "* How often is it updated?\n",
    "* What database(if any) is it stored in?\n",
    "* Who are the admins of the database?\n",
    "* Can you view the schema?\n",
    "* What is the process that the raw data has gone through before it reached your hands? Has it already been pre-processed before it reaches you?\n",
    "* Is there a data dictionary describing every column?\n",
    "* What systems use the data?\n",
    "* Have their been previous data scientists working with this dataset?\n",
    "* How has data changed over time? Which columns have been added/subtracted? \n",
    "* Is data for some columns not being collected?\n",
    "#### Subject Matter Research\n",
    "* Read articles, watch videos, talk to local subject matter experts\n",
    "* Read articles/papers by academics who have already studied the field using statistical analysis\n",
    "* Could be beneficial to do some analysis first as to not bias your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA + Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a data dictionary with the column name, data type, range of values and notes on each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA usually involves a combination of the following methods:\n",
    "\n",
    "* Univariate visualization of and summary statistics for each field in the raw dataset\n",
    "* Bivariate visualization and summary statistics for assessing the relationship between each variable in the dataset and the target variable of interest (e.g. time until churn, spend)\n",
    "* Multivariate visualizations to understand interactions between different fields in the data\n",
    "* Dimensionality reduction to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data\n",
    "* Clustering of similar observations in the dataset into differentiated groupings, which by collapsing the data into a few small data points, patterns of behavior can be more easily identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Is the Data Tidy?\n",
    "* Most data from relational databases will be tidy\n",
    "* Data from spreadsheets or scraped from the web/pdfs might not be\n",
    "* Find data type of each column - continuous, categorical (ordinal or nominal), or date. Fix if needed\n",
    "* Rearrange column order in a sensible manner - categorical first, continuous last. Group common variables together. Melt, Pivot\n",
    "* Spot data quality and data structure issues and any inconsistencies - are there quality issues we can see? are column headers mistakenly values, instead of variable names?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Summary Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many observations do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Example observations\n",
    "Then, you'll want to display example observations from the dataset. This will give you a \"feel\" for the values of each feature.\n",
    "\n",
    "Thank about:\n",
    "Do the columns make sense?\n",
    "Do the values in those columns make sense?\n",
    "Are the values on the right scale?\n",
    "Is missing data going to be a big problem based on a quick eyeball test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the data types of my features? Are they numeric? Categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names, check for spaces, capitalization inconsistencies\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I have a target variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Data Types\n",
    "Ensure all numerical columns are numerical. e.g. sometimes they maybe represented as:\n",
    "term                     36 months\n",
    "emp_length               10+ years\n",
    "revol_util                   83.7%\n",
    "\n",
    "Convert these to numerical values.\n",
    "\n",
    "Same for dates:\n",
    "pull_d        Sep-2016\n",
    "\n",
    "Convert these to datetime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if any columns are missing values\n",
    "# make sure every column's datatype is appropriate - obect, int, float, categorical, datetime etc. It's important to get them right to use pandas calculation/summary functions for that datatype.\n",
    "## If a column has a finite set of values it should be argued to be of categorical type, unless it has a lot of finite values. Categorical data with tons of categories isn't that useful.\n",
    "## Say you had one to a few observations from each country, it would probably be best to treat country like a string and group observations on a larger unit, like world_region (Africa, Asia, Central America, etc.). If you had a lot of observations from a few countries, like test scores from students sampled in a handful of countries, making country categorical would be more appropriate.\n",
    "# Use df[df['col_name'].isnull()] to find rows with missing values\n",
    "# make sure column names are descriptive\n",
    "# Tidiness - does the table contain more than 1 observational unit? are there more tables than needed because one observational unit is expressed in multiple tables?\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Univariate Analysis\n",
    "* Look at one variable at a time.\n",
    "\n",
    "### Categorical variables\n",
    "* There is less available options with categorical variables\n",
    "* Count the frequency of each variable\n",
    "* Low frequency strings might be outliers\n",
    "* You might want to relabel low frequency strings 'other'\n",
    "* Find the number of unique labels for each column\n",
    "* In pandas, change the data type to categorical (better when there aren't too many unique values)\n",
    "* Bar plots of counts\n",
    "* String columns allow for feature engineering by splitting the string, counting certain letters, finding the length of, etc... Feature engineering can be done later when modeling\n",
    "* See if a numerical/datetime variable has a string type incorrectly because there is an 'NA'/'missing' value or '%/$' other sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Way Frequency Table\n",
    "freq_table = pd.crosstab(index=df[\"Symbol\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "freq_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of each value\n",
    "prop = freq_table / freq_table.sum()\n",
    "prop = prop.sort_values('count', ascending=False)\n",
    "\n",
    "# Show top 10 values (by proportion) in column\n",
    "prop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher DImensional Frequency Table\n",
    "surv_sex_class = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                             columns=[titanic_train[\"Pclass\"],\n",
    "                                      titanic_train[\"Sex\"]],\n",
    "                             margins=True)\n",
    "surv_sex_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of each value\n",
    "prop = surv_sex_class/surv_sex_class.ix[\"All\"]    # Divide by column totals\n",
    "\n",
    "prop = prop.sort_values('count', ascending=False)\n",
    "\n",
    "# Show top 10 values (by proportion) in column\n",
    "prop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variables\n",
    "* There are a lot more options for continuous variables\n",
    "* Use the five number summary - with **`.describe`**\n",
    "* Boxplots are great ways to find outliers\n",
    "* Use histograms and kernel density estimators to visualize the distribution.\n",
    "* Know the shape of the distribution\n",
    "* Think about making categorical variables out of continuous variables by cutting them into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.col_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various methods of indexing and selecting data (.loc and bracket notation with/without boolean indexing, also .iloc)\n",
    "df.loc[df['city'] == \"New York\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate weird values, e.g.\n",
    "weight_lbs = patients[patients.surname == 'Zaitseva'].weight * 2.20462\n",
    "height_in = patients[patients.surname == 'Zaitseva'].height\n",
    "bmi_check = 703 * weight_lbs / (height_in * height_in)\n",
    "bmi_check\n",
    "patients[patients.surname == 'Zaitseva'].bmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use bootstrapping to get more 'samples'\n",
    "* Bootstrapping is done by resampling your data with replacement and gives you a 'new' random dataset\n",
    "* This helps you get multiple looks at the data\n",
    "* You can get estimates for the mean and variance of continuous columns this way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total missing values do we have?\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['col_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values exist. Missing vzalues may exist as dashes, slases, zeroes, NA, or none. Account for all of these. Also 0 and null are different because they lead to diff values of std, variance etc.)\n",
    "sum(df.col_name.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this value missing becuase it wasn't recorded or becuase it dosen't exist?\n",
    "\n",
    "This is the point at which we get into the part of data science that I like to call \"data intution\", by which I mean \"really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis\". It can be a frustrating part of data science, especially if you're newer to the field and don't have a lot of experience. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:\n",
    "\n",
    "Is this value missing becuase it wasn't recorded or becuase it dosen't exist?\n",
    "\n",
    "If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\" and we'll learn how to do it next! :)\n",
    "\n",
    "This is a great place to read over the dataset documentation if you haven't already! If you're working with a dataset that you've gotten from another person, you can also try reaching out to them to get more information.\n",
    "\n",
    "If you're doing very careful data analysis, this is the point at which you'd look at each column individually to figure out the best strategy for filling those missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "* Use your natural human ability to look at boxplots to find thresholds for what an outlier might be\n",
    "* Generate a new column of data that is 0/1 for outlier or not. This will quickly help you find them later.\n",
    "\n",
    "\n",
    "Outliers can cause problems with certain types of models. For example, linear regression models are less robust to outliers than decision tree models.\n",
    "\n",
    "In general, if you have a legitimate reason to remove an outlier, it will help your model’s performance.\n",
    "\n",
    "However, **outliers are innocent until proven guilty**. You should never remove an outlier just because it’s a \"big number.\" That big number could be very informative for your model.\n",
    "\n",
    "You must have a good reason for removing an outlier, such as suspicious measurements that are unlikely to be real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Z-score Method **\n",
    "\n",
    "The intuition behind the Z-score method of outlier detection is that, once we’ve centred and rescaled the data, anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier.\n",
    "\n",
    "The Z-score method relies on the mean and standard deviation of a group of data to measure central tendency and dispersion. This is troublesome, because the mean and standard deviation are highly affected by outliers – they are not robust. In fact, the skewing that outliers bring is one of the biggest reasons for finding and removing outliers from a dataset!\n",
    "\n",
    "So we use the modified Z-score method is that it uses the median and MAD rather than the mean and standard deviation. The median and MAD are robust measures of central tendency and dispersion, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all numerical columns\n",
    "df_num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_num\n",
    "\n",
    "# Calculate modified z_score\n",
    "for col in df_num:\n",
    "    df_outliers = df[(np.abs(df[col]-df[col].median()) > (3.5*df[col].mad()))]\n",
    "    \n",
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_modified_z_score(ys):\n",
    "    threshold = 3.5\n",
    "\n",
    "    median_y = np.median(ys)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in ys])\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y\n",
    "                         for y in ys]\n",
    "    return np.where(np.abs(modified_z_scores) > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another robust measure of dispersion.\n",
    "\n",
    "PS: One caveat: both of these methods will encounter problems with a strongly skewed dataset. If the data is distributed in a strongly asymmetrical fashion, it will need to be re-expressed before applying any of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others Ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if the min or max values are too far away from the avg value (this could be inaccurate or this could be because of a difference in units: one value in KGs while the others are in LBs)\n",
    "df.col_name.sort_values()\n",
    "\n",
    "# Fetch outliers\n",
    "very_large_value = 100000000\n",
    "df[df.col_name > very_large_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[np.abs(df.Data-df.Data.mean()) <= (3*df.Data.std())]\n",
    "# keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\n",
    "\n",
    "df[~(np.abs(df.Data-df.Data.mean()) > (3*df.Data.std()))]\n",
    "# or if you prefer the other way around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper/Lower bounded values\n",
    "Sometimes values are cut off at an arbitrary high or low value and therefore, the first and last value have a lot of datapoints. Check for this. This is very important to remember when assessing the generalizability of models trained on this later. Might want to mention this as part of dataset insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Duplicated data\n",
    "* Lots of data gets accidentally duplicated. Check for duplicates or near duplicates of rows and columns\n",
    "* If any columns are calculated entirely by that of another column or columns (like with depth from the diamonds data), ensure the calculation holds. \n",
    "\n",
    "Duplicate observations most frequently arise during data collection, such as when you:\n",
    "* Combine datasets from multiple places\n",
    "* Scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if there's any duplicate values(cause of nicknames/misspellings/multiple ways of expressing a value), or default 'John Doe' type values\n",
    "df[df.col_name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated columns\n",
    "# See if there are a lot of duplicated columns across tables which means that the tables could be combined. Ideally only the id column(s) should be common across tables\n",
    "all_columns = pd.Series(list(first) + list(second) + list(third))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional:) Remove Columns with only One Value\n",
    "These columns won't be useful for the model since they don't add any information to each loan application. In addition, removing these columns will reduce the number of columns we'll need to explore further in the next stage.\n",
    "\n",
    "The pandas Series method nunique() returns the number of unique values, excluding any null values. We can use apply this method across the dataset to remove these columns in one easy step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,df.apply(pd.Series.nunique) != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional:) Remove Columns with unique values that don't occur more than N times\n",
    "there may be some columns with more than one unique values but one of the values has insignificant frequency in the dataset. Let's find out and drop such column(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a good value for n here\n",
    "n = 4\n",
    "for col in df.columns:\n",
    "    if (len(df[col].unique()) < n):\n",
    "        print(df[col].value_counts())\n",
    "        print()\n",
    "df = df.drop('col_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevant data\n",
    "\n",
    "Irrelevant observations are those that don’t actually fit the specific problem that you’re trying to solve.\n",
    "\n",
    "For example, if you were building a model for Single-Family homes only, you wouldn't want observations for Apartments in there.\n",
    "This is also a great time to review your charts from Exploratory Analysis. You can look at the distribution charts for categorical features to see if there are any classes that shouldn’t be there.\n",
    "Checking for irrelevant observations before engineering features can save you many headaches down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Making new binary columns to label some finding\n",
    "* Just like it was described above to make a 0/1 column for outliers, you can do the same for any other finding\n",
    "* You can drop the duplicated rows or you can make a binary column labeling them. \n",
    "* Same for rows that do not have a correct calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typos, Inconsistent Capitalization in Categorical variables\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/typos-example-before.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for mislabeled classes\n",
    "\n",
    "i.e. separate classes that should really be the same.\n",
    "\n",
    "e.g. If ’N/A’ and ’Not Applicable’ appear as two separate classes, you should combine them.\n",
    "e.g. ’IT’ and ’information_technology’ should be a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization - Formatting Issues in other variable types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize all data types - zipcodes, emails, phone numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "OHE\n",
    "Log distribution transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Variables\n",
    "Often, a quick and dirty grid of histograms is enough to understand the distributions\n",
    "\n",
    "Here are a few things to look out for:\n",
    "\n",
    "- Distributions that are unexpected\n",
    "- Potential outliers that don't make sense\n",
    "- Features that should be binary (i.e. \"wannabe indicator variables\")\n",
    "- Boundaries that don't make sense\n",
    "- Potential measurement errors\n",
    "\n",
    "PS: When there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly. In such cases, it's good to look at the plot on a **log scale**. The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/histogram-grid-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables\n",
    "\n",
    "Caterogical variables can be visualized using bar plots.\n",
    "\n",
    "In particular, you'll want to look out for sparse classes, which are classes that have a very small number of observations.\n",
    "\n",
    "A \"class\" is simply a unique value for a categorical feature.CAterogical variables can be visualizing\n",
    "\n",
    "Classes with short bars are sparse classes. They tend to be problematic when building models.\n",
    "\n",
    "In the best case, they don't influence the model much.\n",
    "In the worse case, they can cause the model to be overfit.\n",
    "Therefore, make a note to combine or reassign some of these classes later.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/grouping-sparse-classes-before.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentations - Categorical + Numerical Variables Together\n",
    "\n",
    "Segmentations are powerful ways to observe the relationship between categorical features and numeric features.\n",
    "\n",
    "Box plots allow you to do so.\n",
    "\n",
    "Here are a few insights you could draw from the following chart.\n",
    "\n",
    "The median transaction price (middle vertical bar in the box) for Single-Family homes was much higher than that for Apartments / Condos / Townhomes.\n",
    "The min and max transaction prices are comparable between the two classes.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/boxplot-segmentation-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Univariate vs Bivariate and Graphical vs Non-Graphical\n",
    "\n",
    "| Univariate             | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical | Bar char of frequencies (count/percent) | Contingency table (count/percent) |\n",
    "| Continuous  | Histogram/rugplot/KDE, box/violin/swarm, qqplot, fat tails  | central tendency -mean/median/mode, spread - variance, std, skew, kurt, IQR  |\n",
    "\n",
    "| Bivariate/multivariate            | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical vs Categorical | heat map, mosaic plot | Two-way Contingency table (count/percent) |\n",
    "| Continuous vs Continuous  | all pairwise scatterplots, kde, heatmaps |  all pairwise correlation/regression   |\n",
    "| Categorical vs Continuous  | [bar, violin, swarm, point, strip seaborn plots](http://seaborn.pydata.org/tutorial/categorical.html)  | Summary statistics for each level |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate and Multivariate EDA\n",
    "\n",
    "Boxplots are great when you have a numeric column that you want to compare across different categories.\n",
    "When you want to visualize two numeric columns, scatter plots are ideal.\n",
    "\n",
    "### Categorical vs Categorical\n",
    "* Create two way contingency table of frequency counts\n",
    "* Create a heat map\n",
    "* Find expected counts and possibly do a chi-squared test\n",
    "\n",
    "### Categorical vs Continuous\n",
    "* Use the seaborn categorical plots\n",
    "\n",
    "### Continuous vs Continuous\n",
    "* Plot all combinations of scatterplots\n",
    "* Use a hierarchical clustering plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "\n",
    "In general, you should look out for:\n",
    "\n",
    "Which features are strongly correlated with the target variable?\n",
    "Are there interesting or unexpected strong correlations between other features?\n",
    "\n",
    "Note:\n",
    "Correlation is a value between -1 and 1 that represents how closely two features move in unison. You don't need to remember the math to calculate them. Just know the following intuition:\n",
    "\n",
    "Positive correlation means that as one feature increases, the other increases. E.g. a child’s age and her height.\n",
    "Negative correlation means that as one feature increases, the other decreases. E.g. hours spent studying and number of parties attended.\n",
    "Correlations near -1 or 1 indicate a strong relationship.\n",
    "Those closer to 0 indicate a weak relationship.\n",
    "0 indicates no relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessments From EDA To Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "##### `patients` table\n",
    "- Zip code is a float not a string\n",
    "- Zip code has four digits sometimes\n",
    "- Tim Neudorf height is 27 in instead of 72 in\n",
    "- Full state names sometimes, abbreviations other times\n",
    "- Dsvid Gustafsson\n",
    "- Missing demographic information (address - contact columns) ***(can't clean yet)***\n",
    "- Erroneous datatypes (assigned sex, state, zip_code, and birthdate columns)\n",
    "- Multiple phone number formats\n",
    "- Default John Doe data\n",
    "- Multiple records for Jakobsen, Gersten, Taylor\n",
    "- kgs instead of lbs for Zaitseva weight\n",
    "\n",
    "##### `treatments` table\n",
    "- Missing HbA1c changes\n",
    "- The letter 'u' in starting and ending doses for Auralin and Novodra\n",
    "- Lowercase given names and surnames\n",
    "- Missing records (280 instead of 350)\n",
    "- Erroneous datatypes (auralin and novodra columns)\n",
    "- Inaccurate HbA1c changes (leading 4s mistaken as 9s)\n",
    "- Nulls represented as dashes (-) in auralin and novodra columns\n",
    "\n",
    "##### `adverse_reactions` table\n",
    "- Lowercase given names and surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness\n",
    "- Contact column in `patients` table should be split into phone number and email\n",
    "- Three variables in two columns in `treatments` table (treatment, start dose and end dose)\n",
    "- Adverse reaction should be part of the `treatments` table\n",
    "- Given name and surname columns in `patients` table duplicated in `treatments` and `adverse_reactions` tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cope the dataset\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you deal with missing values – ignore or treat them? The answer would depend on the percentage of those missing values in the dataset, the variables affected by missing values, whether those missing values are a part of dependent or the independent variables, etc. Missing Value treatment becomes important since the data insights or the performance of your predictive model could be impacted if the missing values are not appropriately handled.\n",
    "\n",
    "There are all sorts of strategies for dealing with missing data, and none of them are applicable universally. Some people will say \"never use instances which include empty values.\" Others will argue \"never use an attribute's mean value to replace missing values.\" Conversely, you may hear more complex methods endorsed wholesale, such as \"only first clustering a dataset into the number of known classes and then using intra-cluster regression to calculate missing values is valid.\"\n",
    "\n",
    "Listen to none of this. \"Never\" and \"only\" and other inflexible assertions hold no value in the nuanced world of data finessing; different types of data and processes suggest different best practices for dealing with missing values. However, since this type of knowledge is both experience and domain based, we will focus on the more basic strategies which can be employed.\n",
    "\n",
    "<img src=\"https://d35fo82fjcw0y8.cloudfront.net/2016/03/03210602/missing_value_table.jpg\">\n",
    "\n",
    "Some common methods for dealing with missing values include:\n",
    "\n",
    "* dropping instances\n",
    "* dropping attributes\n",
    "* imputing the attribute mean for all missing values\n",
    "* imputing the attribute median for all missing values\n",
    "* imputing the attribute mode for all missing values\n",
    "* using regression to impute attribute missing values\n",
    "\n",
    "clearly the type of modeling methods being employed will have an effect on your decision -- for example, decision trees are not amenable to missing values. Additionally, you could technically entertain any statistical method you could think of for determining missing values from the dataset, but the listed approaches are tried, tested, and commonly used approaches.\n",
    "\n",
    "--\n",
    "\n",
    "The choice of method to deal with missing values can vary widely from one dataset to the next (largely determined by whether rows with missing values are intrinsically like or unlike those without missing values)\n",
    "\n",
    "You cannot simply ignore missing values in your dataset. You must handle them in some way for the very practical reason that most algorithms do not accept missing values.\n",
    "\n",
    "Dropping missing values is sub-optimal because when you drop observations, you drop information.\n",
    "\n",
    "The fact that the value was missing may be informative in itself.\n",
    "Plus, in the real world, you often need to make predictions on new data even if some of the features are missing!\n",
    "Imputing missing values is sub-optimal because the value was originally missing but you filled it in, which always leads to a loss in information, no matter how sophisticated your imputation method is.\n",
    "\n",
    "Again, \"missingness\" is almost always informative in itself, and you should tell your algorithm if a value was missing.\n",
    "Even if you build a model to impute your values, you’re not adding any real information. You’re just reinforcing the patterns already provided by other features.\n",
    "\n",
    "In short, you should always tell your algorithm that a value was missing because missingness is informative.\n",
    "\n",
    "#### Missing categorical data\n",
    "The best way to handle missing data for categorical features is to simply label them as ’Missing’!\n",
    "\n",
    "You’re essentially adding a new class for the feature.\n",
    "This tells the algorithm that the value was missing.\n",
    "This also gets around the technical requirement for no missing values.\n",
    "\n",
    "#### Missing numeric data\n",
    "For missing numeric data, you should flag and fill the values.\n",
    "\n",
    "Flag the observation with an indicator variable of missingness.\n",
    "Then, fill the original missing value with 0 just to meet the technical requirement of no missing values.\n",
    "By using this technique of flagging and filling, you are essentially allowing the algorithm to estimate the optimal constant for missingness, instead of just filling it in with the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: `Drop missing Values`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop missing values\n",
    "If you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: This is generally not recommended for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)\n",
    "\n",
    "Unless the nature of missing data is ‘Missing completely at random’, the best avoidable method in many cases is deletion.\n",
    "\n",
    "<img src=\"https://d35fo82fjcw0y8.cloudfront.net/2016/03/03210603/listwise-deletion.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes every row in dataset that has at least one missing value, not always good\n",
    "nfl_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns with at least one missing value\n",
    "columns_with_na_dropped = df.dropna(axis=1)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n many cases, you'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames.\n",
    "cols_with_missing = [col for col in original_data.columns \n",
    "                                 if original_data[col].isnull().any()]\n",
    "redued_original_data = original_data.drop(cols_with_missing, axis=1)\n",
    "reduced_test_data = test_data.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2:  `Imputation by Mean`\n",
    "\n",
    "Popular Averaging Techniques\n",
    "\n",
    "Mean, median and mode are the most popular averaging techniques, which are used to infer missing values. Approaches ranging from global average for the variable to averages based on groups are usually considered.\n",
    "\n",
    "For example: if you are inferring missing value for Revenue, you might assign the average defined by mean, median or mode to such missing value. You could also consider taking into account some other variables such as Gender of the User and/or the Device OS to calculate such an average to be assigned to the missing values.\n",
    "\n",
    "Though you can get a quick estimate of the missing values, you are artificially reducing the variation in the dataset as the missing observations could have the same value. This may impact the statistical analysis of the dataset since depending on the percentage of missing observations imputed, metrics such as mean, median, correlation, etc may get affected.\n",
    "\n",
    "<img src=\"https://d35fo82fjcw0y8.cloudfront.net/2016/03/03210602/imputation-by-averaging.jpg\">\n",
    "\n",
    "The above table shows the difference in imputed missing values of Revenue arrived by taking its global mean and mean based on which OS platform it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "my_imputer = Imputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)\n",
    "# The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  `Extending Imputation`\n",
    "Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy to avoid changing original data (when Imputing)\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "cols_with_missing = (col for col in new_data.columns \n",
    "                                 if new_data[c].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "new_data = my_imputer.fit_transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  `Fill In Missing Values Automatically `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NA's with 0\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NA's the value that comes directly after it in the same column, \n",
    "# then replace all the reamining na's with 0\n",
    "df.fillna(method = 'bfill', axis=0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3:  `Imputation by Predictive Techniques`\n",
    "\n",
    "Imputation of missing values from predictive techniques assumes that the nature of such missing observations are not observed completely at random and the variables chosen to impute such missing observations have some relationship with it, else it could yield imprecise estimates.\n",
    "\n",
    "In the examples discussed earlier, a predictive model could be used to impute the missing values for Device, OS, Revenues. There are various statistical methods like regression techniques, machine learning methods like SVM and/or data mining methods to impute such missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation by Regression\n",
    "# Assume ‘y’ depends on ‘x’\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns where all elements are missing values:\n",
    "df.dropna(axis=1, how='all')\n",
    "\n",
    "# Drop the columns where any of the elements are missing values\n",
    "df.dropna(axis=1, how='any')\n",
    "\n",
    "# Keep only the rows which contain 2 missing values maximum\n",
    "df.dropna(thresh=2)\n",
    "\n",
    "# Drop the columns where any of the elements are missing values\n",
    "df.dropna(axis=1, how='any')\n",
    "\n",
    "# Fill all missing values with the mean of the particular column\n",
    "df.fillna(df.mean())\n",
    "\n",
    "# Fill any missing value in column 'A' with the column median\n",
    "df['A'].fillna(df['A'].median())\n",
    "\n",
    "# Fill any missing value in column 'Depeche' with the column mode\n",
    "df['Depeche'].fillna(df['Depeche'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Fetch from another source`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Import the cut treatments into a DataFrame and concatenate it with the original treatments DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = pd.read_csv('df_cut.csv')\n",
    "df_clean = pd.concat([df_clean, df_cut],\n",
    "                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert anew_df.shape[0] == df.shape[0] + df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Calculate from df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Recalculate the `hba1c_change` column: `hba1c_start` minus `hba1c_end`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name = (df.col_name_start - df.col_name_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "There are times when including outliers in modeling is appropriate, and there are times when they are not (regardless of what anyone tries to tell you). This is situation-dependent, and no one can make sweeping assertions as to whether your situation belongs in column A or column B.\n",
    "\n",
    "Outliers can be the result of poor data collection, or they can be genuinely good, anomalous data. These are 2 different scenarios, and must be approached differently, and so no \"one size fits all\" advice is applicable here, similar to that of dealing with missing values.\n",
    "\n",
    "One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Classes\n",
    "\n",
    "So, what if your otherwise robust dataset -- lacking both missing values and outliers -- is made up of 2 classes: one which includes 95 percent of the instances, and the other which includes a mere 5 percent? Or worse -- 99.8 vs 0.2 percent?\n",
    "\n",
    "If so, your dataset is imbalanced, at least as far as the classes are concerned. This can be problematic, in ways which I'm sure do not need to be pointed out. But no need to to toss the data to the side yet; there are, of course, strategies for dealing with this.\n",
    "\n",
    "Note that, while this may not genuinely be a data preparation task, such a dataset characteristic will make itself known early in the data preparation stage (the importance of EDA), and the validity of such data can certainly be assessed preliminarily during this preparation stage.\n",
    "\n",
    "A good explanation of why we can run into imbalanced data, and why we can do so in some domains much more frequently than in others (from 7 Techniques to Handle Imbalanced Data, linked above):\n",
    "\n",
    "Data used in these areas often have less than 1% of rare, but “interesting” events (e.g. fraudsters using credit cards, user clicking advertisement or corrupted server scanning its network). However, most machine learning algorithms do not work very well with imbalanced datasets. The following seven techniques can help you, to train a classifier to detect the abnormal class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations\n",
    "\n",
    "Data transformation is the application of a deterministic mathematical function to each point in a data set — that is, each data point zi is replaced with the transformed value yi = f(zi), where f is a function. Transforms are usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that is to be applied, or to improve the interpretability or appearance of graphs.\n",
    "\n",
    "Transforming data is one of the most important aspects of data preparation, and one which requires more finesse than most others. When missing values manifest themselves in data, they are generally easy to find, and can be (at least, superficially) dealt with by one of the common methods outlined above -- or by more complex measures gained from insight over time in a domain. However, when and if data transformations are required -- to say nothing of the type of transformation required -- is often not as easily identifiable.\n",
    "\n",
    "A plethora of transformations exist; instead of trying to generalize when and why transformations are useful, let's look at a few specific transformations in order to get a better handle on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE\n",
    "One-hot encoding \"transforms categorical features to a format that works better with classification and regression algorithms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transformation\n",
    "The log distribution transformation can be useful if \"you assume a model form that is non-linear but can be transformed to a linear model\" (taken from below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "While melting takes a set of columns and turns it into a single column, pivoting will create a new column for each unique value in a specified column.\n",
    "\n",
    "<img src=\"wrangling-notebook-melt-pivot-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Melt` many columns in table contain one variable\n",
    "Used to fix columns containing values, instead of variables.\n",
    "\n",
    "Explanation friendly -> analysis friendly shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(treatments_clean, id_vars=['given_name', 'surname', 'hba1c_start', 'hba1c_end', 'hba1c_change'],\n",
    "                           value_vars = ['col1', 'col2'], var_name='treatment', value_name='dose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Pivot` one column in table contain many variables\n",
    "Used to fix multiple variables being stored in the same column\n",
    "\n",
    "Analysis friendly -> Explanation friendly shape\n",
    "\n",
    "Pivoting doesn't work if there are duplicate values. Use pivot table method to specify how to reconcile duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot and reset index\n",
    "# columns (the name of the column you want to pivot), and\n",
    "# values (the values to be used when the column is pivoted)\n",
    "\n",
    "# reset_index - pivot_table returns a pandas DataFrame with a hierarchical index (also known as a MultiIndex) -  In essence, they allow you to group columns or rows by another variable - in this case, by 'Keep' as well as 'Fixed'. To fix it to intended form, use reset_index()\n",
    "df = df.pivot_table(index=['Keep', 'Fixed'], columns='measurement', values='reading').reset_index()\n",
    "\n",
    "\n",
    "# By using .pivot_table() and the aggfunc parameter, you can not only reshape your data, but also remove duplicates. Finally, you can then flatten the columns of the pivoted DataFrame using .reset_index().\n",
    "# Removing Duplicate Values\n",
    "# Pivot airquality_dup: airquality_pivot\n",
    "df = df_dup.pivot_table(index=['Keep', 'Fixed'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Melt the *auralin* and *novodra* columns to a *treatment* and a *dose* column (dose will still contain both start and end dose at this point). Then split the dose column on ' - ' to obtain *start_dose* and *end_dose* columns. Drop the intermediate *dose* column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Columns\n",
    "df = df[df.dose != \"-\"]\n",
    "df['dose_start'], df['dose_end'] = df['dose'].str.split(' - ', 1).str\n",
    "df = df.drop('dose', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Extract` one column in table contains many variables: phone number and email\n",
    "\n",
    "E.g. here the column name has multiple values, so there is an extra first step to melt the columns.\n",
    "<img src=\"wrangling-notebook-multiple-values-in-col-example.png\">\n",
    "\n",
    "This is the resulting dataset:\n",
    "<img src=\"wrangling-notebook-multiple-values-in-col-fixed-example.png\">\n",
    "\n",
    "Now we can apply the extraction functions on the values in the column like below.\n",
    "\n",
    "We can use extract, split, or string slicing like def.col_name[k:l], or string functions - whatever works best in this situation.\n",
    "\n",
    "E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt['str_split'].str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt['str_split'].str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Extract the *phone number* and *email* variables from the *contact* column using regular expressions and pandas' `str.extract` method. Drop the *contact* column when done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg expression to get phone nmbers:\n",
    "r_phone = r'([\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9])'\n",
    "# Reg expression to get emails:\n",
    "r_email = r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n",
    "\n",
    "df['phone_number'] = df.col_name.str.extract('((?:\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})', expand=True)\n",
    "df['email'] = df.col_name.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.][a-zA-Z]+)', expand=True)\n",
    "# Note: axis=1 denotes that we are referring to a column, not a row\n",
    "df = df.drop('col_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm contact column is gone\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.col_name.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that no emails start with an integer (regex didn't match for this)\n",
    "df.col_name.sort_values().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Merge & Concat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concat**\n",
    "\n",
    "Think of it as sticking together data that was once a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row uber1, uber2, and uber3\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "# Column concatenate uber1, uber2, and uber3\n",
    "col_concat = pd.concat([uber1, uber2, uber3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating Many Files using GLOB\n",
    "# * = any arbitrary num of characters\n",
    "# ? = matches one alphanumeric character\n",
    "import glob\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge**\n",
    "\n",
    "Merging data allows you to combine disparate datasets into a single dataset to do more complex analysis.\n",
    "\n",
    "Used when you can't concatenate data because the ordering of the rows isn't the same.\n",
    "\n",
    "Think of it as combining disparate datasets based on a commong set of columns.\n",
    "\n",
    "Types of merges:\n",
    "* one-to-one\n",
    "* many-to-one / one-to-many\n",
    "* many-to-many\n",
    "\n",
    "<img src=\"wrangling-notebook-merge-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-to-One Merge**\n",
    "\n",
    "E.g.\n",
    "<img src=\"wrangling-notebook-merge-one-to-one-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many-to-One Merge**\n",
    "\n",
    "E.g.\n",
    "<img src=\"wrangling-notebook-merge--one-to-many-example.png\">\n",
    "\n",
    "**Many-to-Many Merge**\n",
    "\n",
    "E.g.\n",
    "<img src=\"wrangling-notebook-merge--many-to-many-example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code to do all merges\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Merge the *adverse_reaction* column to the `treatments` table, joining on *given name* and *surname*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df2, on=['given_name', 'surname'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns in one table duplicated in other tables and\n",
    "#### `Lowercase column names`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Adverse reactions table is no longer needed so ignore that part. Isolate the patient ID and names in the `patients` table, then convert these names to lower case to join with `treatments`. Then drop the given name and surname columns in the treatments table (so these being lowercase isn't an issue anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_names = df[['patient_id', 'given_name', 'surname']]\n",
    "id_names.given_name = id_names.given_name.str.lower()\n",
    "id_names.surname = id_names.surname.str.lower()\n",
    "df = pd.merge(df, id_names, on=['given_name', 'surname'])\n",
    "df = df.drop(['given_name', 'surname'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm the merge was executed correctly\n",
    "treatments_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient ID should be the only duplicate column\n",
    "all_columns = pd.Series(list(patients_clean) + list(treatments_clean))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "values = array(data)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling, you're changing the range of your data while in normalization you're changing the shape of the distribution of your data.\n",
    "\n",
    "By scaling your variables, you can help compare different variables on equal footing. You are transforming your data so that it fits within a specific scale, like 0-100 or 0-1. You want to scale data when you're using methods based on measures of how far apart data points, like support vector machines, or SVM or k-nearest neighbors, or KNN. With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n",
    "\n",
    "Scaling improves output of regression model and even clustering improves.\n",
    "\n",
    "Scaling just changes the range of your data.\n",
    "\n",
    "Discussion on the difference between Scaling and Normalization:\n",
    "\n",
    "Scaling is a column operation and normalization is a row operation. The standard scaling process is transform all features independently such that they each have a mean of zero and a standard deviation of one. The standard normalization process is to transform all features for a given case to a unit vector.\n",
    "\n",
    "Scaling is particularly important for linear models; it ensures that all inputs are treated equally in the regularization process, and allows one to choose a meaningful range for random starting weights.\n",
    "\n",
    "Normalization can be important when training neural networks, to prevent exploding or vanishing gradients. It can also help to improve convergence time.\n",
    "\n",
    "normalization in the context of math/physics is to create a normal vector, or a vector with unit length. This unit length refers to Euclidean distance, so you need to consider all dimensions of your training data (all columns).\n",
    "\n",
    "You could use normalization to scale a single column, but it's not very efficient. You'd get the same results from a MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "// Normal distribution: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox (Box-Cox Transformation: https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation)\n",
    "normalized_data = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text\n",
    "\n",
    "There are many different encodings, and if you tried to read in text with a different encoding that the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n",
    "\n",
    "æ–‡å—åŒ–ã??\n",
    "\n",
    "You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n",
    "\n",
    "����������\n",
    "\n",
    "Character encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n",
    "\n",
    "UTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble.\n",
    "\n",
    "You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a cd player. If you try to play a cassette in a CD player, it just won't work.\n",
    "\n",
    "Note: We can run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ascii using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!\n",
    "\n",
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character encoding module\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistent Data Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = df['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this.\n",
    "\n",
    "# convert to lower case\n",
    "df['City'] = df['City'].str.lower()\n",
    "# remove trailing white spaces\n",
    "df['City'] = df['City'].str.strip()\n",
    "# Strip u\n",
    "df.dose_start = df.dose_start.str.strip('u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fuzzy matching to correct inconsistent data entry\n",
    "# Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n",
    "# Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings.\n",
    "\n",
    "# get the top 10 closest matches to \"d.i khan\"\n",
    "matches = fuzzywuzzy.process.extract(\"a. b. value\", df, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "# take a look at them\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function above to replace close matches\n",
    "# Choose a good value for min_ratio, by looking at the ratio of similar values in the matches table above\n",
    "replace_matches_in_column(df=df, column='City', string_to_match=\"a. b. value\", min_ratio = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = df['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erroneous Datatypes\n",
    "(assigned sex, state, zip_code, and birthdate columns) and Erroneous datatypes (auralin and novodra columns) and\n",
    "\n",
    "#### The letter 'u' in starting and ending doses for Auralin and Novodra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Convert assigned sex and state to categorical data types. Zip code data type was already addressed above. Convert birthdate to datetime data type. Strip the letter 'u' in start dose and end dose and convert those columns to data type integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the datatypes of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To category (makes DF smaller in memory if # of categories is small)\n",
    "df.assigned_sex = df.assigned_sex.astype('category')\n",
    "df.state = df.state.astype('category')\n",
    "\n",
    "# To string\n",
    "df.astr = df.astr.astype(str)\n",
    "\n",
    "# To datetime\n",
    "df.birthdate = pd.to_datetime(df.birthdate)\n",
    "\n",
    "# Strip u and to integer\n",
    "df.dose_start = df.dose_start.str.astype(int)\n",
    "df.dose_end = df.dose_end.str.astype(int)\n",
    "\n",
    "# String to numeric with errors = 'coerce' to set non-int values like '-' or 'None' to NaN\n",
    "# You can use the pd.to_numeric() function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. You can either use some exploratory data analysis techniques and find the bad value, or you can choose to ignore or coerce the value into a missing value, NaN.\n",
    "tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Remove the Jake Jakobsen, Pat Gersten, and Sandy Taylor rows from the `patients` table. These are the nicknames, which happen to also not be in the `treatments` table (removing the wrong name would create a consistency issue between the `patients` and `treatments` table). These are all the second occurrence of the duplicate. These are also the only occurences of non-null duplicate addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tilde means not: http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\n",
    "df = df[~((df.address.duplicated()) & df.address.notnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.surname == 'Jakobsen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.surname == 'Gersten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.surname == 'Taylor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually fix one off errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Replace height for rows in the `patients` table that have a height of 27 in (there is only one) with 72 in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.height = df.height.replace(27, 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be empty\n",
    "df[df.height == 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the replacement worked\n",
    "df[df.surname == 'Neudorf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Replace given name for rows in the `patients` table that have a given name of 'Dsvid' with 'David'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.given_name = df.given_name.replace('Dsvid', 'David')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.surname == 'Gustafsson']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default John Doe data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Remove the non-recoverable John Doe records from the `patients` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.surname != 'Doe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be no Doe records\n",
    "df.surname.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be no 123 Main Street records\n",
    "df.address.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Number From String\n",
    "\n",
    "e.g. 'the recipe calls for 6 strawberries and 2 bananas'\n",
    "\n",
    "When using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the re.findall() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of all the numeric values\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monetary Values\n",
    "\n",
    "* 17\n",
    "\n",
    "* $17\n",
    "\n",
    "* $17.89\n",
    "\n",
    "* $17.892\n",
    "\n",
    "* $17892.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "result = pattern.match('$17.89')\n",
    "bool(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first few rows of the date column. Check the data type of our date column\n",
    "print(landslides['date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine format of the dates above, and pass it in format parameter (http://strftime.org/)\n",
    "# create a new column, date_parsed, with the parsed dates\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")\n",
    "\n",
    "# If we get a multiple datatypes error, have pandas infer what the format should be\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)\n",
    "\n",
    "# We don't always use infer = true for 2 reasons: The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the day of the month from the date_parsed column\n",
    "day_of_month_landslides = landslides['date_parsed'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the day of the month to check the date parsing\n",
    "# One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense.\n",
    "# To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.)\n",
    "\n",
    "# remove na's\n",
    "day_of_month_landslides = day_of_month_landslides.dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip code\n",
    "is a float not a string and Zip code has four digits sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Convert the zip code column's data type from a float to a string using `astype`, remove the '.0' using string slicing, and pad four digit zip codes with a leading 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.zip_code = df.zip_code.astype(str).str[:-2].str.pad(5, fillchar='0')\n",
    "# Reconvert NaNs entries that were converted to '0000n' by code above\n",
    "df.zip_code = df.zip_code.replace('0000n', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.zip_code.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Strip all \" \", \"-\", \"(\", \")\", and \"+\" and store each number without any formatting. Pad the phone number with a 1 if the length of the number is 10 digits (we want country code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phone_number = df.phone_number.str.replace(r'\\D+', '').str.pad(11, fillchar='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg expression to get phone nmbers:\n",
    "r_phone = r'([\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9])'\n",
    "\n",
    "df['phone_number'] = df.col_name.str.extract('((?:\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.phone_number.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg expression to get emails:\n",
    "r_email = r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)'\n",
    "\n",
    "df['email'] = df.col_name.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.][a-zA-Z]+)', expand=True)\n",
    "# Note: axis=1 denotes that we are referring to a column, not a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Names\n",
    "Full sometimes, abbreviations other times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Apply a function that converts full state name to state abbreviation for California, New York, Illinois, Florida, and Nebraska."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from full state name to abbreviation\n",
    "state_abbrev = {'California': 'CA',\n",
    "                'New York': 'NY',\n",
    "                'Illinois': 'IL',\n",
    "                'Florida': 'FL',\n",
    "                'Nebraska': 'NE'}\n",
    "\n",
    "# Function to apply\n",
    "def abbreviate_state(patient):\n",
    "    if patient['state'] in state_abbrev.keys():\n",
    "        abbrev = state_abbrev[patient['state']]\n",
    "        return abbrev\n",
    "    else:\n",
    "        return patient['state']\n",
    "    \n",
    "df['state'] = df.apply(abbreviate_state, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "kgs instead of lbs and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n",
    "Use [advanced indexing](https://stackoverflow.com/a/44913631) to isolate the row where the surname is Zaitseva and convert the entry in its weight field from kg to lbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_kg = df.weight.min()\n",
    "mask = df.surname == 'Zaitseva'\n",
    "column_name = 'weight'\n",
    "df.loc[mask, column_name] = weight_kg * 2.20462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 48.8 shouldn't be the lowest anymore\n",
    "df.weight.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Cleaning\n",
    "\n",
    "For when the cleaning Step requires multiple steps:\n",
    "e.g.\n",
    "- extract number from string\n",
    "- perform transformation on extracted number\n",
    "\n",
    "We can write a function to apply these transformations to every row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Cleaning out 2 monetary coumns and calculating their difference in a 3rd column\n",
    "def diff_money(row, pattern):\n",
    "    icost = row['Initial Cost']\n",
    "    tef = row['Total Est. Fee']\n",
    "\n",
    "    if bool(pattern.match(icost)) and bool(pattern.match(tef)):\n",
    "    icost = icost.replace(\"$\",\n",
    "    \"\")\n",
    "    tef = tef.replace(\"$\",\n",
    "    \"\")\n",
    "\n",
    "    icost = float(icost)\n",
    "    tef = float(tef)\n",
    "\n",
    "    return icost - tef\n",
    "    else:\n",
    "    return(NaN)\n",
    "\n",
    "df_subset['diff'] = df_subset.apply(diff_money, axis=1, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Encode values of a column (Male/Female) as 0/1\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)\n",
    "\n",
    "# Print the first five rows of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Replace $ sign in monetary value string\n",
    "\n",
    "# Using 2 ways to do this:\n",
    "\n",
    "# lambda function using replace\n",
    "tips['total_dollar_replace'] = tips['total_dollar'].apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips['total_dollar'].apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Sparse Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unused Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded a function score_dataset(X_train, X_test, y_train, y_test) to compare the quality of diffrent approaches to missing values. This function reports the out-of-sample MAE score from a RandomForest.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataland Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataland Functions\n",
    "\n",
    "# Join 2 dataframes\n",
    "dataland_join(df_a, df_b):\n",
    "    return pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "# Send a file, get a dataframe\n",
    "dataland_read(file_to_read):\n",
    "    # TODO: Determine file type and pick appropriate function to read file\n",
    "    pd_from_file = pd.read_csv(file_to_read)\n",
    "    return pd_from_file;\n",
    "\n",
    "# Drops a column from a dataframe\n",
    "dataland_drop_cols(df_a, drop_cols):\n",
    "    return df_a.drop(drop_cols, axis=1)\n",
    "\n",
    "# Copy a column from a datafram\n",
    "dataland_copy(df_a):\n",
    "    return df_a.copy()\n",
    "\n",
    "# Split a column\n",
    "\n",
    "# Strip\n",
    "\n",
    "# Merge columns\n",
    "dataland_copy(df_a, df_b, on_cols, how):\n",
    "    return pd.merge(df_a, df_b, on=on_cols, how=how)\n",
    "\n",
    "# Melt columns\n",
    "dataland_melt(df_a, id_vars, value_vars, var_name, value_name):\n",
    "    return pd.melt(df_a, id_vars=id_vars, value_vars=value_vars, var_name=var_name, value_name=value_name)\n",
    "\n",
    "# Pivot\n",
    "\n",
    "# Replace\n",
    "\n",
    "# Duplicated\n",
    "\n",
    "# IsNull\n",
    "\n",
    "\n",
    "## Standardize Columns\n",
    "\n",
    "# Standardize phone number format\n",
    "dataland_standardize_phone(df_a, phone_col):\n",
    "    r_phone = r'([\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9])'\n",
    "    return df_a[phone_col].str.extract('((?:\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})', expand=True)\n",
    "\n",
    "# Standardize email format\n",
    "dataland_standardize_email(df_a, email_col):\n",
    "    return df_a[email_col].str.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.][a-zA-Z]+)', expand=True)\n",
    "\n",
    "# Make string lowercase\n",
    "dataland_standardize_lowercase(df_a, col):\n",
    "    return df_a['col'].str.lower()\n",
    "\n",
    "# Convert Type\n",
    "\n",
    "# DateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide uncleaned data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key point: let users download both cleaned and uncleaned data, because the test set for their models will need to come from the uncleaned data (to match real world data they’ll see), while the training data will come from the cleaned data.\n",
    "\n",
    "Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data is a first step, and pipelines can help implement this separation. Leaking predictors are a more frequent issue, and leaking predictors are harder to track down. A combination of caution, common sense and data exploration can help identify leaking predictors so you remove them from your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinds of Leaky Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky Validation Strategy\n",
    "This type of leak occurs when you aren't careful distinguishing training data from validation data. **For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split.**\n",
    "\n",
    "Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n",
    "\n",
    "#### Preventing Leaky Validation Strategies\n",
    "If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. This is easier if you use scikit-learn Pipelines. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky Predictors\n",
    "This occurs when your predictors include data that will not be available at the time you make predictions.\n",
    "\n",
    "For example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:\n",
    "\n",
    "got_pneumonia\tage\tweight\tmale\ttook_antibiotic_medicine\t...\n",
    "False\t65\t100\tFalse\tFalse\t...\n",
    "False\t72\t130\tTrue\tFalse\t...\n",
    "True\t58\t100\tFalse\tTrue\t...\n",
    "\n",
    "\n",
    "People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But took_antibiotic_medicine is frequently changed after the value for got_pneumonia is determined. This is target leakage.\n",
    "\n",
    "The model would see that anyone who has a value of False for took_antibiotic_medicine didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.\n",
    "\n",
    "To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n",
    "\n",
    "<img src=\"https://i.imgur.com/CN4INKb.png\">\n",
    "\n",
    "#### Preventing Leaky Predictors\n",
    "There is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.\n",
    "\n",
    "However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n",
    "\n",
    "To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n",
    "If you build a model and find it extremely accurate, you likely have a leakage problem.\n",
    "\n",
    "E.g. We should use cross-validation to ensure accurate measures of model quality.\n",
    "\n",
    "If CV score is very high (e.g. 98%), we know from experience that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = data.card\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\n",
    "modeling_pipeline = make_pipeline(RandomForestClassifier())\n",
    "cv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())\n",
    "# Cross-val accuracy: 0.979528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, basic data comparisons can be very helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditures_cardholders = data.expenditure[data.card]\n",
    "expenditures_noncardholders = data.expenditure[~data.card]\n",
    "\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean()))\n",
    "\n",
    "# Fraction of those who received a card with no expenditures: 0.02\n",
    "# Fraction of those who received a card with no expenditures: 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone with card == False had no expenditures, while only 2% of those with card == True had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means *expenditures on the card they applied for.**.\n",
    "\n",
    "Since share is partially determined by expenditure, it should be excluded too. The variables active, majorcards are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\n",
    "\n",
    "We would run a model without leakage as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "cv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())\n",
    "# Cross-val accuracy: 0.806677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get in the Mindset of a Detective\n",
    "- As an analyst, it is your job to extract information from data\n",
    "- Go beyond the keyboard to investigate as if you are detective\n",
    "- Have courage to ask the important questions\n",
    "- You can also think of yourself as making a documentary about the data. - - You will ultimately tell some story about it. Make it accurate and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think About\n",
    "Training Data -> Cleaned Training Data -- Train Models on cleaned data\n",
    "Testing/Real World Data -- Can you predict directly on it or do you need to transform it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Python Code Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippets for Pandas\n",
    "import pandas as pd\n",
    "‘’’\n",
    "Reading Files, Selecting Columns, and Summarizing\n",
    "‘’’\n",
    "\n",
    "\n",
    "# reading in a file from local computer or directly from a URL\n",
    "\n",
    "# various file formats that can be read in out wrote out\n",
    "‘’’\n",
    "Format Type     Data Description      Reader           Writer\n",
    "text                  CSV            read_csv          to_csv\n",
    "text                 JSON            read_json         to_json\n",
    "text                 HTML            read_html         to_html\n",
    "text             Local clipboard  read_clipboard     to_clipboard\n",
    "binary             MS Excel          read_excel        to_excel\n",
    "binary            HDF5 Format        read_hdf           to_hdf\n",
    "binary           Feather Format     read_feather      to_feather\n",
    "binary              Msgpack         read_msgpack      to_msgpack\n",
    "binary               Stata           read_stata        to_stata\n",
    "binary                SAS             read_sas \n",
    "binary        Python Pickle Format   read_pickle       to_pickle\n",
    "SQL                   SQL             read_sql          to_sql\n",
    "SQL             Google Big Query      read_gbq          to_gbq\n",
    "‘’’\n",
    "\n",
    "#to read about different types of files, and further functionality of reading in files, visit: http://pandas.pydata.org/pandas-docs/version/0.20/io.html\n",
    "df = pd.read_csv(‘local_path/file.csv’)\n",
    "df = pd.read_csv(‘https://file_path/file.csv')\n",
    "\n",
    "# when reading in tables, can specify separators, and note a column to be used as index separators can include tabs (“\\t”), commas(“,”), pipes (“|”), etc.\n",
    "df = pd.read_table(‘https://file_path/file', sep=’|’, index_col=’column_x’)\n",
    "\n",
    "# examine the df data\n",
    "df           \n",
    "# print the first 30 and last 30 rows\n",
    "type(df)     \n",
    "# DataFrame\n",
    "df.head()    \n",
    "# print the first 5 rows\n",
    "df.head(10)  \n",
    "# print the first 10 rows\n",
    "df.tail()    \n",
    "# print the last 5 rows\n",
    "df.index     \n",
    "# “the index” (aka “the labels”)\n",
    "df.columns   \n",
    "# column names (which is “an index”)\n",
    "df.dtypes    \n",
    "# data types of each column\n",
    "df.shape     \n",
    "# number of rows and columns\n",
    "df.values    \n",
    "# underlying numpy array — df are stored as numpy arrays for effeciencies.\n",
    "\n",
    "# select a column\n",
    "df[‘column_y’]         \n",
    "# select one column\n",
    "type(df[‘column_y’])   \n",
    "# determine datatype of column (e.g., Series)\n",
    "df.column_y            \n",
    "# select one column using the DataFrame attribute — not effective if column names have spaces\n",
    "\n",
    "# summarize (describe) the DataFrame\n",
    "df.describe()          \n",
    "# describe all numeric columns\n",
    "df.describe(include=[‘object’]) \n",
    "# describe all object columns\n",
    "df.describe(include=’all’)      \n",
    "# describe all columns\n",
    "\n",
    "# summarize a Series\n",
    "df.column_y.describe()   \n",
    "# describe a single column\n",
    "df.column_z.mean()       \n",
    "# only calculate the mean\n",
    "df[“column_z”].mean()    \n",
    "# alternate method for calculating mean\n",
    " \n",
    "\n",
    "# count the number of occurrences of each value\n",
    "df.column_y.value_counts()   \n",
    "# most useful for categorical variables, but can also be used with numeric variables\n",
    "\n",
    "#filter df by one column, and print out values of another column\n",
    "\n",
    "#when using numeric values, no quotations\n",
    "df[df.column_y == “string_value”].column_z\n",
    "df[df.column_y == 20 ].column_z    \n",
    " \n",
    "\n",
    "# display only the number of rows of the ‘df’ DataFrame\n",
    "df.shape[0]\n",
    "\n",
    "# display the 3 most frequent occurances of column in ‘df’\n",
    "df.column_y.value_counts()[0:3]\n",
    "‘’’\n",
    "Filtering and Sorting\n",
    "‘’’\n",
    "\n",
    "# boolean filtering: only show df with column_z < 20\n",
    "filter_bool = df.column_z < 20    \n",
    "# create a Series of booleans…\n",
    "df[filter_bool]                \n",
    "# …and use that Series to filter rows\n",
    "df[filter_bool].describe()     \n",
    "# describes a data frame filtered by filter_bool\n",
    "df[df.column_z < 20]           \n",
    "# or, combine into a single step\n",
    "df[df.column_z < 20].column_x  \n",
    "# select one column from the filtered results\n",
    "df[df[“column_z”] < 20].column_x     \n",
    "# alternate method \n",
    "df[df.column_z < 20].column_x.value_counts()   \n",
    "# value_counts of resulting Series, can also use .mean(), etc. instead of .value_counts()\n",
    "\n",
    "# boolean filtering with multiple conditions; indexes are in square brackets, conditions are in parens\n",
    "df[(df.column_z < 20) & (df.column_y==’string’)] \n",
    "# ampersand for AND condition \n",
    "df[(df.column_z < 20) | (df.column_z > 60)] \n",
    "# pipe for OR condition\n",
    "\n",
    "# sorting\n",
    "df.column_z.order()          \n",
    "# sort a column\n",
    "df.sort_values(‘column_z’)   \n",
    "# sort a DataFrame by a single column\n",
    "df.sort_values(‘column_z’, ascending=False)     \n",
    "# use descending order instead\n",
    "\n",
    "# Sort dataframe by multiple columns\n",
    "df = df.sort([‘col1’,’col2',’col3'],ascending=[1,1,0]) \n",
    " \n",
    "\n",
    "# can also filter ‘df’ using pandas.Series.isin \n",
    "df[df.column_x.isin([“string_1”, “string_2”])]\n",
    "‘’’\n",
    "Renaming, Adding, and Removing Columns\n",
    "‘’’\n",
    "\n",
    "# rename one or more columns\n",
    "df.rename(columns={‘original_column_1’:’column_x’, ‘original_column_2’:’column_y’}, inplace=True) \n",
    "#saves changes \n",
    " \n",
    "\n",
    "# replace all column names (in place)\n",
    "new_cols = [‘column_x’, ‘column_y’, ‘column_z’]\n",
    "df.columns = new_cols\n",
    "\n",
    "# replace all column names when reading the file\n",
    "df = pd.read_csv(‘df.csv’, header=0, names=new_cols)\n",
    "\n",
    "# add a new column as a function of existing columns\n",
    "df[‘new_column_1’] = df.column_x + df.column_y\n",
    "df[‘new_column_2’] = df.column_x * 1000   \n",
    "#can create new columns without for loops\n",
    "\n",
    "# removing columns\n",
    "df.drop(‘column_x’, axis=1)   \n",
    "# axis=0 for rows, 1 for columns — does not drop in place\n",
    "df.drop([‘column_x’, ‘column_y’], axis=1, inplace=True) \n",
    "# drop multiple columns\n",
    "\n",
    "# Lower-case all DataFrame column names\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# Even more fancy DataFrame column re-naming\n",
    "\n",
    "# lower-case all DataFrame column names (for example)\n",
    "df.rename(columns=lambda x: x.split(‘.’)[-1], inplace=True)\n",
    " \n",
    " \n",
    "‘’’\n",
    "Handling Missing Values\n",
    "‘’’\n",
    "\n",
    "# missing values are usually excluded by default\n",
    "df.column_x.value_counts()             \n",
    "# excludes missing values\n",
    "df.column_x.value_counts(dropna=False) \n",
    "# includes missing values\n",
    "\n",
    "# find missing values in a Series\n",
    "df.column_x.isnull()  \n",
    "# True if missing\n",
    "df.column_x.notnull() \n",
    "# True if not missing\n",
    "\n",
    "# use a boolean Series to filter DataFrame rows\n",
    "df[df.column_x.isnull()]  \n",
    "# only show rows where column_x is missing\n",
    "df[df.column_x.notnull()] \n",
    "# only show rows where column_x is not missing\n",
    "\n",
    "# understanding axes\n",
    "df.sum()       \n",
    "# sums “down” the 0 axis (rows)\n",
    "df.sum(axis=0) \n",
    "# equivalent (since axis=0 is the default)\n",
    "df.sum(axis=1) \n",
    "# sums “across” the 1 axis (columns)\n",
    "\n",
    "# adding booleans\n",
    "pd.Series([True, False, True])       \n",
    "# create a boolean Series\n",
    "pd.Series([True, False, True]).sum() \n",
    "# converts False to 0 and True to 1\n",
    "\n",
    "# find missing values in a DataFrame\n",
    "df.isnull() \n",
    "# DataFrame of booleans\n",
    "df.isnull().sum() \n",
    "# count the missing values in each column\n",
    "\n",
    "# drop missing values\n",
    "df.dropna(inplace=True)   \n",
    "# drop a row if ANY values are missing, defaults to rows, but can be applied to columns with axis=1\n",
    "df.dropna(how=’all’, inplace=True)  \n",
    "# drop a row only if ALL values are missing\n",
    "\n",
    "# fill in missing values\n",
    "df.column_x.fillna(value=’NA’, inplace=True) \n",
    "\n",
    "# fill in missing values with ‘NA’\n",
    "\n",
    "# value does not have to equal a string — can be set as some calculated value like df.column_x.mode(), or just a number like 0\n",
    " \n",
    " \n",
    "\n",
    "# turn off the missing value filter\n",
    "df = pd.read_csv(‘df.csv’, header=0, names=new_cols, na_filter=False)\n",
    "‘’’\n",
    "Split-Apply-Combine\n",
    "Diagram: http://i.imgur.com/yjNkiwL.png\n",
    "‘’’\n",
    "\n",
    "# for each value in column_x, calculate the mean column_y \n",
    "df.groupby(‘column_x’).column_y.mean()\n",
    "\n",
    "# for each value in column_x, count the number of occurrences\n",
    "df.column_x.value_counts()\n",
    "\n",
    "# for each value in column_x, describe column_y\n",
    "df.groupby(‘column_x’).column_y.describe()\n",
    "\n",
    "# similar, but outputs a DataFrame and can be customized\n",
    "df.groupby(‘column_x’).column_y.agg([‘count’, ‘mean’, ‘min’, ‘max’])\n",
    "df.groupby(‘column_x’).column_y.agg([‘count’, ‘mean’, ‘min’, ‘max’]).sort_values(‘mean’)\n",
    "\n",
    "# if you don’t specify a column to which the aggregation function should be applied, it will be applied to all numeric columns\n",
    "df.groupby(‘column_x’).mean()\n",
    "df.groupby(‘column_x’).describe()\n",
    "\n",
    "# can also groupby a list of columns, i.e., for each combination of column_x and column_y, calculate the mean column_z\n",
    "df.groupby([“column_x”,”column_y”]).column_z.mean()\n",
    "\n",
    "#to take groupby results out of hierarchical index format (e.g., present as table), use .unstack() method\n",
    "df.groupby(“column_x”).column_y.value_counts().unstack()\n",
    "\n",
    "#conversely, if you want to transform a table into a hierarchical index, use the .stack() method\n",
    "df.stack()\n",
    "‘’’\n",
    "Selecting Multiple Columns and Filtering Rows\n",
    "‘’’\n",
    "\n",
    "# select multiple columns\n",
    "my_cols = [‘column_x’, ‘column_y’]  \n",
    "# create a list of column names…\n",
    "df[my_cols]                   \n",
    "# …and use that list to select columns\n",
    "df[[‘column_x’, ‘column_y’]]  \n",
    "# or, combine into a single step — double brackets due to indexing a list.\n",
    "\n",
    "# use loc to select columns by name\n",
    "df.loc[:, ‘column_x’]    \n",
    "# colon means “all rows”, then select one column\n",
    "df.loc[:, [‘column_x’, ‘column_y’]]  \n",
    "# select two columns\n",
    "df.loc[:, ‘column_x’:’column_y’]     \n",
    "# select a range of columns (i.e., selects all columns including first through last specified)\n",
    "\n",
    "# loc can also filter rows by “name” (the index)\n",
    "df.loc[0, :]       \n",
    "# row 0, all columns\n",
    "df.loc[0:2, :]     \n",
    "# rows 0/1/2, all columns\n",
    "df.loc[0:2, ‘column_x’:’column_y’] \n",
    "# rows 0/1/2, range of columns\n",
    "\n",
    "# use iloc to filter rows and select columns by integer position\n",
    "df.iloc[:, [0, 3]]     \n",
    "# all rows, columns in position 0/3\n",
    "df.iloc[:, 0:4]        \n",
    "# all rows, columns in position 0/1/2/3\n",
    "df.iloc[0:3, :]        \n",
    "# rows in position 0/1/2, all columns\n",
    "\n",
    "#filtering out and dropping rows based on condition (e.g., where column_x values are null)\n",
    "drop_rows = df[df[“column_x”].isnull()]\n",
    "new_df = df[~df.isin(drop_rows)].dropna(how=’all’)\n",
    " \n",
    " \n",
    " \n",
    "‘’’\n",
    "Merging and Concatenating Dataframes\n",
    "‘’’ \n",
    "\n",
    "#concatenating two dfs together (just smooshes them together, does not pair them in any meaningful way) - axis=1 concats df2 to right side of df1; axis=0 concats df2 to bottom of df1\n",
    "new_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "#merging dfs based on paired columns; columns do not need to have same name, but should match values; left_on column comes from df1, right_on column comes from df2\n",
    "new_df = pd.merge(df1, df2, left_on=’column_x’, right_on=’column_y’)\n",
    "\n",
    "#can also merge slices of dfs together, though slices need to include columns used for merging\n",
    "new_df = pd.merge(df1[[‘column_x1’, ‘column_x2’]], df2, left_on=’column_x2', right_on=’column_y’)\n",
    "\n",
    "#merging two dataframes based on shared index values (left is df1, right is df2)\n",
    "new_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    " \n",
    " \n",
    "‘’’\n",
    "Other Frequently Used Features\n",
    "‘’’\n",
    "\n",
    "# map existing values to a different set of values\n",
    "df[‘column_x’] = df.column_y.map({‘F’:0, ‘M’:1})\n",
    "\n",
    "# encode strings as integer values (automatically starts at 0)\n",
    "df[‘column_x_num’] = df.column_x.factorize()[0]\n",
    "\n",
    "# determine unique values in a column\n",
    "df.column_x.nunique()   \n",
    "# count the number of unique values\n",
    "df.column_x.unique()    \n",
    "# return the unique values\n",
    "\n",
    "# replace all instances of a value in a column (must match entire value)\n",
    "df.column_y.replace(‘old_string’, ‘new_string’, inplace=True)\n",
    "\n",
    "#alter values in one column based on values in another column (changes occur in place)\n",
    "\n",
    "#can use either .loc or .ix methods\n",
    "df.loc[df[“column_x”] == 5, “column_y”] = 1\n",
    " \n",
    "df.ix[df.column_x == “string_value”, “column_y”] = “new_string_value”\n",
    "\n",
    "#transpose data frame (i.e. rows become columns, columns become rows)\n",
    "df.T\n",
    "\n",
    "# string methods are accessed via ‘str’\n",
    "df.column_y.str.upper() \n",
    "# converts to uppercase\n",
    "df.column_y.str.contains(‘value’, na=’False’) \n",
    "# checks for a substring, returns boolean series\n",
    "\n",
    "# convert a string to the datetime_column format\n",
    "df[‘time_column’] = pd.to_datetime_column(df.time_column)\n",
    "df.time_column.dt.hour   \n",
    "# datetime_column format exposes convenient attributes\n",
    "(df.time_column.max() — df.time_column.min()).days   \n",
    "# also allows you to do datetime_column “math”\n",
    "df[df.time_column > pd.datetime_column(2014, 1, 1)]   \n",
    "# boolean filtering with datetime_column format\n",
    "\n",
    "# setting and then removing an index, resetting index can help remove hierarchical indexes while preserving the table in its basic structure\n",
    "df.set_index(‘time_column’, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# sort a column by its index\n",
    "df.column_y.value_counts().sort_index()\n",
    "\n",
    "# change the data type of a column\n",
    "df[‘column_x’] = df.column_x.astype(‘float’)\n",
    "\n",
    "# change the data type of a column when reading in a file\n",
    "pd.read_csv(‘df.csv’, dtype={‘column_x’:float})\n",
    "\n",
    "# create dummy variables for ‘column_x’ and exclude first dummy column\n",
    "column_x_dummies = pd.get_dummies(df.column_x).iloc[:, 1:]\n",
    "\n",
    "# concatenate two DataFrames (axis=0 for rows, axis=1 for columns)\n",
    "df = pd.concat([df, column_x_dummies], axis=1)\n",
    "‘’’\n",
    "Less Frequently Used Features\n",
    "‘’’\n",
    "\n",
    "# create a DataFrame from a dictionary\n",
    "pd.DataFrame({‘column_x’:[‘value_x1’, ‘value_x2’, ‘value_x3’], ‘column_y’:[‘value_y1’, ‘value_y2’, ‘value_y3’]})\n",
    "\n",
    "# create a DataFrame from a list of lists\n",
    "pd.DataFrame([[‘value_x1’, ‘value_y1’], [‘value_x2’, ‘value_y2’], [‘value_x3’, ‘value_y3’]], columns=[‘column_x’, ‘column_y’])\n",
    "\n",
    "# detecting duplicate rows\n",
    "df.duplicated()       \n",
    "# True if a row is identical to a previous row\n",
    "df.duplicated().sum() \n",
    "# count of duplicates\n",
    "df[df.duplicated()]   \n",
    "# only show duplicates\n",
    "df.drop_duplicates()  \n",
    "# drop duplicate rows\n",
    "df.column_z.duplicated()   \n",
    "# check a single column for duplicates\n",
    "df.duplicated([‘column_x’, ‘column_y’, ‘column_z’]).sum()  \n",
    "# specify columns for finding duplicates\n",
    "\n",
    "# Clean up missing values in multiple DataFrame columns\n",
    "df = df.fillna({\n",
    " ‘col1’: ‘missing’,\n",
    " ‘col2’: ‘99.999’,\n",
    " ‘col3’: ‘999’,\n",
    " ‘col4’: ‘missing’,\n",
    " ‘col5’: ‘missing’,\n",
    " ‘col6’: ‘99’\n",
    "})\n",
    "\n",
    "# Concatenate two DataFrame columns into a new, single column - (useful when dealing with composite keys, for example)\n",
    "df[‘newcol’] = df[‘col1’].map(str) + df[‘col2’].map(str)\n",
    "\n",
    "# Doing calculations with DataFrame columns that have missing values\n",
    "\n",
    "# In example below, swap in 0 for df[‘col1’] cells that contain null\n",
    "df[‘new_col’] = np.where(pd.isnull(df[‘col1’]),0,df[‘col1’]) + df[‘col2’]\n",
    " \n",
    "\n",
    "# display a cross-tabulation of two Series\n",
    "pd.crosstab(df.column_x, df.column_y)\n",
    "\n",
    "# alternative syntax for boolean filtering (noted as “experimental” in the documentation)\n",
    "df.query(‘column_z < 20’) \n",
    "# df[df.column_z < 20]\n",
    "df.query(“column_z < 20 and column_y==’string’”)  \n",
    "# df[(df.column_z < 20) & (df.column_y==’string’)]\n",
    "df.query(‘column_z < 20 or column_z > 60’)        \n",
    "# df[(df.column_z < 20) | (df.column_z > 60)]\n",
    "\n",
    "# Loop through rows in a DataFrame\n",
    "for index, row in df.iterrows():\n",
    " print index, row[‘column_x’]\n",
    "\n",
    "# Much faster way to loop through DataFrame rows if you can work with tuples\n",
    "for row in df.itertuples():\n",
    " print(row)\n",
    "\n",
    "# Get rid of non-numeric values throughout a DataFrame:\n",
    "for col in df.columns.values:\n",
    " df[col] = df[col].replace(‘[⁰-9]+.-’, ‘’, regex=True)\n",
    "\n",
    "# Change all NaNs to None (useful before loading to a db)\n",
    "df = df.where((pd.notnull(df)), None)\n",
    "\n",
    "# Split delimited values in a DataFrame column into two new columns\n",
    "df[‘new_col1’], df[‘new_col2’] = zip(*df[‘original_col’].apply(lambda x: x.split(‘: ‘, 1)))\n",
    "\n",
    "# Collapse hierarchical column indexes\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# display the memory usage of a DataFrame\n",
    "df.info()         \n",
    "# total usage\n",
    "df.memory_usage() \n",
    "# usage by column\n",
    "\n",
    "# change a Series to the ‘category’ data type (reduces memory usage and increases performance)\n",
    "df[‘column_y’] = df.column_y.astype(‘category’)\n",
    "\n",
    "# temporarily define a new column as a function of existing columns\n",
    "df.assign(new_column = df.column_x + df.spirit + df.column_y)\n",
    "\n",
    "# limit which rows are read when reading in a file\n",
    "pd.read_csv(‘df.csv’, nrows=10)        \n",
    "# only read first 10 rows\n",
    "pd.read_csv(‘df.csv’, skiprows=[1, 2]) \n",
    "# skip the first two rows of data\n",
    "\n",
    "# randomly sample a DataFrame\n",
    "train = df.sample(frac=0.75, random_column_y=1) \n",
    "# will contain 75% of the rows\n",
    "test = df[~df.index.isin(train.index)] \n",
    "# will contain the other 25%\n",
    "\n",
    "# change the maximum number of rows and columns printed (‘None’ means unlimited)\n",
    "pd.set_option(‘max_rows’, None) \n",
    "# default is 60 rows\n",
    "pd.set_option(‘max_columns’, None) \n",
    "# default is 20 columns\n",
    "print df\n",
    "\n",
    "# reset options to defaults\n",
    "pd.reset_option(‘max_rows’)\n",
    "pd.reset_option(‘max_columns’)\n",
    "\n",
    "# change the options temporarily (settings are restored when you exit the ‘with’ block)\n",
    "with pd.option_context(‘max_rows’, None, ‘max_columns’, None):\n",
    " print df\n",
    "\n",
    "# convert to numpy array\n",
    "df_num = df_num.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
