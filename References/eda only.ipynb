{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid runtime error messages\n",
    "pd.set_option('display.float_format', lambda x:'%f'%x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What process generates this data?\n",
    "* Is it generated from industrial equipment, a website, internal software?\n",
    "* When was it created?\n",
    "* How often is it updated?\n",
    "* What database(if any) is it stored in?\n",
    "* Who are the admins of the database?\n",
    "* Can you view the schema?\n",
    "* What is the process that the raw data has gone through before it reached your hands? Has it already been pre-processed before it reaches you?\n",
    "* Is there a data dictionary describing every column?\n",
    "* What systems use the data?\n",
    "* Have their been previous data scientists working with this dataset?\n",
    "* How has data changed over time? Which columns have been added/subtracted? \n",
    "* Is data for some columns not being collected?\n",
    "#### Subject Matter Research\n",
    "* Read articles, watch videos, talk to local subject matter experts\n",
    "* Read articles/papers by academics who have already studied the field using statistical analysis\n",
    "* Could be beneficial to do some analysis first as to not bias your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a folder in your file system to hold all your files for the analysis\n",
    "- Create a documents/spreadsheets file to store the names, titles, contact information and notes of all the people connected to your data\n",
    "- Find and introduce yourself to all the people connected to your data\n",
    "- Connections to others is key to making your projects work. The more you are visible to others the more information will freely pass your way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: Files -- Reading various filetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip File\n",
    "import zipfile\n",
    "# Extract all contents from zip file\n",
    "with zipfile.ZipFile('armenian-online-job-postings.zip', 'r') as myzip:\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df = pd.read_csv('patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading TSV files\n",
    "df = pd.read_csv('bestofrt.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "df = pd.read_json('patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle file\n",
    "df = pd.read_pickle('adataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error.\n",
    "\n",
    "*UnicodeDecodeError: 'utf-8' codec can't decode byte 0x99 in position 11: invalid start byte*\n",
    "\n",
    "Use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "PS: Just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "import chardet\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that.\n",
    "\n",
    "Set the parameter for rawdata.read(10000) to something higher/lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "### Using os.listdir()\n",
    "# List of dictionaries to build file by file and later convert to a DataFrame\n",
    "df_list = []\n",
    "folder = 'rt_html'\n",
    "\n",
    "for movie_html in os.listdir(folder):\n",
    "    with open(os.path.join(folder, movie_html)) as file:\n",
    "        # Your code here\n",
    "        # Note: a correct implementation may take ~15 seconds to run\n",
    "\t\t\t\t# Parse and Clean data from webpage\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "        title = soup.find('title').contents[0][:-len(\" - Rotten Tomatoes\")]\n",
    "        audience_score = soup.find('div', class_='audience-score').find('span', class_='superPageFontColor').contents[0][:-1]\n",
    "        num_audience_ratings = soup.find('div', class_='audience-info').find_all('div')[1].contents[2].strip().replace(',','')\n",
    "        \n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'title': title,\n",
    "                        'audience_score': int(audience_score),\n",
    "                        'number_of_audience_ratings': int(num_audience_ratings)})\n",
    "df = pd.DataFrame(df_list, columns = ['title', 'audience_score', 'number_of_audience_ratings'])\n",
    "\n",
    "### Using glob.glob()\n",
    "# List of dictionaries to build file by file and later convert to a DataFrame\n",
    "df_list = []\n",
    "for ebert_review in glob.glob('ebert_reviews/*.txt'):\n",
    "    with open(ebert_review, encoding='utf-8') as file:\n",
    "        title = file.readline()[:-1]\n",
    "        review_url = file.readline()[:-1]\n",
    "        review_text = file.read()[:-1]\n",
    "        \n",
    "        break:\n",
    "\n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'title': title,\n",
    "                        'review_url': review_url,\n",
    "                        'review_text': review_text})\n",
    "df = pd.DataFrame(df_list, columns = ['title', 'review_url', 'review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: Downloading Files From The Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.contents is in bytes format, not text, therefore we need to open the file in 'wb' (write binary) mode.\n",
    "\n",
    "import requests\n",
    "import os\n",
    "# Make directory if it doesn't already exist\n",
    "folder_name = 'ebert_reviews'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "ebert_review_urls = ['https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9900_1-the-wizard-of-oz-1939-film/1-the-wizard-of-oz-1939-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9901_2-citizen-kane/2-citizen-kane.txt']\n",
    "\n",
    "# Implement the code in the video above in a for loop for all Ebert reviews\n",
    "for url in ebert_review_urls:\n",
    "    response = requests.get(url)\n",
    "    # print (response.content)\n",
    "    with open(os.path.join(folder_name,url.split('/')[-1]), mode=\"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: Mashup of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If collecting data from multiple sources, ensure there's a column with unique values that you can use to merge between 2 sources. With multiple sources, these bridging unique values can be different columns for different sets of files being merged.\n",
    "\n",
    "Loops through the Wikipedia page titles in *title_list* and:\n",
    "\n",
    "- ***Stores the ranking of that movie in the Top 100 list based on its position in title_list. Ranking is needed so we can join this DataFrame with the master DataFrame later*.** We can't join on title because the titles of the Rotten Tomatoes pages and the Wikipedia pages differ.\n",
    "- Uses **`[try` and `except` blocks](http://www.pythonforbeginners.com/error-handling/python-try-and-except)** to attempt to query MediaWiki for a movie poster image URL and to attempt to download that image. If the attempt fails and an error is encountered, the offending movie is documented in *image_errors*.\n",
    "- Appends a dictionary with *ranking*, *title*, and *poster_url* as the keys and the extracted values for each as the values to *df_list*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wptools\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "title_list = [\n",
    " 'The_Wizard_of_Oz_(1939_film)',\n",
    " 'Citizen_Kane',\n",
    " 'The_Third_Man',\n",
    " 'Get_Out_(film)',\n",
    " 'Mad_Max:_Fury_Road',\n",
    " 'The_Cabinet_of_Dr._Caligari',\n",
    " 'All_About_Eve',\n",
    " 'Inside_Out_(2015_film)',\n",
    " 'The_Godfather',\n",
    " 'Metropolis_(1927_film)',\n",
    " 'E.T._the_Extra-Terrestrial',\n",
    " 'Modern_Times_(film)']\n",
    "\n",
    "folder_name = 'bestofrt_posters'\n",
    "# Make directory if it doesn't already exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# List of dictionaries to build and convert to a DataFrame later\n",
    "df_list = []\n",
    "image_errors = {}\n",
    "for title in title_list:\n",
    "    try:\n",
    "        # This cell is slow so print ranking to gauge time remaining\n",
    "        ranking = title_list.index(title) + 1\n",
    "        print(ranking)\n",
    "        page = wptools.page(title, silent=True)\n",
    "        # Your code here (three lines)\n",
    "        images = page.get().data['image']\n",
    "        # First image is usually the poster\n",
    "        first_image_url = images[0]['url']\n",
    "        r = requests.get(first_image_url)\n",
    "        # Download movie poster image\n",
    "        i = Image.open(BytesIO(r.content))\n",
    "        image_file_format = first_image_url.split('.')[-1]\n",
    "        i.save(folder_name + \"/\" + str(ranking) + \"_\" + title + '.' + image_file_format)\n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'ranking': int(ranking),\n",
    "                        'title': title,\n",
    "                        'poster_url': first_image_url})\n",
    "    \n",
    "    # Not best practice to catch all exceptions but fine for this short script\n",
    "    except Exception as e:\n",
    "        print(str(ranking) + \"_\" + title + \": \" + str(e))\n",
    "        image_errors[str(ranking) + \"_\" + title] = images\n",
    "\n",
    "for key in image_errors.keys():\n",
    "    print(key)\n",
    "\n",
    "# Inspect unidentifiable images and download them individually\n",
    "for rank_title, images in image_errors.items():\n",
    "    if rank_title == '22_A_Hard_Day%27s_Night_(film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/4/47/A_Hard_Days_night_movieposter.jpg'\n",
    "    if rank_title == '53_12_Angry_Men_(1957_film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/9/91/12_angry_men.jpg'\n",
    "    if rank_title == '72_Rosemary%27s_Baby_(film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/e/ef/Rosemarys_baby_poster.jpg'\n",
    "    if rank_title == '93_Harry_Potter_and_the_Deathly_Hallows_–_Part_2':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/d/df/Harry_Potter_and_the_Deathly_Hallows_%E2%80%93_Part_2.jpg'\n",
    "    title = rank_title[3:]\n",
    "    df_list.append({'ranking': int(title_list.index(title) + 1),\n",
    "                    'title': title,\n",
    "                    'poster_url': url})\n",
    "    r = requests.get(url)\n",
    "    # Download movie poster image\n",
    "    i = Image.open(BytesIO(r.content))\n",
    "    image_file_format = url.split('.')[-1]\n",
    "    i.save(folder_name + \"/\" + rank_title + '.' + image_file_format)\n",
    "\n",
    "# Create DataFrame from list of dictionaries\n",
    "df = pd.DataFrame(df_list, columns = ['ranking', 'title', 'poster_url'])\n",
    "df = df.sort_values('ranking').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to CSV\n",
    "Great for simple datasets.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Lack of standards\n",
    "- Data redundancy\n",
    "- Sharing data can be cumbersome\n",
    "- Not great for large datasets (see *\"When does small become large?\"* in the Cornell link in *More Information*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"adataset_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Database\n",
    "Pros:\n",
    "\n",
    "Fast. Scales with data, can handle large amounts of data. Can be queried. Can access data where it lives, no need to make copies. Easy to audit and replicate the analysis. Can run queries on multiple tables. Can answer deeper questions than regular dashboards. Check for data integrity (for e.g. type of column type). Can be accessed by multiple people concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('master', engine, index=False)\n",
    "df_gather = pd.read_sql('SELECT * FROM master', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data wrangling, it's recommended that databases and SQL only come into play for gathering data or storing data. That is:\n",
    "\n",
    "- **Connecting to a database and importing data** into a pandas DataFrame (or the analogous data structure in your preferred programming language), then assessing and cleaning that data, or\n",
    "- **Connecting to a database and storing data** you just gathered (which could potentially be from a database), assessed, and cleaned\n",
    "\n",
    "These tasks are especially necessary when you have large amounts of data, which is where SQL and other databases excel over flat files.\n",
    "\n",
    "The two scenarios above can be further broken down into three main tasks:\n",
    "\n",
    "- Connecting to a database in Python\n",
    "- Storing data ***from*** a pandas DataFrame ***in*** a database to which you're connected, and\n",
    "- Importing data ***from*** a database to which you're connected ***to*** a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring or EDA** will help you augment your data to maximize the potential of your analysis, viz and model. We use visualizations to summarize our data's main characteristics. From there, we remove outliers, create more descriptive features from existing ones (feature engineering.)\n",
    "\n",
    "e.g.\n",
    "- Using summary statistics like `count` on the state column or `mean` on the weight column to see if patients from certain states or of certain weights are more likely to have diabetes, which we can use to exclude certain patients from the analysis and make it less biased.\n",
    "\n",
    "* Create a data dictionary with the column name, data type, range of values and notes on each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA usually involves a combination of the following methods:\n",
    "\n",
    "* **Univariate** visualization of and summary statistics for each field in the raw dataset, specifically for variables of interest\n",
    "* **Bivariate** visualization and summary statistics for assessing the relationship between each variable in the dataset and the target variable of interest (e.g. time until churn, spend)\n",
    "* **Multivariate** visualizations to understand interactions between different fields in the data\n",
    "* Dimensionality reduction to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data\n",
    "* Clustering of similar observations in the dataset into differentiated groupings, which by collapsing the data into a few small data points, patterns of behavior can be more easily identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More data usually results in better accuracy than a finely trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many observations are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Example observations\n",
    "Then, you'll want to display example observations from the dataset. This will give you a \"feel\" for the values of each feature.\n",
    "\n",
    "Thank about:\n",
    "Do the columns make sense?\n",
    "Do the values in those columns make sense?\n",
    "Are the values on the right scale?\n",
    "Is missing data going to be a big problem based on a quick eyeball test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a datapoint of interest look like?\n",
    "\n",
    "What values across different features might it have?\n",
    "\n",
    "Make a list of them.\n",
    "\n",
    "How many POI datapoints exist?\n",
    "\n",
    "What information do you need to google to better inform your intuition around POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some interesting questions that you want to ask of this data?\n",
    "\n",
    "Make a list of them.\n",
    "\n",
    "Questions about POIs, total datapoints that have a certain value for a feature and how many of these are POIs, what values do certain features take on, how are missing values denoted, what % of datapoints have values for interesting features? Are Nans/0s associated with one type of categorical value for the label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Univariate Analysis\n",
    "* Look at one variable at a time.\n",
    "* Summarize and examize\n",
    "\n",
    "Distribution of a variable:\n",
    "* What values the variable takes\n",
    "* How often the variable takes those values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types in Pandas\n",
    "<img src=\"pandas_dtypes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "* There is less available options with categorical variables\n",
    "* Count the frequency of each variable\n",
    "* Low frequency strings might be outliers\n",
    "* You might want to relabel low frequency strings 'other'\n",
    "* Find the number of unique labels for each column\n",
    "* In pandas, change the data type to categorical (better when there aren't too many unique values)\n",
    "* Bar plots of counts\n",
    "* String columns allow for feature engineering by splitting the string, counting certain letters, finding the length of, etc... Feature engineering can be done later when modeling\n",
    "* See if a numerical/datetime variable has a string type incorrectly because there is an 'NA'/'missing' value or '%/$' other sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on df.describe()\n",
    "* df.describe(include='all') - Describing all columns of a DataFrame regardless of data type.\n",
    "* df.describe(include=[np.number]) - Including only numeric columns\n",
    "* df.describe(include=[np.object]) - Including only string columns\n",
    "* df.describe(include=['category']) - Including only categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all features on type categorical/string\n",
    "df.select_dtypes(include=['category'])\n",
    "df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* .count() - gives number of total values in column\n",
    "* .unique() - returns array of all unique values in that column\n",
    "* .value_counts() - returns object containing counts of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an int value is being treated as a string value by pandas,\n",
    "# use this to convert the variable to numeric before calling the value_counts function\n",
    "# Do this for each column you run value_counts on\n",
    "df[col_name] = pandas.to_numeric(df[col_name], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['category'])\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Way Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"wrangling-notebook-1.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to combine both the above cells and get freq table and proportions in one table\n",
    "freq_table = pd.crosstab(index=df[\"payer_name\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "freq_table_per = pd.crosstab(index=df[\"payer_name\"],  # Make a crosstab\n",
    "                              columns=\"percentage\", normalize=True)\n",
    "freq_table['percentage'] = freq_table_per['percentage']\n",
    "freq_table.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code to do the above in an uncompressed way. Can ignore this and move to 2-way frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Way Frequency Table\n",
    "freq_table = pd.crosstab(index=df[\"Symbol\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of each value\n",
    "\n",
    "# Method 1\n",
    "freq_table = pd.crosstab(index=df[\"payer_name\"],  # Make a crosstab\n",
    "                              columns=\"count\", normalize=True)\n",
    "# value_counts() excludes NA values, so numbers might not add up to 1 if there are missing values. This is okay because we care about the % of each value to take the missing values in account.\n",
    "\n",
    "# Method 2\n",
    "prop = freq_table / freq_table.sum()\n",
    "prop = prop.sort_values('count', ascending=False)\n",
    "\n",
    "# Show top 10 values (by proportion) in column\n",
    "prop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times a value occurs\n",
    "# Method 1\n",
    "df.col_name.value_counts(dropna=False)\n",
    "# Method 2\n",
    "df.groupby(col_name).size()\n",
    "\n",
    "# The percentage of how much the value occurs\n",
    "# Method 1\n",
    "df.col_name.value_counts(dropna=False, normalize=True)\n",
    "# Method 2\n",
    "df.groupby(col_name).size() * 100 / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Way Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wrangling-notebook-3.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-4.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher Dimensional Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wrangling-notebook-6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Higher Dimensional Frequency Table\n",
    "surv_s_class = pd.crosstab(index=titanic_train[\"Survived\"], \n",
    "                             columns=[titanic_train[\"Pclass\"],\n",
    "                                      titanic_train[\"Sex\"]],\n",
    "                             margins=True)\n",
    "surv_s_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of each value\n",
    "prop = surv_s_class/surv_sex_class.loc[\"All\"]    # Divide by column totals\n",
    "\n",
    "prop = prop.sort_values('count', ascending=False)\n",
    "\n",
    "# Show top 10 values (by proportion) in column\n",
    "prop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add stacked bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables\n",
    "* There are a lot more options for continuous variables\n",
    "* Use the five number summary - with **`.describe`**\n",
    "* Boxplots are great ways to find outliers\n",
    "* Use histograms and kernel density estimators to visualize the distribution.\n",
    "* Know the shape of the distribution\n",
    "* Think about making categorical variables out of continuous variables by cutting them into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all features on type numerical\n",
    "df.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the min, max values make sense? Do the mean, std and other values seem what you would expect or does anything stand out about them?\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create a dist plot (also a box and whiskers plot)\n",
    "hist_kws={\"alpha\": 0.3}\n",
    "sns.distplot(df[col_name], hist_kws=hist_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"x\", y=\"x\", data=df, estimator=lambda x: len(x) / len(df) * 100)\n",
    "ax.set(ylabel=\"Percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.col_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various methods of indexing and selecting data (.loc and bracket notation with/without boolean indexing, also .iloc)\n",
    "df.loc[df['city'] == \"New York\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Use bootstrapping to get more 'samples'\n",
    "* Bootstrapping is done by resampling your data with replacement and gives you a 'new' random dataset\n",
    "* This helps you get multiple looks at the data\n",
    "* You can get estimates for the mean and variance of continuous columns this way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### A note on missing values and different sources of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adding in the new POI’s in this example, none of whom we have financial information for, has introduced a subtle problem, that our lack of financial information about them can be picked up by an algorithm as a clue that they’re POIs. Another way to think about this is that there’s now a difference in how we generated the data for our two classes--non-POIs all come from the financial spreadsheet, while many POIs get added in by hand afterwards. That difference can trick us into thinking we have better performance than we do--suppose you use your POI detector to decide whether a new, unseen person is a POI, and that person isn’t on the spreadsheet. Then all their financial data would contain “NaN” but the person is very likely not a POI (there are many more non-POIs than POIs in the world, and even at Enron)--you’d be likely to accidentally identify them as a POI, though!\n",
    "\n",
    "This goes to say that, when generating or augmenting a dataset, you should be exceptionally careful if your data are coming from different sources for different classes. It can easily lead to the type of bias or mistake that we showed here. There are ways to deal with this, for example, you wouldn’t have to worry about this problem if you used only email data--in that case, discrepancies in the financial data wouldn’t matter because financial features aren’t being used. There are also more sophisticated ways of estimating how much of an effect these biases can have on your final answer; those are beyond the scope of this course.\n",
    "\n",
    "For now, the takeaway message is to be very careful about introducing features that come from different sources depending on the class! It’s a classic way to accidentally introduce biases and mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total missing values do we have in the dataframe?\n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing in the dataframe\n",
    "(total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['col_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values exist. Missing values may exist as dashes, slases, zeroes, NA, or none. Account for all of these. Also 0 and null are different because they lead to diff values of std, variance etc.)\n",
    "sum(df.col_name.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1st argument contains the x-values (column names), the 2nd argument the y-values (missing counts).\n",
    "base_color = sb.color_palette()[0]\n",
    "sb.barplot(missing_values_count.index.values, missing_values_count, color = base_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "* Use your natural human ability to look at boxplots to find thresholds for what an outlier might be\n",
    "* Generate a new column of data that is 0/1 for outlier or not. This will quickly help you find them later.\n",
    "\n",
    "\n",
    "Outliers can cause problems with certain types of models. For example, linear regression models are less robust to outliers than decision tree models.\n",
    "\n",
    "In general, if you have a legitimate reason to remove an outlier, it will help your model’s performance.\n",
    "\n",
    "However, **outliers are innocent until proven guilty**. You should never remove an outlier just because it’s a \"big number.\" That big number could be very informative for your model.\n",
    "\n",
    "You must have a good reason for removing an outlier, such as suspicious measurements that are unlikely to be real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IQR Method:**\n",
    "A commonly used rule says that a data point is an outlier if it is more than 1.5 . IQR above the third quartile or below the first quartile.\n",
    "Said differently, low outliers are < Q1 - (1.5 . IQR) and high outliers are > Q3 + (1.5 . IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Z-score Method **\n",
    "\n",
    "The intuition behind the Z-score method of outlier detection is that, once we’ve centred and rescaled the data, anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier.\n",
    "\n",
    "The Z-score method relies on the mean and standard deviation of a group of data to measure central tendency and dispersion. This is troublesome, because the mean and standard deviation are highly affected by outliers – they are not robust. In fact, the skewing that outliers bring is one of the biggest reasons for finding and removing outliers from a dataset!\n",
    "\n",
    "So we use the modified Z-score method is that it uses the median and MAD rather than the mean and standard deviation. The median and MAD are robust measures of central tendency and dispersion, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.sort_values()\n",
    "# See if the min or max values are too far away from the avg value (this could be inaccurate or this could be because of a difference in units: one value in KGs while the others are in LBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all numerical columns\n",
    "df_num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_num\n",
    "\n",
    "# Calculate modified z_score\n",
    "for col in df_num:\n",
    "    df_outliers = df[(np.abs(df[col]-df[col].median()) > (3.5*df[col].mad()))]\n",
    "    \n",
    "df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another robust measure of dispersion.\n",
    "\n",
    "PS: One caveat: both of these methods will encounter problems with a strongly skewed dataset. If the data is distributed in a strongly asymmetrical fashion, it will need to be re-expressed before applying any of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))\n",
    "outliers = outliers_iqr(df.dock_count)\n",
    "df.loc[outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ the 2 methods above (modified z_score and outliers_iqr) seem to work the best, but try all methods to see what works for for different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Others Ways:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_modified_z_score(ys):\n",
    "    threshold = 3.5\n",
    "\n",
    "    median_y = np.median(ys)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in ys])\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y\n",
    "                         for y in ys]\n",
    "    return np.where(np.abs(modified_z_scores) > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if the min or max values are too far away from the avg value (this could be inaccurate or this could be because of a difference in units: one value in KGs while the others are in LBs)\n",
    "df.col_name.sort_values()\n",
    "df.col_name.max()\n",
    "df.col_name.min()\n",
    "\n",
    "# Fetch outliers\n",
    "very_large_value = 100000000\n",
    "df[df.col_name > very_large_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to change the limits or scale of the axis to take a closer look at the outliers in the data\n",
    "plt.xlim(0, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper/Lower bounded values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes values are cut off at an arbitrary high or low value and therefore, the first and last value have a lot of datapoints. Check for this. This is very important to remember when assessing the generalizability of models trained on this later. Might want to mention this as part of dataset insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Duplicated data\n",
    "* Lots of data gets accidentally duplicated. Check for duplicates or near duplicates of rows and columns\n",
    "* If any columns are calculated entirely by that of another column or columns (like with depth from the diamonds data), ensure the calculation holds. \n",
    "\n",
    "Duplicate observations most frequently arise during data collection, such as when you:\n",
    "* Combine datasets from multiple places\n",
    "* Scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if there's any duplicate values(cause of nicknames/misspellings/multiple ways of expressing a value), or default 'John Doe' type values\n",
    "df[df.col_name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated columns\n",
    "# See if there are a lot of duplicated columns across tables which means that the tables could be combined. Ideally only the id column(s) should be common across tables\n",
    "all_columns = pd.Series(list(first) + list(second) + list(third))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add how to identify Imabalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrelevant observations are those that don’t actually fit the specific problem that you’re trying to solve.\n",
    "\n",
    "For example, if you were building a model for Single-Family homes only, you wouldn't want observations for Apartments in there.\n",
    "This is also a great time to review your charts from Exploratory Analysis. You can look at the distribution charts for categorical features to see if there are any classes that shouldn’t be there.\n",
    "Checking for irrelevant observations before engineering features can save you many headaches down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Summary\n",
    "\n",
    "| Univariate             | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical | Bar char of frequencies (count / percent) | Frequency/Contingency table (count / percent) |\n",
    "| Num/Continuous  | Histogram/rugplot/KDE, box/violin/swarm, qqplot, fat tails  | central tendency -mean/median/mode, spread - variance, std, skew, kurt, IQR  |\n",
    "\n",
    "| Bivariate/multivariate            | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical vs Categorical | heat map, mosaic plot | Two-way Contingency table (count/percent) |\n",
    "| Continuous vs Continuous  | all pairwise scatterplots, kde, heatmaps |  all pairwise correlation/regression   |\n",
    "| Categorical vs Continuous  | [bar, violin, swarm, point, strip seaborn plots](http://seaborn.pydata.org/tutorial/categorical.html)  | Summary statistics for each level |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures, Axes, Subplots, Legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure size (width, height)\n",
    "plt.figure(figsize = [10, 5])\n",
    "\n",
    "# Create subplots\n",
    "# creates a new Axes in our Figure, its size determined by the subplot() function arguments. The first two arguments says to divide the figure into one row and two columns, and the third argument says to create a new Axes in the first slot. Slots are numbered from left to right in rows from top to bottom. Note in particular that the index numbers start at 1 (rather than the usual Python indexing starting from 0).\n",
    "# subplot 1: example of somewhat too-large bin size\n",
    "plt.subplot(1, 2, 1) # 1 row, 2 cols, subplot 1\n",
    "bin_edges = np.arange(0, df['num_var'].max()+4, 4)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)\n",
    "\n",
    "# subplot 2: example of somewhat too-small bin size\n",
    "plt.subplot(1, 2, 2) # 1 row, 2 cols, subplot 2\n",
    "bin_edges = np.arange(0, df['num_var'].max()+1/4, 1/4)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)\n",
    "\n",
    "# Creating a bunch of subplots\n",
    "fig, axes = plt.subplots(3, 4) # grid of 3x4 subplots\n",
    "axes = axes.flatten() # reshape from 3x4 array into 12-element vector\n",
    "for i in range(12):\n",
    "    plt.sca(axes[i]) # set the current Axes\n",
    "    # do something with the current axis.. in this case print the index number in the middle of the axis\n",
    "    plt.text(0.5, 0.5, i+1)\n",
    "\n",
    "# Move legend outside the chart\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='wrangling-notebook-22.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variable Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the order of categorical variable values in charts:\n",
    "# Specify order in cat_classes and run this code\n",
    "cat_classes = ['Minicompact Cars', 'Subcompact Cars', 'Compact Cars', 'Midsize Cars', 'Large Cars']\n",
    "pd_ver = pd.__version__.split(\".\")\n",
    "if (int(pd_ver[0]) > 0) or (int(pd_ver[1]) >= 21): # v0.21 or later\n",
    "    vclasses = pd.api.types.CategoricalDtype(ordered = True, categories = cat_classes)\n",
    "    fuel_econ['VClass'] = fuel_econ['VClass'].astype(vclasses)\n",
    "else: # pre-v0.21\n",
    "    fuel_econ['VClass'] = fuel_econ['VClass'].astype('category', ordered = True,\n",
    "                                                     categories = cat_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caterogical variables can be visualized using bar plots.\n",
    "\n",
    "In particular, you'll want to look out for sparse classes, which are classes that have a very small number of observations.\n",
    "\n",
    "A \"class\" is simply a unique value for a categorical feature.CAterogical variables can be visualizing\n",
    "\n",
    "Classes with short bars are sparse classes. They tend to be problematic when building models.\n",
    "\n",
    "In the best case, they don't influence the model much.\n",
    "In the worse case, they can cause the model to be overfit.\n",
    "Therefore, make a note to combine or reassign some of these classes later.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/grouping-sparse-classes-before.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_name].value_counts(normalize=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='col_name', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(14,6))\n",
    "df['col_name'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=False)\n",
    "ax[0].set_title('col_name')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('col_name',data=df,ax=ax[1])\n",
    "ax[1].set_title('col_name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Charts (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For nominal data:\n",
    "# arrange bar chart in order of frequency. Don't rearrange for ordinal data,\n",
    "# the inherent order or the levelswill be more important to convey\n",
    "# Start your chart with 0 as the baseline to avoid information distortion\n",
    "# If you have a lot of categories, or the category names are long, a horizontal orientation might be more convinient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by plotting everything in one color, and sort by frequency\n",
    "base_color = sb.color_palette()[0]\n",
    "cat_order = df['cat_var'].value_counts().index\n",
    "sns.countplot(data = df, x = 'cat_var', color = base_color, order = cat_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ordinal data:\n",
    "# By default, pandas reads in string data as object types, and will plot the bars in the order\n",
    "# in which the unique values were seen. By converting the data into an ordered type, the order\n",
    "# of categories becomes innate to the feature, and we won't need to specify an \"order\" parameter\n",
    "# each time it's required in a plot.\n",
    "\n",
    "# convert variable to ordered type\n",
    "level_order = ['Alpha', 'Beta', 'Gamma', 'Delta']\n",
    "ordered_cat = pd.api.types.CategoricalDtype(ordered = True, categories = level_order)\n",
    "df['cat_var'] = df['cat_var'].astype(ordered_cat)\n",
    "\n",
    "# plot the variable\n",
    "base_color = sb.color_palette()[0]\n",
    "sb.countplot(data = df, x = 'cat_var', color = base_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate category variables\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal bar chart - change x parameter to y\n",
    "sns.countplot(data = df, y = 'cat_var', color = base_color, order = cat_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative frequency - Plotting proportions on the axis instead of absolute counts\n",
    "\n",
    "# get proportion taken by most common group for derivation\n",
    "# of tick marks\n",
    "n_points = df.shape[0]\n",
    "max_count = df['cat_var'].value_counts().max()\n",
    "max_prop = max_count / n_points\n",
    "\n",
    "# generate tick mark locations and names\n",
    "tick_props = np.arange(0, max_prop, 0.05)\n",
    "tick_names = ['{:0.2f}'.format(v) for v in tick_props]\n",
    "\n",
    "# create the plot\n",
    "base_color = sb.color_palette()[0]\n",
    "sb.countplot(data = df, x = 'cat_var', color = base_color)\n",
    "# plt.yticks(tick_position, tick_labels)\n",
    "plt.yticks(tick_props * n_points, tick_names)\n",
    "plt.ylabel('proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='wrangling-notebook-20.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative frequency variation - Plotting absolute counts on axis and porportions on the bars\n",
    "\n",
    "# create the plot\n",
    "base_color = sb.color_palette()[0]\n",
    "sb.countplot(data = df, x = 'cat_var', color = base_color)\n",
    "\n",
    "# add annotations\n",
    "n_points = df.shape[0]\n",
    "cat_counts = df['cat_var'].value_counts()\n",
    "locs, labels = plt.xticks() # get the current tick locations and labels\n",
    "\n",
    "# loop through each pair of locations and labels\n",
    "for loc, label in zip(locs, labels):\n",
    "\n",
    "    # get the text property for the label to get the correct count\n",
    "    count = cat_counts[label.get_text()]\n",
    "    pct_string = '{:0.1f}%'.format(100*count/n_points)\n",
    "\n",
    "    # print the annotation just below the top of the bar\n",
    "    plt.text(loc, count-8, pct_string, ha = 'center', color = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='wrangling-notebook-21.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot() - if data is summarized and you still want to build a bar chart.\n",
    "# countplot() - if data is not yet summarized, so that you don't need to do extra summarization work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart / Donut Plot (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart is better than pie/donut charts aas its hard to get an accurate estimate of the relative\n",
    "# frequency using just the pie/donut chart alone. If there are a lot of categories, you have to choose\n",
    "# a few to plot. If you have a lot of categories, or categories that have small proportions,\n",
    "# consider grouping them together so that fewer wedges are plotted, or use an 'Other' category.\n",
    "\n",
    "# If these guidelines cannot be met, use a bar chart instead. A bar chart is a safer choice in general.\n",
    "# The bar heights are more precisely interpreted than areas or angles, and a bar chart can be displayed\n",
    "# more compactly than a pie chart. There's also more flexibility with a bar chart for plotting variables\n",
    "# with a lot of levels, like plotting the bars horizontally.\n",
    "\n",
    "# User these charts when you want to show how the data is broken down into parts\n",
    "# and only plot 2-3 categories so the relative portions are distinct\n",
    "# Start at the top and plot clockwise according to frequency\n",
    "\n",
    "# Pie Chart\n",
    "# The axis function call and 'square' argument makes it so that the scaling of the plot is equal on both the x- and y-axes. Without this call, the pie could end up looking oval-shaped, rather than a circle.\n",
    "sorted_counts = df['cat_var'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False);\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donut Chart\n",
    "sorted_counts = df['cat_var'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False, wedgeprops = {'width' : 0.4});\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waffle Plot (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no built-in function for waffle plots in Matplotlib or Seaborn, so we'll need to take some additional steps in order to build one with the tools available.\n",
    "# The blocks are drawn from left to right, bottom to top\n",
    "# First, we need to create a function to decide how many blocks to allocate to each category. The function below, percentage_blocks, uses a rule where each category gets a number of blocks equal to the number of full percentage points it covers. The remaining blocks to get to the full one hundred are assigned to the categories with the largest fractional parts.\n",
    "def percentage_blocks(df, var):\n",
    "    \"\"\"\n",
    "    Take as input a dataframe and variable, and return a Pandas series with\n",
    "    approximate percentage values for filling out a waffle plot.\n",
    "    \"\"\"\n",
    "    # compute base quotas\n",
    "    percentages = 100 * df[var].value_counts() / df.shape[0]\n",
    "    counts = np.floor(percentages).astype(int) # integer part = minimum quota\n",
    "    decimal = (percentages - counts).sort_values(ascending = False)\n",
    "    # add in additional counts to reach 100\n",
    "    rem = 100 - counts.sum()\n",
    "    for cat in decimal.index[:rem]:\n",
    "        counts[cat] += 1\n",
    "    return counts\n",
    "\n",
    "# Second, plot those counts as boxes in the waffle plot form\n",
    "waffle_counts = percentage_blocks(df, 'cat_var')\n",
    "prev_count = 0\n",
    "# for each category,\n",
    "for cat in range(waffle_counts.shape[0]):\n",
    "    # get the block indices\n",
    "    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n",
    "    # and put a block at each index's location\n",
    "    x = blocks % 10 # use mod operation to get ones digit\n",
    "    y = blocks // 10 # use floor division to get tens digit\n",
    "    plt.bar(x = x, height = 0.8, width = 0.8, bottom = y)\n",
    "    prev_count += waffle_counts[cat]\n",
    "\n",
    "# Third, we need to do involve aesthetic cleaning to polish it up for interpretability. We can take away the plot border and ticks, since they're arbitrary, but we should change the limits so that the boxes are square. We should also add a legend so that the mapping from colors to category levels is clear.\n",
    "# aesthetic wrangling\n",
    "plt.legend(waffle_counts.index, bbox_to_anchor = (1, 0.5), loc = 6)\n",
    "plt.axis('off')\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='wrangling-notebook-25.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other variants of the waffle plot exist to extend it beyond just displaying probabilities.\n",
    "# By associating each square with an amount rather than a percentage, we can use waffle plots to show absolute frequencies instead. This might cause us to end up with something other than 100 squares.\n",
    "# each box represents five full counts\n",
    "# boxes are plotted in columns from left to right\n",
    "num_counts = 5\n",
    "waffle_counts = (df['cat_var'].value_counts() / num_counts).astype(int)\n",
    "\n",
    "prev_count = 0\n",
    "# for each category,\n",
    "for cat in range(waffle_counts.shape[0]):\n",
    "    # get the block indices\n",
    "    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n",
    "    # and put a block at each index's location\n",
    "    x = blocks % 10\n",
    "    y = blocks // 10\n",
    "    plt.bar(y, 0.8, 0.8, x)\n",
    "    prev_count += waffle_counts[cat]\n",
    "\n",
    "# box size legend\n",
    "plt.bar(7.5, 0.8, 0.8, 2, color = 'white', edgecolor = 'black', lw = 2)\n",
    "plt.text(8.1, 2.4,'= 5 data points', va = 'center')\n",
    "\n",
    "# aesthetic wrangling\n",
    "plt.legend(waffle_counts.index, bbox_to_anchor = (0.8, 0.5), loc = 6)\n",
    "plt.axis('off')\n",
    "plt.axis('square')\n",
    "\n",
    "# Notes:\n",
    "# PyWaffle makes it easier to create Waffle Plots: https://github.com/ligyxy/PyWaffle\n",
    "# Using icons for each tally, rather than just squares - Infographics often take this approach, by having each icon represent some number of units (e.g. one person icon representing one million people). But while it can be tempting to use icons to represent values as a bit of visual flair, an icon-based plot contains more chart junk than a bar chart that conveys the same information. There’s a larger cognitive challenge in having to count a number of icons to understand the scale of a value, compared to just referencing a box's endpoint on a labeled axis.\n",
    "# The effort required to create a meaningful and useful waffle plot means that it is best employed carefully as a part of explanatory visualizations. During the exploratory phase, you're better off using more traditional plots like the bar chart to more rapidly build your understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='wrangling-notebook-26.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Variable Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, a quick and dirty grid of histograms is enough to understand the distributions\n",
    "\n",
    "Here are a few things to look out for:\n",
    "\n",
    "- Distributions that are unexpected\n",
    "- Potential outliers that don't make sense\n",
    "- Features that should be binary (i.e. \"wannabe indicator variables\")\n",
    "- Boundaries that don't make sense\n",
    "- Potential measurement errors\n",
    "\n",
    "PS: When there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly. In such cases, it's good to look at the plot on a **log scale**. The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/histogram-grid-example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution charts for all independent, numerical variables together\n",
    "# assuming independent variables start at index 2\n",
    "for column in df.columns[2:]:\n",
    "    print(column)\n",
    "    #s=df['column']\n",
    "    s=df[column]\n",
    "    mu, sigma =norm.fit(s)\n",
    "    count, bins, ignored = plt.hist(s, 30, normed=True, color='g')\n",
    "    plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=1, color='r')\n",
    "\n",
    "    title = \"Plot used: mu = %.2f,  std = %.2f\" % (mu, sigma)\n",
    "    plt.title(title, loc='right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Plot\n",
    "sns.kdeplot(df.col_name)\n",
    "plt.show()\n",
    "\n",
    "# KDE + Hist\n",
    "hist_kws={\"alpha\": 0.3}\n",
    "sns.distplot(df['col_name'], hist_kws=hist_kws)\n",
    "\n",
    "# Optional: Trim long-tail/other values and create KDE again\n",
    "df_trimmed = df.loc[df.index[df.price < ceiling_value]].sample(n=50000)\n",
    "sns.kdeplot(df_trimmed.col_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these variables may not be close to a normal distribution, but that might not be very critical if the techniques we apply are not very sensitive to normality.\n",
    "There's also the option of converting some of these variables to categories for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify bin values explicitly\n",
    "bin_edges = np.arange(0, df['num_var'].max()+1, 1)\n",
    "# larger bin size\n",
    "bin_edges = np.arange(0, df['num_var'].max()+5, 5)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges)\n",
    "\n",
    "# for discrete data,\n",
    "# rwidth =0.7 adds space between bars to emphasize discrete values, cause the bars will take up 70% of the space allocated by each bin, with 30% of the space left empty\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges, rwidth = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Axis Limits\n",
    "# Note any aspects of the data like number of modes and skew, and note the presence of outliers in the data for further investigation.\n",
    "# you might need to change the limits or scale of the axis to take a closer look at the underlying patterns in the data\n",
    "# use of axis limits can allow focusing on data points in that range without needing to go through creation of a new DataFrame filtering out the data points in the latter group (> 40)\n",
    "plt.xlim(0, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the graph\n",
    "# Logarithmic scaling captures the multiplicative differences rather than the arithmetic differences\n",
    "# Each tick mark is twice the value of the previous bar\n",
    "# Lognormal distribution: The data has a normal distribution after a log transform - i.e. data that, in its natural units, can look highly skewed: e.g. lots of points with low values, with a very long tail of data points with large values.\n",
    "# (this suggests that building a logarithmic model for the variable might work better than a linear model - linear changes in predictor features will result in multiplicative effects in y)\n",
    "# all values must be +ve in order to use a log transform\n",
    "# For visualizations it is best to do axis transformations rather than data transformations\n",
    "# We can transform the data during the analysis phase\n",
    "# Here we aren't changing the values taken by the data, only how they're displayed\n",
    "# Between integer powers of 10, we don't have clean values for even markings, but we can still get close. Setting ticks in cycles of 1-3-10 or 1-2-5-10 are very useful for base-10 log transforms.\n",
    "plt.figure(figsize = [10, 5])\n",
    "\n",
    "# left histogram: data plotted in natural units\n",
    "plt.subplot(1, 2, 1)\n",
    "bin_edges = np.arange(0, data.max()+100, 100)\n",
    "plt.hist(data, bins = bin_edges)\n",
    "plt.xlabel('values')\n",
    "\n",
    "# right histogram: data plotted after direct log transformation\n",
    "bin_edges = 10 ** np.arange(0.8, np.log10(data.max())+0.1, 0.1)\n",
    "plt.hist(data, bins = bin_edges)\n",
    "plt.xscale('log')\n",
    "tick_locs = [10, 30, 100, 300, 1000, 3000]\n",
    "plt.xticks(tick_locs, tick_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='wrangling-notebook-23.png' width=50%>\n",
    "<img src='wrangling-notebook-24.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distplot (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a quick start on choosing a representative bin size for histogram plotting, you might take a quick look at the basic distplot first before getting into the customization\n",
    "# distplot function has built-in rules for specifying histogram bins, and by default plots a curve depicting the kernel density estimate (KDE) on top of the data. The vertical axis is based on the KDE, rather than the histogram: you shouldn't expect the total heights of the bars to equal 1, but the area under the curve should equal 1\n",
    "bin_edges = np.arange(0, df['num_var'].max()+1, 1)\n",
    "sb.distplot(df['num_var'], hist_kws = {'alpha' : 0.8})\n",
    "# Despite the fact that the default bin-selection formula used by distplot might be better than the choice of ten bins that .hist uses, you'll still want to do some tweaking to align the bins to 'round' values.\n",
    "sb.distplot(df['num_var'], bins = bin_edges, hist_kws = {'alpha' : 0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box and whiskers (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sns.color_palette()[0]\n",
    "sns.boxplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "# switch x and y parameter values to make this a horizontal chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin Plot (Visualize the distribution of one numerical variable or lots of categorical variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area depicts the distribution of points. > width = > the number of points\n",
    "base_color = sns.color_palette()[0]\n",
    "sns.violinplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "# switch x and y parameter values to make this a horizontal chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentations - Categorical + Numerical Variables Together\n",
    "\n",
    "Segmentations are powerful ways to observe the relationship between categorical features and numeric features.\n",
    "\n",
    "Box plots allow you to do so.\n",
    "\n",
    "Here are a few insights you could draw from the following chart.\n",
    "\n",
    "The median transaction price (middle vertical bar in the box) for Single-Family homes was much higher than that for Apartments / Condos / Townhomes.\n",
    "The min and max transaction prices are comparable between the two classes.\n",
    "\n",
    "<img src=\"https://34tzkp3af7ck1k675g1stf6n-wpengine.netdna-ssl.com/wp-content/uploads/2017/06/boxplot-segmentation-example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of distributions\n",
    "\n",
    "Described their their:\n",
    "+ Shape\n",
    "+ Center\n",
    "+ Spread\n",
    "\n",
    "**Shape:**\n",
    "<img src=\"wrangling-notebook-7.png\">\n",
    "          \n",
    "All of these are symmetric, but differ in their peakness or modality (unimodal, bimodal, uniform).\n",
    "\n",
    "Modes - values that occur most often.\n",
    "The mode doesn't always occur at the center.\n",
    "The center is the midpoint, the value that divides the distribution so that half of the values take smaller values and the other half take larger values.\n",
    "Spread - approx range covered by the distribution. Measured by range.\n",
    "\n",
    "Distributions can be both left/right kewed and have more than one mode.\n",
    "<img src=\"wrangling-notebook-8.png\">\n",
    "\n",
    "The histogram can give us the shape, but the center and spread are harder to determine from the histrogram.\n",
    "\n",
    "**Center:**\n",
    "* Mean: Average = sum of values/n\n",
    "* Median: Write the values in ascending order. Median is the middle value or the avg of the middle 2 values.\n",
    "* Mode: Value with the highest frequency\n",
    "\n",
    "<img src=\"wrangling-notebook-9.png\">\n",
    "These distributions have the same center. So spread alone isn't enough to describe a distribution. We also need to talk about the spreads which are different for each distribution above.\n",
    "\n",
    "**Spread:**\n",
    "* **Standard Deviation** - Typical/avg difference between the observations and the mean. Sqrt(variance). We take the sqrt inorder to bring the units back to units (we squared them while calculating variance)\n",
    "* **Variance** - |(x - x^)^2|/(n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate and Multivariate EDA\n",
    "\n",
    "Under bivariate analysis we will examine the realtionship between Dependent variable and each of the independent variables. We will also check selected pairs of independent variables.\n",
    "\n",
    "Boxplots are great when you have a numeric column that you want to compare across different categories.\n",
    "When you want to visualize two numeric columns, scatter plots are ideal.\n",
    "\n",
    "### Categorical vs Categorical\n",
    "* Create two way contingency table of frequency counts\n",
    "* Create a heat map\n",
    "* Find expected counts and possibly do a chi-squared test\n",
    "\n",
    "### Categorical vs Continuous\n",
    "* Use the seaborn categorical plots\n",
    "\n",
    "### Continuous vs Continuous\n",
    "* Plot all combinations of scatterplots\n",
    "* Use a hierarchical clustering plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "In general, you should look out for:\n",
    "\n",
    "Which features are strongly correlated with the target variable?\n",
    "Are there interesting or unexpected strong correlations between other features?\n",
    "\n",
    "Note:\n",
    "Correlation is a value between -1 and 1 that represents how closely two features move in unison. You don't need to remember the math to calculate them. Just know the following intuition:\n",
    "\n",
    "Positive correlation means that as one feature increases, the other increases. E.g. a child’s age and her height.\n",
    "Negative correlation means that as one feature increases, the other decreases. E.g. hours spent studying and number of parties attended.\n",
    "Correlations near -1 or 1 indicate a strong relationship.\n",
    "Those closer to 0 indicate a weak relationship.\n",
    "0 indicates no relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bivariate analysis, there's a dependent and an independent variable.\n",
    "\n",
    "** X is a predictor of Y. **\n",
    "\n",
    "<img src=\"wrangling-notebook-10.png\">\n",
    "\n",
    "Determine which variable plays which role in our analysis.\n",
    "\n",
    "Use this classification system to determine which plots and statistical tools to use:\n",
    "\n",
    "<img src=\"wrangling-notebook-12.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-11.png\">\n",
    "\n",
    "<img src=\"** Visualizing_Flowchart.png\">\n",
    "\n",
    "** Types of relationships shown in scatter plots **\n",
    "<img src=\"wrangling-notebook-13.png\">\n",
    "\n",
    "<img src=\"wrangling-notebook-14.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "df.corr()\n",
    "\n",
    "# Kendall Tau correlation\n",
    "df.corr('kendall')\n",
    "\n",
    "# Spearman Rank correlation\n",
    "df.corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendall and Spearman correlation measures require you to rank the data before calculating the coefficients. You can easily do this with rank().\n",
    "\n",
    "Furthermore, there are some assumptions that these correlations work with: the Pearson correlation assumes that your variables are normally distributed, that there is a straight line relationship between each of the variables and that the data is normally distributed about the regression line. The Spearman correlation, on the other hand, assumes that you have two ordinal variables or two variables that are related in some way, but not linearly.\n",
    "\n",
    "The Kendall Tau correlation is a coefficient that represents the degree of concordance between two columns of ranked data. You can use the Spearman correlation to measure the degree of association between two variables. These seem very similar to each other, don’t they?\n",
    "\n",
    "Even though the Kendal and the Spearman correlation measures seem similar, but they do differ: the exact difference lies in the fact that the calculations are different. The Kendal Tau coefficient is calculated by the number of concordant pairs minus the number of discordant pairs divided by the total number of pairs. The Spearman coefficient is the sum of deviation squared by n times n minus 1.\n",
    "\n",
    "Spearman’s coefficient will usually be larger than the Kendall’s Tau coefficient, but this is not always the case: you’ll get a smaller Spearman’s coefficient when the deviations are huge among the observations of your data. The Spearman correlation is very sensitive to this and this might come in handy in some cases!\n",
    "\n",
    "So, when do you want to use which coefficient, because the two of these correlation actually test something different; Kendall’s Tau is representing the proportion of concordant pairs relative to discordant pairs and the Spearman’s coefficient doesn’t do that. You can also argue that the Kendall Tau correlation has a more intuitive interpretation and easier to calculate, that it gives a better estimate of the corresponding population parameter and that the p values are more accurate in small sample sizes.\n",
    "\n",
    "Tip add the print() function to see the results of the specific pairwise correlation compututations of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Summary\n",
    "\n",
    "| Univariate             | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical | Bar char of frequencies (count / percent) | Frequency/Contingency table (count / percent) |\n",
    "| Num/Continuous  | Histogram/rugplot/KDE, box/violin/swarm, qqplot, fat tails  | central tendency -mean/median/mode, spread - variance, std, skew, kurt, IQR  |\n",
    "\n",
    "| Bivariate/multivariate            | Graphical                               | Non-Graphical                     | \n",
    "|-------------|-----------------------------------------|-----------------------------------|\n",
    "| Categorical vs Categorical | heat map, mosaic plot | Two-way Contingency table (count/percent) |\n",
    "| Continuous vs Continuous  | all pairwise scatterplots, kde, heatmaps |  all pairwise correlation/regression   |\n",
    "| Categorical vs Continuous  | [bar, violin, swarm, point, strip seaborn plots](http://seaborn.pydata.org/tutorial/categorical.html)  | Summary statistics for each level |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2')\n",
    "# adding a diagonal line through the chart values can help determine trends easily\n",
    "# You can see if points are above or below the x=y relationship line, therefore if x is higher than y or vice versa\n",
    "plt.plot([min_value,max_value], [min_value,max_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regplot combines scatterplot with regression function fitting\n",
    "sb.regplot(data = df, x = 'num_var1', y = 'num_var2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have a very large number of points to plot or our numeric variables are discrete-valued, then it is possible that using a scatterplot straightforwardly will not be informative. The visualization will suffer from overplotting, where the high amount of overlap in points makes it difficult to see the actual relationship between the plotted variables.\n",
    "# In cases like this, we may want to employ transparency and jitter to make the scatterplot more informative.\n",
    "# Add Transparency to deal with Overplotting\n",
    "# Where more points overlap, the darker the image will be.\n",
    "plt.scatter(data = df, x = 'disc_var1', y = 'disc_var2', alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Jitter to deal with Overplotting\n",
    "# adding jitter moves the position of each point slightly from its true value\n",
    "# each point will be plotted in a uniform ±0.2 range of its true values\n",
    "# jitter won't affect the fit of any regression function, if made\n",
    "sb.regplot(data = df, x = 'disc_var1', y = 'disc_var2', fit_reg = False,\n",
    "           x_jitter = 0.2, y_jitter = 0.2, scatter_kws = {'alpha' : 1/3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual relationship between the color and density of data\n",
    "# Add counts cause color perception is imprecise\n",
    "# Prefer this over a scatterplot if we have 2 discrete variables since the jitter scatterplot can be imprecise. It's also better than adding transparency to a scatterplot in cases where there's a large amount of data.\n",
    "# However, bin size does affect our interpretation of the data, so make sure to pick the right bin size\n",
    "# darker values = higher value counts\n",
    "# specify bin edges for the x and y axis\n",
    "bins_x = np.arange(0.6, df['disc_var1'].max()+0.4, 0.4)\n",
    "bins_y = np.arange(0, df['disc_var2'].max()+50, 50)\n",
    "plt.hist2d(data = df, x = 'disc_var1', y = 'disc_var2',\n",
    "           bins = [bins_x, bins_y], cmap = 'viridis_r', cmin = 0.5)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Displacement (l)')\n",
    "plt.ylabel('CO2 (g/mi)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat Map with counts in each cell\n",
    "# If you have too many cells in your heat map, then the annotations will end up being too overwhelming,\n",
    "# too much to attend to. In cases like that, it's best to leave off the annotations and let the data\n",
    "# and colorbar speak for themselves. You're more likely to see annotations in a categorical heat map,\n",
    "# where there are going to be fewer cells plotted.\n",
    "# hist2d returns a number of different variables, including an array of counts\n",
    "bins_x = np.arange(0.6, df['disc_var1'].max()+0.4, 0.4)\n",
    "bins_y = np.arange(0, df['disc_var2'].max()+50, 50)\n",
    "h2d = plt.hist2d(data = df, x = 'disc_var1', y = 'disc_var2',\n",
    "               bins = [bins_x, bins_y], cmap = 'viridis_r', cmin = 0.5)\n",
    "counts = h2d[0]\n",
    "plt.xlabel('Displacement (l)')\n",
    "plt.ylabel('CO2 (g/mi)')\n",
    "\n",
    "# loop through the cell counts and add text annotations for each\n",
    "for i in range(counts.shape[0]):\n",
    "    for j in range(counts.shape[1]):\n",
    "        c = counts[i,j]\n",
    "        if c >= 7: # increase visibility on darkest cells\n",
    "            plt.text(bins_x[i]+0.5, bins_y[j]+0.5, int(c),\n",
    "                     ha = 'center', va = 'center', color = 'white')\n",
    "        elif c > 0:\n",
    "            plt.text(bins_x[i]+0.5, bins_y[j]+0.5, int(c),\n",
    "                     ha = 'center', va = 'center', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin Plot (Categorical vs Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area depicts the distribution of points. > width = > the number of points\n",
    "base_color = sns.color_palette()[0]\n",
    "sns.violinplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "# switch x and y parameter values to make this a horizontal chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot (Categorical vs Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sns.color_palette()[0]\n",
    "sns.boxplot(data = df, x = 'cat_var', y = 'num_var', color = base_color)\n",
    "# switch x and y parameter values to make this a horizontal chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustered Bar Charts (Categorical vs Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.countplot(data = df, x = 'cat_var1', hue = 'cat_var2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat Map (Categorical vs Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of providing the original dataframe, we need to summarize the counts into a matrix that will then be plotted.\n",
    "ct_counts = df.groupby(['cat_var1', 'cat_var2']).size()\n",
    "ct_counts = ct_counts.reset_index(name = 'count')\n",
    "ct_counts = ct_counts.pivot(index = 'cat_var2', columns = 'cat_var1', values = 'count')\n",
    "sb.heatmap(ct_counts, annot = True, fmt = 'd')\n",
    "# You can use fmt = '.0f' if you have any cells with no counts, in order to account for NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faceting (Categorical vs Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faceting histograms of the quantitative variable, against the qualitative subsets of data\n",
    "# Facet by col='' parameter\n",
    "bin_edges = np.arange(-3, df['num_var'].max()+1/3, 1/3)\n",
    "g = sns.FacetGrid(data = df, col = 'cat_var', size = 2, col_wrap=3)\n",
    "g.map(plt.hist, \"num_var\", bins = bin_edges)\n",
    "g.set_titles('{col_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the means of a feature by different categories of label_colname\n",
    "df.groupby('label_colname')['afeature'].agg(['count','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these means by different categories of label_colname\n",
    "df['afeature'].groupby(df.label_colname).mean().plot(kind='bar', color=['blue', 'green']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot + print pearson correlation\n",
    "sns.lmplot('price', 'deal_probability', hue='another_categorical_feature', data=df, fit_reg=True, size=10, aspect=2, scatter_kws={'s': 10, 'alpha':0.3})\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Deal Probability')\n",
    "plt.show()\n",
    "print(f'Pearson correlation : {scipy.stats.pearsonr(df.price.values, df.deal_probability.values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with Bokeh (good for large datasets)\n",
    "from bokeh.charts import Scatter, output_file, show\n",
    "\n",
    "# Construct the scatter plot\n",
    "p = Scatter(iris, x='Petal_length', y='Petal_width', color=\"Class\", title=\"Petal Length vs Petal Width\",\n",
    "            xlabel=\"Sepal Length\", ylabel=\"Sepal Width\")\n",
    "\n",
    "# Output the file \n",
    "output_file('scatter.html')\n",
    "\n",
    "# Show the scatter plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot -- to know if a categorical_var leans towards to a specific range of numerical_var\n",
    "sns.violinplot('categorical_var', 'numerical_var', data=df)\n",
    "\n",
    "# Optional: Trim long-tail/other values and create violin plot again\n",
    "df_trimmed = df.loc[df.index[df.price < ceiling_value]].sample(n=50000)\n",
    "sns.violinplot('categorical_var', 'numerical_var', data=df_trimmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of different values of categorical vars with numerical var in the same graph\n",
    "hist_kws={\"alpha\": 0.2}\n",
    "sns.distplot(df[df.categorical_var == 'Private']['numerical_var'], label='Private', hist_kws=hist_kws)\n",
    "sns.distplot(df[df.categorical_var == 'Company']['numerical_var'], label='Company', hist_kws=hist_kws)\n",
    "sns.distplot(df[df.categorical_var == 'Shop']['numerical_var'], label='Shop', hist_kws=hist_kws)\n",
    "sns.distplot(df.numerical_var, label='All', hist_kws=hist_kws)\n",
    "plt.title('numerical_var distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked plot\n",
    "# libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    " \n",
    "# Data\n",
    "r = [0,1,2,3,4]\n",
    "raw_data = {'greenBars': [20, 1.5, 7, 10, 5], 'orangeBars': [5, 15, 5, 10, 15],'blueBars': [2, 15, 18, 5, 10]}\n",
    "df = pd.DataFrame(raw_data)\n",
    " \n",
    "# From raw value to percentage\n",
    "totals = [i+j+k for i,j,k in zip(df['greenBars'], df['orangeBars'], df['blueBars'])]\n",
    "greenBars = [i / j * 100 for i,j in zip(df['greenBars'], totals)]\n",
    "orangeBars = [i / j * 100 for i,j in zip(df['orangeBars'], totals)]\n",
    "blueBars = [i / j * 100 for i,j in zip(df['blueBars'], totals)]\n",
    " \n",
    "# plot\n",
    "barWidth = 0.85\n",
    "names = ('A','B','C','D','E')\n",
    "# Create green Bars\n",
    "plt.bar(r, greenBars, color='#b5ffb9', edgecolor='white', width=barWidth, label=\"group A\")\n",
    "# Create orange Bars\n",
    "plt.bar(r, orangeBars, bottom=greenBars, color='#f9bc86', edgecolor='white', width=barWidth, label=\"group B\")\n",
    "# Create blue Bars\n",
    "plt.bar(r, blueBars, bottom=[i+j for i,j in zip(greenBars, orangeBars)], color='#a3acff', edgecolor='white', width=barWidth, label=\"group C\")\n",
    " \n",
    "# Custom x axis\n",
    "plt.xticks(r, names)\n",
    "plt.xlabel(\"group\")\n",
    " \n",
    "# Add a legend\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='stacked_percent_barplot.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **independent variable (X)** is the variable that is changed or controlled in a scientific experiment to test the effects on the dependent variable.\n",
    "A **dependent variable (Y)** is the variable being tested and measured in a scientific experiment.\n",
    "\n",
    "D is the dependent variable\n",
    "R is the responding variable\n",
    "Y is the axis on which the dependent or responding variable is graphed (the vertical axis)\n",
    "\n",
    "M is the manipulated variable or the one that is changed in an experiment\n",
    "I is the independent variable\n",
    "X is the axis on which the independent or manipulated variable is graphed (the horizontal axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Choose a data set that you would like to work with.\n",
    "\n",
    "**STEP 2.** Identify a specific topic of interest\n",
    "\n",
    "**STEP 3.** Prepare a codebook of your own (i.e., print individual pages or copy screen and paste into a new document) from the larger codebook that includes the questions/items/variables that measure your selected topics.)\n",
    "\n",
    "**STEP 4.** Identify a second topic that you would like to explore in terms of its association with your original topic.\n",
    "\n",
    "**STEP 5.** Add questions/items/variables documenting this second topic to your personal codebook.\n",
    "\n",
    "**STEP 6.** Identify a basic research question that interests you. - \"Is there an association between A and B\n",
    "in subset of the population C\"?\n",
    "\n",
    "**STEP 7.** Once you have a basic research question, you can think about other variables that might explain your association.\n",
    "\n",
    "**STEP 8.** Perform a literature review to see what research has been previously done on this topic. Use sites such as Google Scholar ([http://scholar.google.com](http://scholar.google.com/)) to search for published academic work in the area(s) of interest. Try to find multiple sources, and take note of basic bibliographic information.\n",
    "\n",
    "**STEP 9.** Based on your literature review, develop a hypothesis about what you believe the association might be between these topics. Be sure to integrate the specific variables you selected into the hypothesis.\n",
    "\n",
    "**STEP 10.** Ddscribe what you intend to do with your variables (data management/feature creation). For example, do I want to\n",
    "look at how often adolescents binge drink, or do I want to create a binary variables (yes/no) for whether adolescents binge drank in the past month? Do I want depression to be a quantitative variable that aggregates multiple questions that ask about depression symptoms, or do I want to use a cutoff to create a categorical variable that groups adolescents into low, medium, or high levels of depression? These decisions should be described when you describe the variables you will be using.\n",
    "\n",
    "All of this is a starting point for your analysis and will change as you perform the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Writing About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"How-to-Write-About-Your-Data1.png\">\n",
    "\n",
    "<img src=\"How-to-Write-About-Your-Data2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example:\n",
    "\n",
    "Sample\n",
    "\n",
    "The sample is from the first wave of the National Epidemiologic Survey on Alcohol and\n",
    "Related Conditions (NESARC), the largest nationwide longitudinal survey of alcohol and\n",
    "drug use and associated psychiatric and medical comorbidities. Participants (N=43,093)\n",
    "represented the civilian, non-institutionalized adult population of the United States, and\n",
    "included persons living in households, military personnel living off base, and persons\n",
    "residing in the following group quarters: boarding or rooming houses, non-transient hotels\n",
    "and motels, shelters, facilities for housing workers, college quarters, and group homes. The\n",
    "NESARC included over sampling of Blacks, Hispanics and young adults aged 18 to 24 years.\n",
    "The data analytic sample for this study included participants 18-25 years old who reported\n",
    "smoking at least 1 cigarette per day in the past 30 days (N=1,320).\n",
    "\n",
    "Procedure\n",
    "\n",
    "Data were collected by trained U.S. Census Bureau Field Representatives during 2001–\n",
    "2002 through computer-assisted personal interviews (CAPI). One adult was selected for\n",
    "interview in each household, and interviews were conducted in respondents’ homes\n",
    "following informed consent procedures.\n",
    "\n",
    "Measures\n",
    "\n",
    "Lifetime major depression (i.e. those experienced in the past 12 months and prior to the\n",
    "past 12 months) was assessed using the NIAAA, Alcohol Use Disorder and Associated\n",
    "Disabilities Interview Schedule – DSM-IV (AUDADIS-IV) (Grant et al., 2003; Grant, Harford,\n",
    "Dawson, & Chou, 1995). The tobacco module of the AUDADIS-IV contains detailed\n",
    "questions on the frequency, quantity, and patterning of tobacco use as well as symptom\n",
    "criteria for DSM-IV nicotine dependence. Current smoking was evaluated through both\n",
    "smoking frequency (“About how often did you usually smoke in the past year?”) coded\n",
    "dichotomously to represent presence or absence of daily smoking, and quantity (“On the\n",
    "days that you smoked in the last year, about how many cigarettes did you usually smoke?”),\n",
    "a quantitative variable that ranged from 1 cigarette per day to 98 cigarettes per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "<img src=\"wrangling-notebook-15.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P values\n",
    "\n",
    "<img src=\"Mind-Map-p-values.png\">\n",
    "\n",
    "<img src=\"Mind-Map-p-values-scale.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Test\n",
    "Categorical → Quantitative\n",
    "\n",
    "- Goal: A (C) is correlated to the level of B (Q) i.e. Examine the differences in the means of B across the categories of A.\n",
    "- Null Hypothesis: The means across the categories of A are equal.\n",
    "- Alternate Hypothesis: The means across categories of A are ≠.\n",
    "- There are many ways for the population means to not be equal.\n",
    "\n",
    "<img src=\"wrangling-notebook-16.png\">\n",
    "\n",
    "<img src=\"Mind-Map-ANOVA-scale.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi \n",
    "\n",
    "data = pandas.read_csv('nesarc.csv', low_memory=False)\n",
    "\n",
    "#setting variables you will be working with to numeric\n",
    "data['S3AQ3B1'] = pandas.to_numeric(data['S3AQ3B1'], errors='coerce')\n",
    "data['S3AQ3C1'] = pandas.to_numeric(data['S3AQ3C1'], errors='coerce')\n",
    "data['CHECK321'] = pandas.to_numeric(data['CHECK321'], errors='coerce')\n",
    "\n",
    "#subset data to young adults age 18 to 25 who have smoked in the past 12 months\n",
    "sub1=data[(data['AGE']>=18) & (data['AGE']<=25) & (data['CHECK321']==1)]\n",
    "\n",
    "#SETTING MISSING DATA\n",
    "sub1['S3AQ3B1']=sub1['S3AQ3B1'].replace(9, numpy.nan)\n",
    "sub1['S3AQ3C1']=sub1['S3AQ3C1'].replace(99, numpy.nan)\n",
    "\n",
    "# ADDITIONAL DATA WRANGLING\n",
    "#recoding number of days smoked in the past month\n",
    "recode1 = {1: 30, 2: 22, 3: 14, 4: 5, 5: 2.5, 6: 1}\n",
    "sub1['USFREQMO']= sub1['S3AQ3B1'].map(recode1)\n",
    "\n",
    "#converting new variable USFREQMMO to numeric\n",
    "data['USFREQMO'] = pandas.to_numeric(data['USFREQMO'], errors='coerce')\n",
    "\n",
    "# Creating a secondary variable multiplying the days smoked/month and the number of cig/per day\n",
    "sub1['NUMCIGMO_EST']=sub1['USFREQMO'] * sub1['S3AQ3C1']\n",
    "\n",
    "data['NUMCIGMO_EST'] = pandas.to_numeric(data['NUMCIGMO_EST'], errors='coerce')\n",
    "\n",
    "ct1 = sub1.groupby('NUMCIGMO_EST').size()\n",
    "print (ct1)\n",
    "\n",
    "\n",
    "# ANOVA Testing\n",
    "# ols - ordinary least squares\n",
    "# using ols function for calculating the F-statistic and associated p value\n",
    "# Calculate differences in the mean num of response var B (NUMCIGMO_EST), for cases with and without explanatory var A (MAJORDEPLIFE)\n",
    "# C() indicates categorical variable\n",
    "\n",
    "# ---- For categorical variables with 2 Groups\n",
    "sub1 = df\n",
    "model1 = smf.ols(formula='NUMCIGMO_EST ~ C(MAJORDEPLIFE)', data=sub1)\n",
    "results1 = model1.fit()\n",
    "# Generates F-statistic and p-value which is called Prob (F-statistic) in the output\n",
    "# We look at the P-value here to determine if correlation exists or not (< 0.5 correlation exists, otherwise no)\n",
    "print (results1.summary())\n",
    "\n",
    "# Create a subset of the dataset and calculate the means of B, dropping na to get only non-null values, because the OLS analysis also includes only non-null values\n",
    "# We get the means and std to say that these means are statistically equal or not (depending on the p-value)\n",
    "# If p-value < 0.5, by looking at the means tables, we can see which category (presence of A or its absence) resulted in more B.\n",
    "\n",
    "sub2 = sub1[['NUMCIGMO_EST', 'MAJORDEPLIFE']].dropna()\n",
    "\n",
    "print ('means for numcigmo_est by major depression status')\n",
    "m1= sub2.groupby('MAJORDEPLIFE').mean()\n",
    "print (m1)\n",
    "\n",
    "print ('standard deviations for numcigmo_est by major depression status')\n",
    "sd1 = sub2.groupby('MAJORDEPLIFE').std()\n",
    "print (sd1)\n",
    "\n",
    "# ---- For categorical variables with more than 2 Groups\n",
    "sub3 = sub1[['NUMCIGMO_EST', 'ETHRACE2A']].dropna()\n",
    "\n",
    "model2 = smf.ols(formula='NUMCIGMO_EST ~ C(ETHRACE2A)', data=sub3).fit()\n",
    "print (model2.summary())\n",
    "\n",
    "print ('means for numcigmo_est by major depression status')\n",
    "m2= sub3.groupby('ETHRACE2A').mean()\n",
    "print (m2)\n",
    "\n",
    "print ('standard deviations for numcigmo_est by major depression status')\n",
    "sd2 = sub3.groupby('ETHRACE2A').std()\n",
    "print (sd2)\n",
    "\n",
    "# Post-Hoc Tests\n",
    "# Help you figure out which categories are similar in dependence\n",
    "# F-test and p-value don't provide insight into why the null hypothesis can be rejected because there are multiple levels to the categorical explanatory variable\n",
    "# They do not tell us in what way the population means are not statistically equal.\n",
    "# There are many ways for population means not to be all equal.\n",
    "# Maybe they are not all equal to each other. Maybe only 2 of the population means are not equal to each other, but the rest are. \n",
    "\n",
    "# ANOVA doesn't tell us which groups are different. To determine that, we need or perform a post-hoc test (conducts post-hoc paired comparisons)\n",
    "# Performs paired post-hoc ANOVA testing in a way to prevent Type One Error (rejecting the null hypothesis when the null hypothesis is true)\n",
    "# We can't manually pair different categories and do ANOVA testing on them because there's a ahcnace of making a 5% error on each analysis that we conduct (cause our p value ceiling is 0.05). Our overall chance of committing type 1 error can be far > 5%. For e,g, after 15 tests, it is upto 54%. This error rate for group pair comparison is called the family wise error rate.\n",
    "# Post-hoc tests are designed to evaluate the difference between pairs of means while protecting against inflation of Type 1 errors.\n",
    "# There's a lot of post-hoc tests for ANOVA to choose from, they differ in their conservativeness of preventing type 1 error inflation.\n",
    "# We use Tukey's Honestly Significant Difference Test\n",
    "# multi.MultiComparison(quantitative response var, categorical explanatory var)\n",
    "# the last column (reject) shows pairs in which we can reject the null hypothesis. True = there is difference in B for these different categories of A.\n",
    "# Type 1 Error: When we think there is correlation, when there's actually no correlation.\n",
    "mc1 = multi.MultiComparison(sub3['NUMCIGMO_EST'], sub3['ETHRACE2A'])\n",
    "res1 = mc1.tukeyhsd()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Explanation of ANOVA results from code above:\n",
    "\n",
    "**Model Interpretation for ANOVA:**\n",
    "\n",
    "When examining the association between current number of B (quantitative response) and A (categorical explanatory), an Analysis of Variance (ANOVA) revealed that among daily, B of type T (my sample), those with A reported significantly more B (Mean=14.6, s.d. ±9.15) compared to those without A (Mean=11.4, s.d. ±7.43), F(1, 1313)=44.68, p<0001.\n",
    "\n",
    "Note that the degrees of freedom that I report in parentheses) following ‘F’ can be found in the OLS table as the DF model and DF residuals. In this example 44.68 is the actual F value from the OLS table and we commonly report a very small p value as simply <.0001.\n",
    "\n",
    "**Model Interpretation for post hoc ANOVA results:**\n",
    "\n",
    "ANOVA revealed that among daily, B of type T (my sample), A (collapsed into 5 ordered categories, which is the categorical explanatory variable) and B (quantitative response variable) were significantly associated, F (4, 1308)=11.79, p=0001. Post hoc comparisons of mean number of A by pairs of B per day categories revealed that those individuals with B more than 10 per day (i.e. 11 to 15, 16 to 20 and >20) reported significantly more A compared to those with B of 10 or fewer per day (i.e. 1 to 5 and 6 to 10). All other comparisons were statistically similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Test\n",
    "\n",
    "- Categorical explanatory → Categorical response vars\n",
    "- Can be used for 2 or more categories.\n",
    "- Step 1: Which variable plays the role of the explanatory variable\n",
    "- Step 2: Calculate conditional percentages separately (% of the response variable for each value of the explanatory variable).\n",
    "- Step 3: Is it statistically significant?\n",
    "  - Null Hypothesis: No relationship between the 2 categorical variables, independent e.g. genders and drinking (difference is due to sample variability).\n",
    "  - Alternate Hypothesis: Correlation between the 2 categorical variables, not indepedent e.g. genders and drinking (difference is due to gender).\n",
    "- The idea behind the Chi-squared test of independence is to measure how far the data is from what is claimed in the null hypothesis.\n",
    "- We'll have 2 sets of counts:\n",
    "  - **The Observed Counts**: The data as is\n",
    "  - **The Expected Counts**: To represent data if there was no correlation between A and B, and the null hypothesis were true.\n",
    "    - Calculate the counts we'd expect to see if both the explanatory variable and response variable were independent, and the null hypothesis is true.\n",
    "    - P(A and B) = P(A) * P(B) (given A & B are independent - which we are assuming here)\n",
    "    - This is the probability of one instance of A and B.\n",
    "    - The probability of A and B happening N times is = N*P(A and B)\n",
    "    - Measure how far the observed counts are from the expected counts.\n",
    "  - We'll base our decision on the size of the discrepancy between what we observe and what we would expect to observe if the null hypothesis was true.\n",
    "- The Chi-Squared test tells us how far the observed data is from the expected data, if the null hypothesis was true.\n",
    "  - For a 2 by 2, the cutoff limit is 3.84. Chi-square score > 3.84 is considered large.\n",
    "  - For cases other than 2 by 2s, the cutoff limit is determined by the null distribution in that case.\n",
    "  - Therefore, we don't use the chi-square statistic, and only rely on the p-value.\n",
    "  - The p-value of a chi-squared test is the probability of observing a chi-square score at least as large as the one observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pandas.read_csv('nesarc.csv', low_memory=False)\n",
    "\n",
    "# new code setting variables you will be working with to numeric\n",
    "data['TAB12MDX'] = pandas.to_numeric(data['TAB12MDX'], errors='coerce')\n",
    "data['CHECK321'] = pandas.to_numeric(data['CHECK321'], errors='coerce')\n",
    "data['S3AQ3B1'] = pandas.to_numeric(data['S3AQ3B1'], errors='coerce')\n",
    "data['S3AQ3C1'] = pandas.to_numeric(data['S3AQ3C1'], errors='coerce')\n",
    "data['AGE'] = pandas.to_numeric(data['AGE'], errors='coerce')\n",
    "\n",
    "#subset data to young adults age 18 to 25 who have smoked in the past 12 months\n",
    "sub1=data[(data['AGE']>=18) & (data['AGE']<=25) & (data['CHECK321']==1)]\n",
    "\n",
    "#make a copy of my new subsetted data\n",
    "sub2 = sub1.copy()\n",
    "\n",
    "# recode missing values to python missing (NaN)\n",
    "sub2['S3AQ3B1']=sub2['S3AQ3B1'].replace(9, numpy.nan)\n",
    "sub2['S3AQ3C1']=sub2['S3AQ3C1'].replace(99, numpy.nan)\n",
    "\n",
    "#recoding values for S3AQ3B1 into a new variable, USFREQMO\n",
    "recode1 = {1: 30, 2: 22, 3: 14, 4: 6, 5: 2.5, 6: 1}\n",
    "sub2['USFREQMO']= sub2['S3AQ3B1'].map(recode1)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct1=pandas.crosstab(sub2['TAB12MDX'], sub2['USFREQMO'])\n",
    "print (ct1)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct1.sum(axis=0)\n",
    "colpct=ct1/colsum\n",
    "print(colpct)\n",
    "\n",
    "# chi-square\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs1= scipy.stats.chi2_contingency(ct1)\n",
    "print (cs1)\n",
    "\n",
    "\n",
    "# We calculate column percentages because:\n",
    "# + Row %: all rows add up to 100%\n",
    "# + Cell %: all cells in the table together add up to 100%\n",
    "# + Column %: all column add up to 100%\n",
    "'''\n",
    "We calculate column percentages to determine the Chi-Squared test results because\n",
    "the explanatory variable categories are represented as different columns,\n",
    "and response variable values are represented as rows,\n",
    "it'll be the explanatory variable categories(i.e. columns) that we want to interpret.\n",
    "i.e. we are interested in how the rate of response variable differs according to which\n",
    "explanatory group the observations belong to.\n",
    "We're not interested in the column percentages in observations without dependence.\n",
    "\n",
    "'''\n",
    "# If our explanatory variable had only 2 categories, we could interpret the 2 corresponding column percentages\n",
    "# and be able to say which group had a significantly higher rate of dependence.\n",
    "# In case of more than one category, we know that not all are equal,\n",
    "# but don't know which are different and which are not.\n",
    "\n",
    "# Plot the frequency of different categories of explanatory variable vs % of response variable as bar charts.\n",
    "# See next cell for plot\n",
    "# set variable types \n",
    "sub2[\"USFREQMO\"] = sub2[\"USFREQMO\"].astype('category')\n",
    "# new code for setting variables to numeric:\n",
    "sub2['TAB12MDX'] = pandas.to_numeric(sub2['TAB12MDX'], errors='coerce')\n",
    "\n",
    "# graph percent with nicotine dependence within each smoking frequency group \n",
    "seaborn.factorplot(x=\"USFREQMO\", y=\"TAB12MDX\", data=sub2, kind=\"bar\", ci=None)\n",
    "plt.xlabel('Days smoked per month')\n",
    "plt.ylabel('Proportion Nicotine Dependent')\n",
    "\n",
    "# Same post-hoc principles as in ANOVA test. Same principles of avoiding Type 1 errors.\n",
    "# Using Bonferroni Adjustment: p-value/c (number of comparisons we plan to make)\n",
    "# We need to then run a Chi-Square test for each of the paired comparisons.\n",
    "# If your explanatory variable has more than two levels or groups, you'll also need to conduct\n",
    "# a post hoc test. We use the Bonferroni Adjustment to protect against Type I error,\n",
    "# and then run the Chi Square Test of Independence for each paired comparison.\n",
    "\n",
    "# Create new variables that allow you to choose only 2 categories at a time. We use recodes with the map func to do this\n",
    "# We keep only 2 values (1 and 2.5) and remove the rest.\n",
    "# We create a contingency table, get col %s and a new chi-square table with only these 2 variables\n",
    "# If the p-value is greater than the Bonferroni Adjusted p-value, we know there is no correlation.\n",
    "recode2 = {1: 1, 2.5: 2.5}\n",
    "sub2['COMP1v2']= sub2['USFREQMO'].map(recode2)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct2=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v2'])\n",
    "print (ct2)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct2.sum(axis=0)\n",
    "colpct=ct2/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs2= scipy.stats.chi2_contingency(ct2)\n",
    "print (cs2)\n",
    "\n",
    "# ---- We finish by doing the same thing above for each of the other pairs\n",
    "recode3 = {1: 1, 6: 6}\n",
    "sub2['COMP1v6']= sub2['USFREQMO'].map(recode3)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct3=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v6'])\n",
    "print (ct3)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct3.sum(axis=0)\n",
    "colpct=ct3/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs3= scipy.stats.chi2_contingency(ct3)\n",
    "print (cs3)\n",
    "\n",
    "recode4 = {1: 1, 14: 14}\n",
    "sub2['COMP1v14']= sub2['USFREQMO'].map(recode4)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct4=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v14'])\n",
    "print (ct4)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct4.sum(axis=0)\n",
    "colpct=ct4/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs4= scipy.stats.chi2_contingency(ct4)\n",
    "print (cs4)\n",
    "\n",
    "recode5 = {1: 1, 22: 22}\n",
    "sub2['COMP1v22']= sub2['USFREQMO'].map(recode5)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct5=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v22'])\n",
    "print (ct5)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct5.sum(axis=0)\n",
    "colpct=ct5/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs5= scipy.stats.chi2_contingency(ct5)\n",
    "print (cs5)\n",
    "\n",
    "recode6 = {1: 1, 30: 30}\n",
    "sub2['COMP1v30']= sub2['USFREQMO'].map(recode6)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct6=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP1v30'])\n",
    "print (ct6)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct6.sum(axis=0)\n",
    "colpct=ct6/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs6= scipy.stats.chi2_contingency(ct6)\n",
    "print (cs6)\n",
    "\n",
    "recode7 = {2.5: 2.5, 6: 6}\n",
    "sub2['COMP2v6']= sub2['USFREQMO'].map(recode7)\n",
    "\n",
    "# contingency table of observed counts\n",
    "ct7=pandas.crosstab(sub2['TAB12MDX'], sub2['COMP2v6'])\n",
    "print (ct7)\n",
    "\n",
    "# column percentages\n",
    "colsum=ct7.sum(axis=0)\n",
    "colpct=ct7/colsum\n",
    "print(colpct)\n",
    "\n",
    "print ('chi-square value, p value, expected counts')\n",
    "cs7=scipy.stats.chi2_contingency(ct7)\n",
    "print (cs7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Interpretation of Chi-Square Tests results from code above:\n",
    "\n",
    "When examining the association between lifetime major depression (categorical response) and past year nicotine dependence (categorical explanatory), a chi-square test of independence revealed that among daily, young adults smokers (my sample), those with past year nicotine dependence were more likely to have experienced major depression in their lifetime(36.2%) compared to those without past year nicotine dependence (12.7%), X2 =88.60, 1 df, p=0001.\n",
    "\n",
    "The df or degree of freedom we record is the number of levels of the explanatory variable -1. Here the df is 1 nicotine dependence which has 2 levels (df 2-1=1).\n",
    "\n",
    "Model Interpretation for post hoc Chi-Square Test results:\n",
    "\n",
    "A Chi Square test of independence revealed that among daily, young adult smokers (my sample), number of cigarettes smoked per day (collapsed into 5 ordered categories) and past year\n",
    "\n",
    "nicotine dependence (binary categorical variable) were significantly associated, X2 =45.16, 4 df, p=.0001.\n",
    "\n",
    "Post hoc comparisons of rates of nicotine dependence by pairs of cigarettes per day categories revealed that higher rates of nicotine dependence were seen among those smoking more cigarettes, up to 11 to 15 cigarettes per day. In comparison, prevalence of nicotine dependence was statistically similar among those groups smoking 10 to 15, 16 to 20, and > 20 cigarettes per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chart generated from above code.\n",
    "<img src=\"wrangling-notebook-18.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Chi-Square Test**\n",
    "One way to plot the similarities: Use letters to denote categories where depdendent variable rates are similar.\n",
    "<img src=\"wrangling-notebook-19.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "558px",
    "left": "160px",
    "top": "277.5px",
    "width": "359px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
